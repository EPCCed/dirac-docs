
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.2.2, mkdocs-material-7.2.2">
    
    
      
        <title>Running jobs on Tursa - DiRAC Extreme Scaling User Documentation</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.1118c9be.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.ba0d045b.min.css">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Roboto";--md-code-font-family:"Roboto Mono"}</style>
      
    
    
    
    
      


    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="" data-md-color-accent="">
  
    
    <script>function __prefix(e){return new URL("../..",location).pathname+"."+e}function __get(e,t=localStorage){return JSON.parse(t.getItem(__prefix(e)))}</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#running-jobs-on-tursa" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="DiRAC Extreme Scaling User Documentation" class="md-header__button md-logo" aria-label="DiRAC Extreme Scaling User Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            DiRAC Extreme Scaling User Documentation
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Running jobs on Tursa
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        
<a href="https://github.com/EPCCed/dirac-docs.git/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    EPCCed/dirac-docs
  </div>
</a>
      </div>
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="DiRAC Extreme Scaling User Documentation" class="md-nav__button md-logo" aria-label="DiRAC Extreme Scaling User Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    DiRAC Extreme Scaling User Documentation
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/EPCCed/dirac-docs.git/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    EPCCed/dirac-docs
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Documentation overview
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" data-md-state="indeterminate" type="checkbox" id="__nav_2" checked>
      
      <label class="md-nav__link" for="__nav_2">
        Tesseract User Guide
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Tesseract User Guide" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Tesseract User Guide
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../tesseract-user-guide/" class="md-nav__link">
        Overview
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" checked>
      
      <label class="md-nav__link" for="__nav_3">
        Tursa User Guide
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Tursa User Guide" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Tursa User Guide
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        Overview
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../connecting/" class="md-nav__link">
        Connecting to Tursa
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../data/" class="md-nav__link">
        Data Management and transfer
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../sw-environment/" class="md-nav__link">
        Software environment
      </a>
    </li>
  

          
            
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Running jobs on Tursa
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Running jobs on Tursa
      </a>
      
        
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#resources" class="md-nav__link">
    Resources
  </a>
  
    <nav class="md-nav" aria-label="Resources">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#coreh" class="md-nav__link">
    coreh
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpuh" class="md-nav__link">
    GPUh
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#checking-available-budget" class="md-nav__link">
    Checking available budget
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#charging" class="md-nav__link">
    Charging
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#basic-slurm-commands" class="md-nav__link">
    Basic Slurm commands
  </a>
  
    <nav class="md-nav" aria-label="Basic Slurm commands">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sinfo-information-on-resources" class="md-nav__link">
    sinfo: information on resources
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbatch-submitting-jobs" class="md-nav__link">
    sbatch: submitting jobs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#squeue-monitoring-jobs" class="md-nav__link">
    squeue: monitoring jobs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scancel-deleting-jobs" class="md-nav__link">
    scancel: deleting jobs
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#resource-limits" class="md-nav__link">
    Resource Limits
  </a>
  
    <nav class="md-nav" aria-label="Resource Limits">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#primary-resource" class="md-nav__link">
    Primary resource
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#partitions" class="md-nav__link">
    Partitions
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quality-of-service-qos" class="md-nav__link">
    Quality of Service (QoS)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#e-mail-notifications" class="md-nav__link">
    E-mail notifications
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#troubleshooting" class="md-nav__link">
    Troubleshooting
  </a>
  
    <nav class="md-nav" aria-label="Troubleshooting">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slurm-error-messages" class="md-nav__link">
    Slurm error messages
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slurm-queued-reasons" class="md-nav__link">
    Slurm queued reasons
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#output-from-slurm-jobs" class="md-nav__link">
    Output from Slurm jobs
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#specifying-resources-in-job-scripts" class="md-nav__link">
    Specifying resources in job scripts
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#srun-launching-parallel-jobs" class="md-nav__link">
    srun: Launching parallel jobs
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#example-job-submission-scripts" class="md-nav__link">
    Example job submission scripts
  </a>
  
    <nav class="md-nav" aria-label="Example job submission scripts">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#example-job-submission-script-for-mpi-parallel-job" class="md-nav__link">
    Example: job submission script for MPI parallel job
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-job-submission-script-for-mpiopenmp-mixed-mode-parallel-job" class="md-nav__link">
    Example: job submission script for MPI+OpenMP (mixed mode) parallel job
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#job-arrays" class="md-nav__link">
    Job arrays
  </a>
  
    <nav class="md-nav" aria-label="Job arrays">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#job-script-for-a-job-array" class="md-nav__link">
    Job script for a job array
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#submitting-a-job-array" class="md-nav__link">
    Submitting a job array
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#job-chaining" class="md-nav__link">
    Job chaining
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#using-multiple-srun-commands-in-a-single-job-script" class="md-nav__link">
    Using multiple srun commands in a single job script
  </a>
  
    <nav class="md-nav" aria-label="Using multiple srun commands in a single job script">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#running-multiple-full-node-subjobs-within-a-larger-job" class="md-nav__link">
    Running multiple, full-node subjobs within a larger job
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#interactive-jobs" class="md-nav__link">
    Interactive Jobs
  </a>
  
    <nav class="md-nav" aria-label="Interactive Jobs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#using-salloc-to-reserve-resources" class="md-nav__link">
    Using salloc to reserve resources
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#using-srun-directly" class="md-nav__link">
    Using srun directly
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#heterogeneous-jobs" class="md-nav__link">
    Heterogeneous jobs
  </a>
  
    <nav class="md-nav" aria-label="Heterogeneous jobs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#heterogeneous-jobs-for-a-clientserver-model" class="md-nav__link">
    Heterogeneous jobs for a client/server model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#heterogeneous-jobs-for-a-shared-mpi_com_world" class="md-nav__link">
    Heterogeneous jobs for a shared MPI_COM_WORLD
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#heterogeneous-placement-for-mixed-mpiopenmp-work" class="md-nav__link">
    Heterogeneous placement for mixed MPI/OpenMP work
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#low-priority-access" class="md-nav__link">
    Low priority access
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reservations" class="md-nav__link">
    Reservations
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#best-practices-for-job-submission" class="md-nav__link">
    Best practices for job submission
  </a>
  
    <nav class="md-nav" aria-label="Best practices for job submission">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#time-limits" class="md-nav__link">
    Time Limits
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#long-running-jobs" class="md-nav__link">
    Long Running Jobs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#io-performance" class="md-nav__link">
    I/O performance
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#large-jobs" class="md-nav__link">
    Large Jobs
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#resources" class="md-nav__link">
    Resources
  </a>
  
    <nav class="md-nav" aria-label="Resources">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#coreh" class="md-nav__link">
    coreh
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpuh" class="md-nav__link">
    GPUh
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#checking-available-budget" class="md-nav__link">
    Checking available budget
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#charging" class="md-nav__link">
    Charging
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#basic-slurm-commands" class="md-nav__link">
    Basic Slurm commands
  </a>
  
    <nav class="md-nav" aria-label="Basic Slurm commands">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sinfo-information-on-resources" class="md-nav__link">
    sinfo: information on resources
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbatch-submitting-jobs" class="md-nav__link">
    sbatch: submitting jobs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#squeue-monitoring-jobs" class="md-nav__link">
    squeue: monitoring jobs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scancel-deleting-jobs" class="md-nav__link">
    scancel: deleting jobs
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#resource-limits" class="md-nav__link">
    Resource Limits
  </a>
  
    <nav class="md-nav" aria-label="Resource Limits">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#primary-resource" class="md-nav__link">
    Primary resource
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#partitions" class="md-nav__link">
    Partitions
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quality-of-service-qos" class="md-nav__link">
    Quality of Service (QoS)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#e-mail-notifications" class="md-nav__link">
    E-mail notifications
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#troubleshooting" class="md-nav__link">
    Troubleshooting
  </a>
  
    <nav class="md-nav" aria-label="Troubleshooting">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slurm-error-messages" class="md-nav__link">
    Slurm error messages
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slurm-queued-reasons" class="md-nav__link">
    Slurm queued reasons
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#output-from-slurm-jobs" class="md-nav__link">
    Output from Slurm jobs
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#specifying-resources-in-job-scripts" class="md-nav__link">
    Specifying resources in job scripts
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#srun-launching-parallel-jobs" class="md-nav__link">
    srun: Launching parallel jobs
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#example-job-submission-scripts" class="md-nav__link">
    Example job submission scripts
  </a>
  
    <nav class="md-nav" aria-label="Example job submission scripts">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#example-job-submission-script-for-mpi-parallel-job" class="md-nav__link">
    Example: job submission script for MPI parallel job
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-job-submission-script-for-mpiopenmp-mixed-mode-parallel-job" class="md-nav__link">
    Example: job submission script for MPI+OpenMP (mixed mode) parallel job
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#job-arrays" class="md-nav__link">
    Job arrays
  </a>
  
    <nav class="md-nav" aria-label="Job arrays">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#job-script-for-a-job-array" class="md-nav__link">
    Job script for a job array
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#submitting-a-job-array" class="md-nav__link">
    Submitting a job array
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#job-chaining" class="md-nav__link">
    Job chaining
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#using-multiple-srun-commands-in-a-single-job-script" class="md-nav__link">
    Using multiple srun commands in a single job script
  </a>
  
    <nav class="md-nav" aria-label="Using multiple srun commands in a single job script">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#running-multiple-full-node-subjobs-within-a-larger-job" class="md-nav__link">
    Running multiple, full-node subjobs within a larger job
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#interactive-jobs" class="md-nav__link">
    Interactive Jobs
  </a>
  
    <nav class="md-nav" aria-label="Interactive Jobs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#using-salloc-to-reserve-resources" class="md-nav__link">
    Using salloc to reserve resources
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#using-srun-directly" class="md-nav__link">
    Using srun directly
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#heterogeneous-jobs" class="md-nav__link">
    Heterogeneous jobs
  </a>
  
    <nav class="md-nav" aria-label="Heterogeneous jobs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#heterogeneous-jobs-for-a-clientserver-model" class="md-nav__link">
    Heterogeneous jobs for a client/server model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#heterogeneous-jobs-for-a-shared-mpi_com_world" class="md-nav__link">
    Heterogeneous jobs for a shared MPI_COM_WORLD
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#heterogeneous-placement-for-mixed-mpiopenmp-work" class="md-nav__link">
    Heterogeneous placement for mixed MPI/OpenMP work
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#low-priority-access" class="md-nav__link">
    Low priority access
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reservations" class="md-nav__link">
    Reservations
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#best-practices-for-job-submission" class="md-nav__link">
    Best practices for job submission
  </a>
  
    <nav class="md-nav" aria-label="Best practices for job submission">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#time-limits" class="md-nav__link">
    Time Limits
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#long-running-jobs" class="md-nav__link">
    Long Running Jobs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#io-performance" class="md-nav__link">
    I/O performance
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#large-jobs" class="md-nav__link">
    Large Jobs
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/EPCCed/dirac-docs.git/edit/main/docs/tursa-user-guide/scheduler.md" title="Edit this page" class="md-content__button md-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
                  </a>
                
                
                <h1 id="running-jobs-on-tursa">Running jobs on Tursa</h1>
<p>As with most HPC services, Tursa uses a scheduler to manage access to
resources and ensure that the thousands of different users of system are
able to share the system and all get access to the resources they
require. Tursa uses the Slurm software to schedule jobs.</p>
<p>Writing a submission script is typically the most convenient way to
submit your job to the scheduler. Example submission scripts (with
explanations) for the most common job types are provided below.</p>
<p>Interactive jobs are also available and can be particularly useful for
developing and debugging applications. More details are available below.</p>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>If you have any questions on how to run jobs on Tursa do not hesitate
to contact the <a href="mailto:dirac-support@epcc.ed.ac.uk">DiRAC Service Desk</a>.</p>
</div>
<p>You typically interact with Slurm by issuing Slurm commands from the
login nodes (to submit, check and cancel jobs), and by specifying Slurm
directives that describe the resources required for your jobs in job
submission scripts.</p>
<h2 id="resources">Resources</h2>
<h3 id="coreh">coreh</h3>
<p>Time used on Tursa CPU nodes is measured in coreh.<br />
1 coreh = 1 physical core for 1 hour. So a Tursa compute node with 2, 64 core CPUs would cost
128 coreh per hour. </p>
<h3 id="gpuh">GPUh</h3>
<p>Time used on Tursa GPU nodes  is measured in GPUh.<br />
1 GPUh = 1 GPU for 1 hour. So a Tursa compute node with 4 GPUs would cost
4 GPUh per hour. </p>
<h3 id="checking-available-budget">Checking available budget</h3>
<p>You can check in <a href="https://safe.epcc.ed.ac.uk">SAFE</a> by selecting <code>Login accounts</code> from the menu, select the login account you want to query.</p>
<p>Under <code>Login account details</code> you will see each of the budget codes you have access to listed e.g.
<code>e123 resources</code> and then under Resource Pool to the right of this, a note of the remaining budgets. </p>
<p>When logged in to the machine you can also use the command </p>
<pre><code>sacctmgr show assoc where user=$LOGNAME format=account,user,maxtresmins
</code></pre>
<p>This will list all the budget codes that you have access to e.g.</p>
<pre><code>   Account       User   MaxTRESMins
---------- ---------- -------------
      e123      userx         cpu=0
 e123-test      userx
</code></pre>
<p>This shows that <code>userx</code> is a member of budgets <code>e123</code> and <code>e123-test</code>.  However, the <code>cpu=0</code> indicates that the <code>e123</code> budget is empty or disabled.   This user can submit jobs using the <code>e123-test</code> budget.</p>
<p>To see the number of CUs remaining you must check in <a href="https://safe.epcc.ed.ac.uk">SAFE</a>.</p>
<h3 id="charging">Charging</h3>
<p>Jobs run on Tursa are charged for the time they use i.e. from the time the job begins to run until the time the job ends (not the full wall time requested).</p>
<p>Jobs are charged for the full number of nodes which are requested, even if they are not all used.</p>
<p>Charging takes place at the time the job ends, and the job is charged in full to the budget which is live at the end time.</p>
<h2 id="basic-slurm-commands">Basic Slurm commands</h2>
<p>There are three key commands used to interact with the Slurm on the
command line:</p>
<ul>
<li><code>sinfo</code> - Get information on the partitions and resources available</li>
<li><code>sbatch jobscript.slurm</code> - Submit a job submission script (in this
    case called: <code>jobscript.slurm</code>) to the scheduler</li>
<li><code>squeue</code> - Get the current status of jobs submitted to the scheduler</li>
<li><code>scancel 12345</code> - Cancel a job (in this case with the job ID
    <code>12345</code>)</li>
</ul>
<p>We cover each of these commands in more detail below.</p>
<h3 id="sinfo-information-on-resources"><code>sinfo</code>: information on resources</h3>
<p><code>sinfo</code> is used to query information about available resources and
partitions. Without any options, <code>sinfo</code> lists the status of all
resources and partitions, e.g.</p>
<pre><code>sinfo

PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST 
standard     up 1-00:00:00    105  down* nid[001006,...,002014]
standard     up 1-00:00:00     12  drain nid[001016,...,001969]
standard     up 1-00:00:00      5   resv nid[001000,001002-001004,001114] 
standard     up 1-00:00:00    683  alloc nid[001001,...,001970-001991] 
standard     up 1-00:00:00    214   idle nid[001022-001023,...,002015-002023]
standard     up 1-00:00:00      2   down nid[001021,001050]
</code></pre>
<p>Here we see the number of nodes in different states. For example, 683
nodes are allocated (running jobs), and 214 are idle (available to run
jobs). !!! note that long lists of node IDs have been abbreviated with
<code>...</code>.</p>
<h3 id="sbatch-submitting-jobs"><code>sbatch</code>: submitting jobs</h3>
<p><code>sbatch</code> is used to submit a job script to the job submission system.
The script will typically contain one or more <code>srun</code> commands to launch
parallel tasks.</p>
<p>When you submit the job, the scheduler provides the job ID, which is
used to identify this job in other Slurm commands and when looking at
resource usage in SAFE.</p>
<pre><code>sbatch test-job.slurm
Submitted batch job 12345
</code></pre>
<h3 id="squeue-monitoring-jobs"><code>squeue</code>: monitoring jobs</h3>
<p><code>squeue</code> without any options or arguments shows the current status of
all jobs known to the scheduler. For example:</p>
<pre><code>squeue
</code></pre>
<p>will list all jobs on Tursa.</p>
<p>The output of this is often overwhelmingly large. You can restrict the
output to just your jobs by adding the <code>-u $USER</code> option:</p>
<pre><code>squeue -u $USER
</code></pre>
<h3 id="scancel-deleting-jobs"><code>scancel</code>: deleting jobs</h3>
<p><code>scancel</code> is used to delete a jobs from the scheduler. If the job is
waiting to run it is simply cancelled, if it is a running job then it is
stopped immediately. You need to provide the job ID of the job you wish
to cancel/stop. For example:</p>
<pre><code>scancel 12345
</code></pre>
<p>will cancel (if waiting) or stop (if running) the job with ID <code>12345</code>.</p>
<h2 id="resource-limits">Resource Limits</h2>
<p>The Tursa resource limits for any given job are covered by three
separate attributes.</p>
<ul>
<li>The amount of <em>primary resource</em> you require, i.e., number of
    compute nodes.</li>
<li>The <em>partition</em> that you want to use - this specifies the nodes that
    are eligible to run your job.</li>
<li>The <em>Quality of Service (QoS)</em> that you want to use - this specifies
    the job limits that apply.</li>
</ul>
<h3 id="primary-resource">Primary resource</h3>
<p>The <em>primary resource</em> you can request for your job is the compute node.</p>
<div class="admonition information">
<p class="admonition-title">Information</p>
<p>The <code>--exclusive</code> option is enforced on Tursa which means you will
always have access to all of the memory on the compute node regardless
of how many processes are actually running on the node.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You will not generally have access to the full amount of memory resource
on the the node as some is retained for running the operating system and
other system processes.</p>
</div>
<h3 id="partitions">Partitions</h3>
<p>On Tursa, compute nodes are grouped into partitions. You will have to
specify a partition using the <code>--partition</code> option in your Slurm
submission script. The following table has a list of active partitions
on Tursa.</p>
<table>
<thead>
<tr>
<th>Partition</th>
<th>Description</th>
<th>Max nodes available</th>
</tr>
</thead>
<tbody>
<tr>
<td>standard</td>
<td>CPU nodes with AMD EPYC 7742 64-core processor &times; 2</td>
<td>1024</td>
</tr>
</tbody>
</table>
<p>Tursa Partitions</p>
<p>You can list the active partitions by running <code>sinfo</code>.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>You may not have access to all the available partitions.</p>
</div>
<h3 id="quality-of-service-qos">Quality of Service (QoS)</h3>
<p>On Tursa, job limits are defined by the requested Quality of Service
(QoS), as specified by the <code>--qos</code> Slurm directive. The following table
lists the active QoS on Tursa.</p>
<table>
<thead>
<tr>
<th>QoS</th>
<th>Max Nodes Per Job</th>
<th>Max Walltime</th>
<th>Jobs Queued</th>
<th>Jobs Running</th>
<th>Partition(s)</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>standard</td>
<td>256</td>
<td>24 hrs</td>
<td>64</td>
<td>16</td>
<td>standard</td>
<td>Maximum of 256 nodes in use by any one user at any time</td>
</tr>
<tr>
<td>short</td>
<td>8</td>
<td>20 mins</td>
<td>16</td>
<td>4</td>
<td>standard</td>
<td></td>
</tr>
<tr>
<td>long</td>
<td>64</td>
<td>48 hrs</td>
<td>16</td>
<td>16</td>
<td>standard</td>
<td>Minimum walltime of 24 hrs</td>
</tr>
<tr>
<td>largescale</td>
<td>940</td>
<td>3 hrs</td>
<td>4</td>
<td>1</td>
<td>standard</td>
<td>Minimum job size of 257 nodes</td>
</tr>
<tr>
<td>lowpriority</td>
<td>256</td>
<td>3 hrs</td>
<td>4</td>
<td>1</td>
<td>standard</td>
<td>Maximum of 256 nodes in use by any one user at any time. Jobs not charged but requires at least 1 CU in budget to use.</td>
</tr>
</tbody>
</table>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you want to use the <code>short</code> QoS then you also need to add the
<code>--reservation=shortqos</code> to your job submission command.</p>
</div>
<p>You can find out the QoS that you can use by running the following
command:</p>
<pre><code>sacctmgr show assoc user=$USER cluster=tursa-es format=cluster,account,user,qos%50
</code></pre>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>If you have needs which do not fit within the current QoS, please
<a href="https://www.tursa.ac.uk/support-access/servicedesk.html">contact the Service
Desk</a> and we
can discuss how to accommodate your requirements.</p>
</div>
<h3 id="e-mail-notifications">E-mail notifications</h3>
<p>E-mail notifications from the batch system are not currently available
on Tursa.</p>
<h2 id="troubleshooting">Troubleshooting</h2>
<h3 id="slurm-error-messages">Slurm error messages</h3>
<p>An incorrect submission will cause Slurm to return an error.
Some common problems are listed below, with a suggestion about
the likely cause:</p>
<ul>
<li>
<p><code>sbatch: unrecognized option &lt;text&gt;</code></p>
<p>One of your options is invalid or has a typo. <code>man sbatch</code> to help.</p>
</li>
<li>
<p><code>error: Batch job submission failed: No partition specified or system default partition</code></p>
<p>A <code>--partition=</code> option is missing. You must specify the partition
(see the list above). This is most often <code>--partition=standard</code>.</p>
</li>
<li>
<p><code>error: invalid partition specified: &lt;partition&gt;</code></p>
<p><code>error: Batch job submission failed: Invalid partition name specified</code></p>
<p>Check the partition exists and check the spelling is correct.</p>
</li>
<li>
<p><code>error: Batch job submission failed: Invalid account or account/partition combination specified</code></p>
<p>This probably means an invalid account has been given. Check the
<code>--account=</code> options against valid accounts in SAFE.</p>
</li>
<li>
<p><code>error: Batch job submission failed: Invalid qos specification</code></p>
<p>A QoS option is either missing or invalid. Check the script has a
<code>--qos=</code> option and that the option is a valid one from the
table above. (Check the spelling of the QoS is correct.)</p>
</li>
<li>
<p><code>error: Your job has no time specification (--time=)...</code></p>
<p>Add an option of the form <code>--time=hours:minutes:seconds</code> to the
submission script. E.g., <code>--time=01:30:00</code> gives a time limit of
90 minutes.</p>
</li>
<li>
<p><code>error: QOSMaxWallDurationPerJobLimit</code>
    <code>error: Batch job submission failed: Job violates accounting/QOS policy</code>
    <code>(job submit limit, user's size and/or time limits)</code></p>
<p>The script has probably specified a time limit which is too long for
the corresponding QoS. E.g., the time limit for the short QoS
is 20 minutes.</p>
</li>
</ul>
<h3 id="slurm-queued-reasons">Slurm queued reasons</h3>
<p>The <code>squeue</code> command allows users to view information for jobs managed by Slurm. Jobs
typically go through the following states: PENDING, RUNNING, COMPLETING, and COMPLETED.
The first table provides a description of some job state codes. The second table provides a description
of the reasons that cause a job to be in a state.</p>
<table>
<thead>
<tr>
<th>Status</th>
<th>Code</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>PENDING</td>
<td>PD</td>
<td>Job is awaiting resource allocation.</td>
</tr>
<tr>
<td>RUNNING</td>
<td>R</td>
<td>Job currently has an allocation.</td>
</tr>
<tr>
<td>SUSPENDED</td>
<td>S</td>
<td>Job currently has an allocation.</td>
</tr>
<tr>
<td>COMPLETING</td>
<td>CG</td>
<td>Job is in the process of completing. Some processes on some nodes may still be active.</td>
</tr>
<tr>
<td>COMPLETED</td>
<td>CD</td>
<td>Job has terminated all processes on all nodes with an exit code of zero.</td>
</tr>
<tr>
<td>TIMEOUT</td>
<td>TO</td>
<td>Job terminated upon reaching its time limit.</td>
</tr>
<tr>
<td>STOPPED</td>
<td>ST</td>
<td>Job has an allocation, but execution has been stopped with SIGSTOP signal. CPUS have been retained by this job.</td>
</tr>
<tr>
<td>OUT_OF_MEMORY</td>
<td>OOM</td>
<td>Job experienced out of memory error.</td>
</tr>
<tr>
<td>FAILED</td>
<td>F</td>
<td>Job terminated with non-zero exit code or other failure condition.</td>
</tr>
<tr>
<td>NODE_FAIL</td>
<td>NF</td>
<td>Job terminated due to failure of one or more allocated nodes.</td>
</tr>
<tr>
<td>CANCELLED</td>
<td>CA</td>
<td>Job was explicitly cancelled by the user or system administrator. The job may or may not have been initiated.</td>
</tr>
</tbody>
</table>
<p>For a full list of see <a href="https://slurm.schedmd.com/squeue.html#lbAG">Job State Codes</a>.</p>
<table>
<thead>
<tr>
<th>Reason</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Priority</td>
<td>One or more higher priority jobs exist for this partition or advanced reservation.</td>
</tr>
<tr>
<td>Resources</td>
<td>The job is waiting for resources to become available.</td>
</tr>
<tr>
<td>BadConstraints</td>
<td>The job's constraints can not be satisfied.</td>
</tr>
<tr>
<td>BeginTime</td>
<td>The job's earliest start time has not yet been reached.</td>
</tr>
<tr>
<td>Dependency</td>
<td>This job is waiting for a dependent job to complete.</td>
</tr>
<tr>
<td>Licenses</td>
<td>The job is waiting for a license.</td>
</tr>
<tr>
<td>WaitingForScheduling</td>
<td>No reason has been set for this job yet. Waiting for the scheduler to determine the appropriate reason.</td>
</tr>
<tr>
<td>Prolog</td>
<td>Its PrologSlurmctld program is still running.</td>
</tr>
<tr>
<td>JobHeldAdmin</td>
<td>The job is held by a system administrator.</td>
</tr>
<tr>
<td>JobHeldUser</td>
<td>The job is held by the user.</td>
</tr>
<tr>
<td>JobLaunchFailure</td>
<td>The job could not be launched. This may be due to a file system problem, invalid program name, etc.</td>
</tr>
<tr>
<td>NonZeroExitCode</td>
<td>The job terminated with a non-zero exit code.</td>
</tr>
<tr>
<td>InvalidAccount</td>
<td>The job's account is invalid.</td>
</tr>
<tr>
<td>InvalidQOS</td>
<td>The job's QOS is invalid.</td>
</tr>
<tr>
<td>QOSUsageThreshold</td>
<td>Required QOS threshold has been breached.</td>
</tr>
<tr>
<td>QOSJobLimit</td>
<td>The job's QOS has reached its maximum job count.</td>
</tr>
<tr>
<td>QOSResourceLimit</td>
<td>The job's QOS has reached some resource limit.</td>
</tr>
<tr>
<td>QOSTimeLimit</td>
<td>The job's QOS has reached its time limit.</td>
</tr>
<tr>
<td>NodeDown</td>
<td>A node required by the job is down.</td>
</tr>
<tr>
<td>TimeLimit</td>
<td>The job exhausted its time limit.</td>
</tr>
<tr>
<td>ReqNodeNotAvail</td>
<td>Some node specifically required by the job is not currently available. The node may currently be in use, reserved for another job, in an advanced reservation, DOWN, DRAINED, or not responding. Nodes which are DOWN, DRAINED, or not responding will be identified as part of the job's "reason" field as "UnavailableNodes". Such nodes will typically require the intervention of a system administrator to make available.</td>
</tr>
</tbody>
</table>
<p>For a full list of see <a href="https://slurm.schedmd.com/squeue.html#lbAF">Job Reasons</a>.</p>
<h2 id="output-from-slurm-jobs">Output from Slurm jobs</h2>
<p>Slurm places standard output (STDOUT) and standard error (STDERR) for
each job in the file <code>slurm_&lt;JobID&gt;.out</code>. This file appears in the job's
working directory once your job starts running.</p>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>Output may be buffered - to enable live output, e.g. for monitoring
job status, add <code>--unbuffered</code> to the <code>srun</code> command in your SLURM
script.</p>
</div>
<h2 id="specifying-resources-in-job-scripts">Specifying resources in job scripts</h2>
<p>You specify the resources you require for your job using directives at
the top of your job submission script using lines that start with the
directive <code>#SBATCH</code>.</p>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>Most options provided using <code>#SBATCH</code> directives can also be specified as
command line options to <code>srun</code>.</p>
</div>
<p>If you do not specify any options, then the default for each option will
be applied. As a minimum, all job submissions must specify the budget
that they wish to charge the job too with the option:</p>
<ul>
<li><code>--account=&lt;budgetID&gt;</code> your budget ID is usually something like
     <code>t01</code> or <code>t01-test</code>. You can see which budget codes you can charge
     to in SAFE.</li>
</ul>
<p>Other common options that are used are:</p>
<ul>
<li><code>--time=&lt;hh:mm:ss&gt;</code> the maximum walltime for your job. <em>e.g.</em> For
     a 6.5 hour walltime, you would use <code>--time=6:30:0</code>.</li>
<li><code>--job-name=&lt;jobname&gt;</code> set a name for the job to help identify it
     in</li>
</ul>
<p>In addition, parallel jobs will also need to specify how many nodes,
parallel processes and threads they require.</p>
<ul>
<li><code>--nodes=&lt;nodes&gt;</code> the number of nodes to use for the job.</li>
<li><code>--tasks-per-node=&lt;processes per node&gt;</code> the number of parallel
     processes (e.g. MPI ranks) per node.</li>
<li><code>--cpus-per-task=1</code> if you are using parallel processes only with
     no threading then you should set the number of CPUs (cores) per
     parallel process to 1. <strong>!!! note:</strong> if you are using threading (e.g.
     with OpenMP) then you will need to change this option as described
     below.</li>
</ul>
<p>For parallel jobs that use threading (e.g. OpenMP), you will also need
to change the <code>--cpus-per-task</code> option.</p>
<ul>
<li><code>--cpus-per-task=&lt;threads per task&gt;</code> the number of threads per
     parallel process (e.g. number of OpenMP threads per MPI task for
     hybrid MPI/OpenMP jobs). <strong>!!! note:</strong> you must also set the
     <code>OMP_NUM_THREADS</code> environment variable if using OpenMP in your
     job.</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For parallel jobs, Tursa operates in a <em>node exclusive</em> way. This
means that you are assigned resources in the units of full compute nodes
for your jobs (<em>i.e.</em> 128 cores) and that no other user can share those
compute nodes with you. Hence, the minimum amount of resource you can
request for a parallel job is 1 node (or 128 cores).</p>
</div>
<p>To prevent the behaviour of batch scripts being dependent on the user
environment at the point of submission, the option</p>
<ul>
<li><code>--export=none</code> prevents the user environment from being exported
     to the batch system.</li>
</ul>
<p>Using the <code>--export=none</code> means that the behaviour of batch submissions
should be repeatable. We strongly recommend its use, although see
<a href="./#using-modules-in-the-batch-system-the-epcc-job-env-module">the following section</a>
to enable access to the usual modules.</p>
<h2 id="srun-launching-parallel-jobs"><code>srun</code>: Launching parallel jobs</h2>
<p>If you are running parallel jobs, your job submission script should
contain one or more <code>srun</code> commands to launch the parallel executable
across the compute nodes. In most cases you will want to add the options
<code>--distribution=block:block</code> and <code>--hint=nomultithread</code> to your 
<code>srun</code> command to ensure you get the correct pinning of processes to 
cores on a compute node.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you do not add the <code>--distribution=block:block</code> and <code>--hint=nomultithread</code>
options to your <code>srun</code> command the default process placement 
may lead to a drop in performance for your jobs on Tursa.</p>
</div>
<p>A brief explanation of these options:
 - <code>--hint=nomultithread</code> - do not use hyperthreads/SMP
 - <code>--distribution=block:block</code> - the first <code>block</code> means use a block distribution
   of processes across nodes (i.e. fill nodes before moving onto the next one) and
   the second <code>block</code> means use a block distribution of processes across NUMA regions
   within a node (i.e. fill a NUMA region before moving on to the next one).</p>
<h2 id="example-job-submission-scripts">Example job submission scripts</h2>
<p>A subset of example job submission scripts are included in full below.</p>
<h3 id="example-job-submission-script-for-mpi-parallel-job">Example: job submission script for MPI parallel job</h3>
<p>A simple MPI job submission script to submit a job using 4 compute nodes
and 128 MPI ranks per node for 20 minutes would look like:</p>
<pre><code>#!/bin/bash

# Slurm job options (job-name, compute nodes, job time)
#SBATCH --job-name=Example_MPI_Job
#SBATCH --time=0:20:0
#SBATCH --nodes=4
#SBATCH --tasks-per-node=128
#SBATCH --cpus-per-task=1

# Replace [budget code] below with your budget code (e.g. t01)
#SBATCH --account=[budget code]             
#SBATCH --partition=standard
#SBATCH --qos=standard

# Setup the job environment (this module needs to be loaded before any other modules)
module load epcc-job-env

# Set the number of threads to 1
#   This prevents any threaded system libraries from automatically 
#   using threading.
export OMP_NUM_THREADS=1

# Launch the parallel job
#   Using 512 MPI processes and 128 MPI processes per node
#   srun picks up the distribution from the sbatch options

srun --distribution=block:block --hint=nomultithread ./my_mpi_executable.x
</code></pre>
<p>This will run your executable "my_mpi_executable.x" in parallel on 512
MPI processes using 4 nodes (128 cores per node, i.e. not using
hyper-threading). Slurm will allocate 4 nodes to your job and srun will
place 128 MPI processes on each node (one per physical core).</p>
<p>See above for a more detailed discussion of the different <code>sbatch</code>
options</p>
<h3 id="example-job-submission-script-for-mpiopenmp-mixed-mode-parallel-job">Example: job submission script for MPI+OpenMP (mixed mode) parallel job</h3>
<p>Mixed mode codes that use both MPI (or another distributed memory
parallel model) and OpenMP should take care to ensure that the shared
memory portion of the process/thread placement does not span more than
one NUMA region. Nodes on Tursa are made up of two sockets each
containing 4 NUMA regions of 16 cores, i.e. there are 8 NUMA regions in
total. Therefore the total number of threads should ideally not be
greater than 16, and also needs to be a factor of 16. Sensible choices
for the number of threads are therefore 1 (single-threaded), 2, 4, 8,
and 16. More information about using OpenMP and MPI+OpenMP can be found
in the Tuning chapter.</p>
<p>To ensure correct placement of MPI processes the number of cpus-per-task
needs to match the number of OpenMP threads, and the number of
tasks-per-node should be set to ensure the entire node is filled with
MPI tasks.</p>
<p>In the example below, we are using 4 nodes for 6 hours. There are 32 MPI
processes in total (8 MPI processes per node) and 16 OpenMP threads per
MPI process. This results in all 128 physical cores per node being used.</p>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>Note the use of the <code>export OMP_PLACES=cores</code> environment option to
generate the correct thread pinning.</p>
</div>
<div class="highlight"><pre><span></span><code>#!/bin/bash

# Slurm job options (job-name, compute nodes, job time)
#SBATCH --job-name=Example_MPI_Job
#SBATCH --time=0:20:0
#SBATCH --nodes=4
#SBATCH --tasks-per-node=8
#SBATCH --cpus-per-task=16

# Replace [budget code] below with your project code (e.g. t01)
#SBATCH --account=[budget code] 
#SBATCH --partition=standard
#SBATCH --qos=standard

# Setup the job environment (this module needs to be loaded before any other modules)
module load epcc-job-env

# Set the number of threads to 16 and specify placement
#   There are 16 OpenMP threads per MPI process
#   We want one thread per physical core
export OMP_NUM_THREADS=16
export OMP_PLACES=cores

# Launch the parallel job
#   Using 32 MPI processes
#   8 MPI processes per node
#   16 OpenMP threads per MPI process
#   Additional srun options to pin one thread per physical core
srun --hint=nomultithread --distribution=block:block ./my_mixed_executable.x arg1 arg2
</code></pre></div>
<h2 id="job-arrays">Job arrays</h2>
<p>The Slurm job scheduling system offers the <em>job array</em> concept, for
running collections of almost-identical jobs. For example, running the
same program several times with different arguments or input data.</p>
<p>Each job in a job array is called a <em>subjob</em>. The subjobs of a job array
can be submitted and queried as a unit, making it easier and cleaner to
handle the full set, compared to individual jobs.</p>
<p>All subjobs in a job array are started by running the same job script.
The job script also contains information on the number of jobs to be
started, and Slurm provides a subjob index which can be passed to the
individual subjobs or used to select the input data per subjob.</p>
<h3 id="job-script-for-a-job-array">Job script for a job array</h3>
<p>As an example, the following script runs 56 subjobs, with the subjob
index as the only argument to the executable. Each subjob requests a
single node and uses all 128 cores on the node by placing 1 MPI process
per core and specifies 4 hours maximum runtime per subjob:</p>
<pre><code>#!/bin/bash
# Slurm job options (job-name, compute nodes, job time)
#SBATCH --job-name=Example_Array_Job
#SBATCH --time=04:00:00
#SBATCH --nodes=1
#SBATCH --tasks-per-node=128
#SBATCH --cpus-per-task=1
#SBATCH --array=0-55

# Replace [budget code] below with your budget code (e.g. t01)
#SBATCH --account=[budget code]  
#SBATCH --partition=standard
#SBATCH --qos=standard

# Setup the job environment (this module needs to be loaded before any other modules)
module load epcc-job-env

# Set the number of threads to 1
#   This prevents any threaded system libraries from automatically 
#   using threading.
export OMP_NUM_THREADS=1

srun --distribution=block:block --hint=nomultithread /path/to/exe $SLURM_ARRAY_TASK_ID
</code></pre>
<h3 id="submitting-a-job-array">Submitting a job array</h3>
<p>Job arrays are submitted using <code>sbatch</code> in the same way as for standard
jobs:</p>
<pre><code>sbatch job_script.pbs
</code></pre>
<h2 id="job-chaining">Job chaining</h2>
<p>Job dependencies can be used to construct complex pipelines or chain
together long simulations requiring multiple steps.</p>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>The <code>--parsable</code> option to <code>sbatch</code> can simplify working with job
dependencies. It returns the job ID in a format that can be used as the
input to other commands.</p>
</div>
<p>For example:</p>
<pre><code>jobid=$(sbatch --parsable first_job.sh)
sbatch --dependency=afterok:$jobid second_job.sh
</code></pre>
<p>or for a longer chain:</p>
<pre><code>jobid1=$(sbatch --parsable first_job.sh)
jobid2=$(sbatch --parsable --dependency=afterok:$jobid1 second_job.sh)
jobid3=$(sbatch --parsable --dependency=afterok:$jobid1 third_job.sh)
sbatch --dependency=afterok:$jobid2,afterok:$jobid3 last_job.sh
</code></pre>
<h2 id="using-multiple-srun-commands-in-a-single-job-script">Using multiple <code>srun</code> commands in a single job script</h2>
<p>You can use multiple <code>srun</code> commands within in a Slurm job submission script
to allow you to use the resource requested more flexibly. For example, you 
could run a collection of smaller jobs within the requested resources or
you could even subdivide nodes if your individual calculations do not scale
up to use all 128 cores on a node.</p>
<p>In this guide we will cover two scenarios:</p>
<ol>
<li>Subdividing the job into multiple full-node or multi-node subjobs, e.g.
    requesting 100 nodes and running 100, 1-node subjobs or 50, 2-node 
    subjobs.</li>
<li>Subdividing the job into multiple subjobs that each use a fraction of a
    node, e.g. requesting 2 nodes and running 256, 1-core subjobs or 16,
    16-core subjobs.</li>
</ol>
<h3 id="running-multiple-full-node-subjobs-within-a-larger-job">Running multiple, full-node subjobs within a larger job</h3>
<p>When subdivding a larger job into smaller subjobs you typically need to 
overwrite the <code>--nodes</code> option to <code>srun</code> and add the <code>--ntasks</code> option
to ensure that each subjob runs on the correct number of nodes and that
subjobs are placed correctly onto separate nodes.</p>
<p>For example, we will show how to request 100 nodes and then run 100
separate 1-node jobs, each of which use 128 MPI processes and which
run on a different compute node. We start by showing 
the job script that would achieve this and then explain how this works
and the options used. In our case, we will run 100 copies of the <code>xthi</code> 
program that prints the process placement on the node it is running on.</p>
<div class="highlight"><pre><span></span><code><span class="ch">#!/bin/bash</span>

<span class="c1"># Slurm job options (job-name, compute nodes, job time)</span>
<span class="kp">#SBATCH --job-name=multi_xthi</span>
<span class="kp">#SBATCH --time=0:20:0</span>
<span class="kp">#SBATCH --nodes=100</span>
<span class="kp">#SBATCH --tasks-per-node=128</span>
<span class="kp">#SBATCH --cpus-per-task=1</span>

<span class="c1"># Replace [budget code] below with your budget code (e.g. t01)</span>
<span class="kp">#SBATCH --account=[budget code]             </span>
<span class="kp">#SBATCH --partition=standard</span>
<span class="kp">#SBATCH --qos=standard</span>

<span class="c1"># Setup the job environment (this module needs to be loaded before any other modules)</span>
module load epcc-job-env

<span class="c1"># Load the xthi module</span>
module load xthi

<span class="c1"># Set the number of threads to 1</span>
<span class="c1">#   This prevents any threaded system libraries from automatically </span>
<span class="c1">#   using threading.</span>
<span class="nb">export</span> <span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">1</span>

<span class="c1"># Loop over 100 subjobs starting each of them on a separate node</span>
<span class="k">for</span> i <span class="k">in</span> <span class="k">$(</span>seq <span class="m">1</span> <span class="m">100</span><span class="k">)</span>
<span class="k">do</span>
   <span class="c1"># Launch this subjob on 1 node, note nodes and ntasks options and &amp; to place subjob in the background</span>
   <span class="nb">srun</span> --nodes<span class="o">=</span><span class="m">1</span> --ntasks<span class="o">=</span><span class="m">128</span> --distribution<span class="o">=</span>block:block --hint<span class="o">=</span>nomultithread xthi &gt; placement<span class="si">${</span><span class="nv">i</span><span class="si">}</span>.txt <span class="p">&amp;</span>
<span class="k">done</span>
<span class="c1"># Wait for all background subjobs to finish</span>
<span class="nb">wait</span>
</code></pre></div>
<p>Key points from the example job script:</p>
<ul>
<li>The <code>#SBATCH</code> options select 100 full nodes in the usual way.</li>
<li>Each subjob <code>srun</code> command sets the following:<ul>
<li><code>--nodes=1</code> We need override this setting from the main job so that each subjob only uses 1 node</li>
<li><code>--ntasks=128</code> For normal jobs, the number of parallel tasks (MPI processes) is calculated from
  the number of nodes you request and the number of tasks per node. We need to explicitly tell <code>srun</code>
  how many we require for this subjob.</li>
<li><code>--distribution=block:block --hint=nomultithread</code> These options ensure correct placement of
  processes within the compute nodes.</li>
<li><code>&amp;</code> Each subjob <code>srun</code> command ends with an ampersand to place the process in the background
  and move on to the next loop iteration (and subjob submission). Without this, the script would
  wait for this subjob to complete before moving on to submit the next.</li>
</ul>
</li>
<li>Finally, there is the <code>wait</code> command to tell the script to wait for all the background subjobs
to complete before exiting. If we did not have this in place, the script would exit as soon as the
last subjob was submitted and kill all running subjobs.</li>
</ul>
<h2 id="interactive-jobs">Interactive Jobs</h2>
<h3 id="using-salloc-to-reserve-resources">Using <code>salloc</code> to reserve resources</h3>
<p>When you are developing or debugging code you often want to run many
short jobs with a small amount of editing the code between runs. This
can be achieved by using the login nodes to run MPI but you may want to
test on the compute nodes (e.g. you may want to test running on multiple
nodes across the high performance interconnect). One of the best ways to
achieve this on Tursa is to use interactive jobs.</p>
<p>An interactive job allows you to issue <code>srun</code> commands directly from the
command line without using a job submission script, and to see the
output from your program directly in the terminal.</p>
<p>You use the <code>salloc</code> command to reserve compute nodes for interactive
jobs.</p>
<p>To submit a request for an interactive job reserving 8 nodes (1024
physical cores) for 20 minutes on the short queue you would issue the
following command from the command line:</p>
<pre><code>auser@login01:&gt; salloc --nodes=8 --tasks-per-node=128 --cpus-per-task=1 \
              --time=00:20:00 --partition=standard --qos=short \
              --reservation=shortqos --account=[budget code]
</code></pre>
<p>When you submit this job your terminal will display something like:</p>
<pre><code>salloc: Granted job allocation 24236
salloc: Waiting for resource configuration
salloc: Nodes nid000002 are ready for job
auser@login01:&gt;
</code></pre>
<p>It may take some time for your interactive job to start. Once it runs
you will enter a standard interactive terminal session (a new shell).
Note that this shell is still on the front end (the prompt has not
change). Whilst the interactive session lasts you will be able to run
parallel jobs on the compute nodes by issuing the <code>srun
--distribution=block:block --hint=nomultithread</code> command directly at 
your command prompt using the same syntax as you would inside a job
script. The maximum number of nodes you can use is limited by resources
requested in the <code>salloc</code> command.</p>
<p>If you know you will be doing a lot of intensive debugging you may find
it useful to request an interactive session lasting the expected length
of your working session, say a full day.</p>
<p>Your session will end when you hit the requested walltime. If you wish
to finish before this you should use the <code>exit</code> command - this will
return you to your prompt before you issued the <code>salloc</code> command.</p>
<h3 id="using-srun-directly">Using <code>srun</code> directly</h3>
<p>A second way to run an interactive job is to use <code>srun</code> directly in the
following way (here using the "short queue"):</p>
<pre><code>auser@login01:/work/t01/t01/auser&gt; srun --nodes=1 --exclusive --time=00:20:00 \
               --partition=standard --qos=short --reservation=shortqos \
               --pty /bin/bash
auser@login01:/work/t01/t01/auser&gt; hostname
nid001261
</code></pre>
<p>The <code>--pty /bin/bash</code> will cause a new shell to be started on the first
node of a new allocation (note that while the shell prompt has not
changed, we are now on the compute node). This is perhaps closer to what
many people consider an 'interactive' job than the method using <code>salloc</code>
appears.</p>
<p>One can now issue shell commands in the usual way. A further invocation
of <code>srun</code> is required to launch a parallel job in the allocation.</p>
<p>When finished, type <code>exit</code> to relinquish the allocation and control will
be returned to the front end.</p>
<p>By default, the interactive shell will retain the environment of the
parent. If you want a clean shell, remember to specify <code>--export=none</code>.
If you need to
<a href="./#using-modules-in-the-batch-system-the-epcc-job-env-module">use modules within your job</a>,
you will need to start a login shell by passing the <code>--login</code> argument
to <code>bash</code>.</p>
<h2 id="heterogeneous-jobs">Heterogeneous jobs</h2>
<p>The SLURM submissions discussed above involve a single executable image.
However, there are situtions where two or more distinct executables are
coupled and need to be run at the same time. This is most easily handled
via the SLURM heterogeneous job mechanism.</p>
<p>The essential feature of a heterogeneous job is to create a single batch
submission which specifies the resource requirements for the individual
components. Schematically, we would use</p>
<p><div class="highlight"><pre><span></span><code>#!/bin/bash

# SLURM specifications for the first component

#SBATCH --partition=standard

...

#SBATCH hetjob

# SLURM specifications for the second component

#SBATCH --partition=standard

...
</code></pre></div>
where new each component beyond the first is introduced by the special
token <code>#SBATCH hetjob</code> (note this is not a normal option and is not
<code>--hetjob</code>). Each component must specify a partition.</p>
<p>Such a job will appear in the queue system as, e.g.,
<div class="highlight"><pre><span></span><code>           50098+0  standard qscript-    user  PD       0:00      1 (None) 
           50098+1  standard qscript-    user  PD       0:00      2 (None) 
</code></pre></div>
and counts as (in this case) two separate jobs from the point of
QoS limits.</p>
<p>Two common cases are discussed below: first, a client server model in
which client and server each have a different <code>MPI_COMM_WORLD</code>, and second
the case were two or more executables share <code>MPI_COMM_WORLD</code>.</p>
<h3 id="heterogeneous-jobs-for-a-clientserver-model">Heterogeneous jobs for a client/server model</h3>
<p>Consider a case where we have two executables which may both be parallel (in
that they use MPI), both run at the same time, and communicate with each
other by some means other than MPI. In the following example, we run two
different executables, both of which must finish before the jobs completes.</p>
<p><div class="highlight"><pre><span></span><code>#!/bin/bash

#SBATCH --time=00:20:00
#SBATCH --exclusive
#SBATCH --export=none

#SBATCH --partition=standard
#SBATCH --qos=standard

#SBATCH --nodes=1
#SBATCH --ntasks-per-node=8

#SBATCH hetjob

#SBATCH --partition=standard
#SBATCH --qos=standard

#SBATCH --nodes=2
#SBATCH --ntasks-per-node=4


# Run two execuatables with separate MPI_COMM_WORLD

srun --distribution=block:block --hint=nomultithread --het-group=0 ./xthi-a &amp;
srun --distribution=block:block --hint=nomultithread --het-group=1 ./xthi-b &amp;
wait
</code></pre></div>
In this case, each executable is launched with a separate call to
<code>srun</code> but specifies a different heterogeneous group via the
<code>--het-group</code> option. The first group is <code>--het-group=0</code>.
Both are run in the background with <code>&amp;</code> and the <code>wait</code> is required
to ensure both executables have completed before the job submission
exits.</p>
<p>In this rather artificial example, where each component makes a
simple report about its placement, the output might be
<div class="highlight"><pre><span></span><code>Node    0, hostname nid001028, mpi   4, omp   1, executable xthi-b
Node    1, hostname nid001048, mpi   4, omp   1, executable xthi-b
Node    0, rank    0, thread   0, (affinity =    0)
Node    0, rank    1, thread   0, (affinity =    1)
Node    0, rank    2, thread   0, (affinity =    2)
Node    0, rank    3, thread   0, (affinity =    3)
Node    1, rank    4, thread   0, (affinity =    0)
Node    1, rank    5, thread   0, (affinity =    1)
Node    1, rank    6, thread   0, (affinity =    2)
Node    1, rank    7, thread   0, (affinity =    3)
Node    0, hostname nid001027, mpi   8, omp   1, executable xthi-a
Node    0, rank    0, thread   0, (affinity =    0)
Node    0, rank    1, thread   0, (affinity =    1)
Node    0, rank    2, thread   0, (affinity =    2)
Node    0, rank    3, thread   0, (affinity =    3)
Node    0, rank    4, thread   0, (affinity =    4)
Node    0, rank    5, thread   0, (affinity =    5)
Node    0, rank    6, thread   0, (affinity =    6)
Node    0, rank    7, thread   0, (affinity =    7)
</code></pre></div>
Here we have the first executable running on one node with
a communicator size 8 (ranks 0-7). The second executable runs on
two nodes also with communicator size 8 (ranks 0-7, 4 ranks per node).
Further examples of placement for heterogenenous jobs are given below.</p>
<h3 id="heterogeneous-jobs-for-a-shared-mpi_com_world">Heterogeneous jobs for a shared <code>MPI_COM_WORLD</code></h3>
<p>If two or more heterogeneous components need to share a unique
<code>MPI_COMM_WORLD</code>, a single <code>srun</code> invocation with the differrent
components separated by a colon <code>:</code> should be used. For example,</p>
<div class="highlight"><pre><span></span><code>#!/bin/bash

#SBATCH --time=00:20:00
#SBATCH --exclusive
#SBATCH --export=none

#SBATCH --partition=standard
#SBATCH --qos=standard

#SBATCH --nodes=1
#SBATCH --ntasks-per-node=8

#SBATCH hetjob

#SBATCH --partition=standard
#SBATCH --qos=standard

#SBATCH --nodes=2
#SBATCH --ntasks-per-node=4

srun --distribution=block:block --hint=nomultithread --het-group=0 ./xthi-a : \
     --distribution=block:block --hint=nomultithread --het-group=1 ./xthi-b
</code></pre></div>
<p>The output should confirm we have a single <code>MPI_COMM_WORLD</code> with
ranks 0-15.
<div class="highlight"><pre><span></span><code>Node    0, hostname nid001027, mpi   8, omp   1, executable xthi-a
Node    1, hostname nid001028, mpi   4, omp   1, executable xthi-b
Node    2, hostname nid001048, mpi   4, omp   1, executable xthi-b
Node    0, rank    0, thread   0, (affinity =    0)
Node    0, rank    1, thread   0, (affinity =    1)
Node    0, rank    2, thread   0, (affinity =    2)
Node    0, rank    3, thread   0, (affinity =    3)
Node    0, rank    4, thread   0, (affinity =    4)
Node    0, rank    5, thread   0, (affinity =    5)
Node    0, rank    6, thread   0, (affinity =    6)
Node    0, rank    7, thread   0, (affinity =    7)
Node    1, rank    8, thread   0, (affinity =    0)
Node    1, rank    9, thread   0, (affinity =    1)
Node    1, rank   10, thread   0, (affinity =    2)
Node    1, rank   11, thread   0, (affinity =    3)
Node    2, rank   12, thread   0, (affinity =    0)
Node    2, rank   13, thread   0, (affinity =    1)
Node    2, rank   14, thread   0, (affinity =    2)
Node    2, rank   15, thread   0, (affinity =    3)
</code></pre></div></p>
<h3 id="heterogeneous-placement-for-mixed-mpiopenmp-work">Heterogeneous placement for mixed MPI/OpenMP work</h3>
<p>Some care may be required for placement of tasks/threads in heterogeneous
jobs in which the number of threads needs to be specified differently
for different components.</p>
<p>In the following we have two components. The
first component runs 8 MPI tasks each with 16 OpenMP threads.
The second component runs 8 MPI tasks with
one task per NUMA region on one node; each task has one thread.
An appropriate SLURM submission might be:</p>
<p><div class="highlight"><pre><span></span><code>#!/bin/bash

#SBATCH --time=00:20:00
#SBATCH --exclusive
#SBATCH --export=none

# First component 

#SBATCH --partition=standard
#SBATCH --qos=standard

#SBATCH --nodes=1
#SBATCH --ntasks-per-node=8
#SBATCH --cpus-per-task=16
#SBATCH --hint=nomultithread

# Second component

#SBATCH hetjob

#SBATCH --partition=standard

#SBATCH --nodes=1
#SBATCH --ntasks-per-node=8
#SBATCH --cpus-per-task=16

# Do not set OMP_NUM_THREADS in the calling environment

unset OMP_NUM_THREADS
export OMP_PROC_BIND=spread

srun --het-group=0 --export=all,OMP_NUM_THREADS=16 ./xthi-a : \
     --het-group=1 --export=all,OMP_NUM_THREADS=1  ./xthi-b
</code></pre></div>
The important point here is that <code>OMP_NUM_THREADS</code> must not be set
in the environment that calls <code>srun</code> in order that the different
specifications for the separate groups via <code>--export</code> on the <code>srun</code>
command line take effect. If <code>OMP_NUM_THREADS</code> is set in the calling
environment, then that value takes precedence, and each component will
see the same value of <code>OMP_NUM_THREADS</code>.</p>
<p>The output would be:
<div class="highlight"><pre><span></span><code>Node    0, hostname nid001111, mpi   8, omp  16, executable xthi-a
Node    1, hostname nid001126, mpi   8, omp   1, executable xthi-b
Node    0, rank    0, thread   0, (affinity =    0)
Node    0, rank    0, thread   1, (affinity =    1)
Node    0, rank    0, thread   2, (affinity =    2)
Node    0, rank    0, thread   3, (affinity =    3)
Node    0, rank    0, thread   4, (affinity =    4)
Node    0, rank    0, thread   5, (affinity =    5)
Node    0, rank    0, thread   6, (affinity =    6)
Node    0, rank    0, thread   7, (affinity =    7)
Node    0, rank    0, thread   8, (affinity =    8)
Node    0, rank    0, thread   9, (affinity =    9)
Node    0, rank    0, thread  10, (affinity =   10)
Node    0, rank    0, thread  11, (affinity =   11)
Node    0, rank    0, thread  12, (affinity =   12)
Node    0, rank    0, thread  13, (affinity =   13)
Node    0, rank    0, thread  14, (affinity =   14)
Node    0, rank    0, thread  15, (affinity =   15)
Node    0, rank    1, thread   0, (affinity =   16)
Node    0, rank    1, thread   1, (affinity =   17)
...
Node    0, rank    7, thread  14, (affinity =  126)
Node    0, rank    7, thread  15, (affinity =  127)
Node    1, rank    8, thread   0, (affinity =    0)
Node    1, rank    9, thread   0, (affinity =   16)
Node    1, rank   10, thread   0, (affinity =   32)
Node    1, rank   11, thread   0, (affinity =   48)
Node    1, rank   12, thread   0, (affinity =   64)
Node    1, rank   13, thread   0, (affinity =   80)
Node    1, rank   14, thread   0, (affinity =   96)
Node    1, rank   15, thread   0, (affinity =  112)
</code></pre></div></p>
<h2 id="low-priority-access">Low priority access</h2>
<p>Low priority jobs are not charged against your allocation but will only run when
other, higher-priority, jobs cannot be run or there are no higher-priority jobs in
the queue. Although low priority jobs are not charged, you do need a valid, positive
budget to be able to submit and run low priority jobs, i.e. you need at least 1 CU
in your budget.</p>
<p>Low priority access is always available and has the following limits:</p>
<ul>
<li>256 node maximum job size</li>
<li>256 nodes maximum in use by any one user</li>
<li>512 nodes maximum in use by low priority at any one time</li>
<li>Maximum 4 low priority jobs in the queue per user</li>
<li>Maximum 1 low priority job running per user (of the 4 queued)</li>
<li>Maximum runtime of 3 hours</li>
</ul>
<p>You submit a low priority job on Tursa by using the <code>lowpriority</code> QoS. For example,
you would usually have the following line in your job submission script sbatch 
options:</p>
<div class="highlight"><pre><span></span><code>#SBATCH --qos=lowpriority
</code></pre></div>
<h2 id="reservations">Reservations</h2>
<p>Reservations are available on Tursa. These allow users to reserve a number of nodes
for a specified length of time starting at a particular time on the system.</p>
<p>Reservations require justification. They will only be approved if the request could not
be fulfilled with the standard queues. For instance, you require a job/jobs to run at a
particular time e.g. for a demonstration or course.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Reservation requests must be submitted at least 60 hours in advance of the reservation
start time. If requesting a reservation for a Monday at 18:00, please ensure this is
received by the Friday at 12:00 the latest. The same applies over Service Holidays.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Reservations are only valid for standard compute nodes, high memory compute nodes
and/or PP nodes cannot be included in reservations.</p>
</div>
<p>Reservations will be charged at 1.5 times the usual CU rate and our policy is that they
will be charged the full rate for the entire reservation at the time of booking, whether
or not you use the nodes for the full time. In addition, you will not be refunded the
CUs if you fail to use them due to a job crash unless this crash is due to a system failure.</p>
<div class="admonition bug">
<p class="admonition-title">Bug</p>
<p>At the moment, we are only able to charge for jobs in reservations, not for the full 
reservation itself. Jobs in reservations are charged at 1.5x the standard rate.</p>
</div>
<p>To request a reservation please <a href="mailto:support@tursa.ac.uk">contact the Tursa Service Desk</a>.
You need to provide the following:</p>
<ul>
<li>The start time and date of the reservation.</li>
<li>The end time and date of the reservation.</li>
<li>The project code for the reservation.</li>
<li>The number of nodes required.</li>
<li>Your justification for the reservation -- this must be provided or the request will be rejected.</li>
</ul>
<p>Your request will be checked by the Tursa User Administration team and, if approved, you will be provided a reservation ID which can be used on the system. To submit jobs to a reservation, you need to add <code>--reservation=&lt;reservation ID&gt;</code> to your job submission script or command.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>You must have at least 1 CU in the budget to submit a job on Tursa, even to a pre-paid reservation.</p>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>You can submit jobs to a reservation as soon as the reservation has been set up; jobs will remain queued until the reservation starts.</p>
</div>
<h2 id="best-practices-for-job-submission">Best practices for job submission</h2>
<p>This guidance is adapted from <a href="https://docs.nersc.gov/jobs/best-practices/">the advice provided by
NERSC</a></p>
<h3 id="time-limits">Time Limits</h3>
<p>Due to backfill scheduling, short and variable-length jobs generally
start quickly resulting in much better job throughput. You can specify a
minimum time for your job with the <code>--time-min</code> option to SBATCH:</p>
<pre><code>#SBATCH --time-min=&lt;lower_bound&gt;
#SBATCH --time=&lt;upper_bound&gt;
</code></pre>
<p>Within your job script, you can get the time remaining in the job with
<code>squeue -h -j ${Slurm_JOBID} -o %L</code> to allow you to deal with
potentially varying runtimes when using this option.</p>
<h3 id="long-running-jobs">Long Running Jobs</h3>
<p>Simulations which must run for a long period of time achieve the best
throughput when composed of many small jobs using a checkpoint and
restart method chained together (see above for how to chain jobs
together). However, this method does occur a startup and shutdown
overhead for each job as the state is saved and loaded so you should
experiment to find the best balance between runtime (long runtimes
minimise the checkpoint/restart overheads) and throughput (short
runtimes maximise throughput).</p>
<h3 id="io-performance">I/O performance</h3>
<h3 id="large-jobs">Large Jobs</h3>
<p>Large jobs may take longer to start up. The <code>sbcast</code> command is
recommended for large jobs requesting over 1500 MPI tasks. By default,
Slurm reads the executable on the allocated compute nodes from the
location where it is installed; this may take long time when the file
system (where the executable resides) is slow or busy. The <code>sbcast</code>
command, the executable can be copied to the <code>/tmp</code> directory on each of
the compute nodes. Since <code>/tmp</code> is part of the memory on the compute
nodes, it can speed up the job startup time.</p>
<div class="highlight"><pre><span></span><code>sbcast --compress=lz4 /path/to/exe /tmp/exe
srun /tmp/exe
</code></pre></div>
                
              
              
                


              
            </article>
          </div>
        </div>
        
          <a href="#" class="md-top md-icon" data-md-component="top" data-md-state="hidden">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"/></svg>
            Back to top
          </a>
        
      </main>
      
        
<footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        
        <a href="../sw-environment/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Software environment" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Software environment
            </div>
          </div>
        </a>
      
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
        
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.expand", "navigation.top"], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "search": "../../assets/javascripts/workers/search.709b4209.min.js", "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.2b46852b.min.js"></script>
      
    
  </body>
</html>
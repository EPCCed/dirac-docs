
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../sw-environment/">
      
      
        <link rel="next" href="../spack/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.44">
    
    
      
        <title>Running jobs on Tursa - DiRAC Extreme Scaling User Documentation</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.0253249f.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#running-jobs-on-tursa" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="DiRAC Extreme Scaling User Documentation" class="md-header__button md-logo" aria-label="DiRAC Extreme Scaling User Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            DiRAC Extreme Scaling User Documentation
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Running jobs on Tursa
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/EPCCed/dirac-docs.git" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    EPCCed/dirac-docs
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="DiRAC Extreme Scaling User Documentation" class="md-nav__button md-logo" aria-label="DiRAC Extreme Scaling User Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    DiRAC Extreme Scaling User Documentation
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/EPCCed/dirac-docs.git" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    EPCCed/dirac-docs
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Documentation overview
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Tursa User Guide
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Tursa User Guide
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Overview
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../hardware/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Hardware
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../connecting/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Connecting to Tursa
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../data/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data Management and transfer
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../sw-environment/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Software environment
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Running jobs on Tursa
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Running jobs on Tursa
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#resources" class="md-nav__link">
    <span class="md-ellipsis">
      Resources
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Resources">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gpuh" class="md-nav__link">
    <span class="md-ellipsis">
      GPUh
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#checking-available-budget" class="md-nav__link">
    <span class="md-ellipsis">
      Checking available budget
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#charging" class="md-nav__link">
    <span class="md-ellipsis">
      Charging
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#basic-slurm-commands" class="md-nav__link">
    <span class="md-ellipsis">
      Basic Slurm commands
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Basic Slurm commands">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sinfo-information-on-resources" class="md-nav__link">
    <span class="md-ellipsis">
      sinfo: information on resources
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbatch-submitting-jobs" class="md-nav__link">
    <span class="md-ellipsis">
      sbatch: submitting jobs
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#squeue-monitoring-jobs" class="md-nav__link">
    <span class="md-ellipsis">
      squeue: monitoring jobs
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scancel-deleting-jobs" class="md-nav__link">
    <span class="md-ellipsis">
      scancel: deleting jobs
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#resource-limits" class="md-nav__link">
    <span class="md-ellipsis">
      Resource Limits
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Resource Limits">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#primary-resource" class="md-nav__link">
    <span class="md-ellipsis">
      Primary resource
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#partitions" class="md-nav__link">
    <span class="md-ellipsis">
      Partitions
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quality-of-service-qos" class="md-nav__link">
    <span class="md-ellipsis">
      Quality of Service (QoS)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#priority" class="md-nav__link">
    <span class="md-ellipsis">
      Priority
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#troubleshooting" class="md-nav__link">
    <span class="md-ellipsis">
      Troubleshooting
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Troubleshooting">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slurm-error-messages" class="md-nav__link">
    <span class="md-ellipsis">
      Slurm error messages
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slurm-queued-reasons" class="md-nav__link">
    <span class="md-ellipsis">
      Slurm queued reasons
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#output-from-slurm-jobs" class="md-nav__link">
    <span class="md-ellipsis">
      Output from Slurm jobs
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#specifying-resources-in-job-scripts" class="md-nav__link">
    <span class="md-ellipsis">
      Specifying resources in job scripts
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Specifying resources in job scripts">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#resources-for-gpu-jobs" class="md-nav__link">
    <span class="md-ellipsis">
      Resources for GPU jobs
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Resources for GPU jobs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gpu-frequency" class="md-nav__link">
    <span class="md-ellipsis">
      GPU frequency
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#resources-for-cpu-jobs" class="md-nav__link">
    <span class="md-ellipsis">
      Resources for CPU jobs
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#srun-launching-parallel-jobs" class="md-nav__link">
    <span class="md-ellipsis">
      srun: Launching parallel jobs
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#example-job-submission-scripts" class="md-nav__link">
    <span class="md-ellipsis">
      Example job submission scripts
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Example job submission scripts">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#example-job-submission-script-for-a-parallel-gpu-job" class="md-nav__link">
    <span class="md-ellipsis">
      Example: job submission script for a parallel GPU job
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Example: job submission script for a parallel GPU job">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gpu_launchsh-wrapper-script" class="md-nav__link">
    <span class="md-ellipsis">
      gpu_launch.sh wrapper script
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-job-submission-script-for-a-parallel-cpu-job" class="md-nav__link">
    <span class="md-ellipsis">
      Example: job submission script for a parallel CPU job
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#using-the-dev-qos-for-short-gpu-jobs" class="md-nav__link">
    <span class="md-ellipsis">
      Using the dev QoS for short GPU jobs
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../spack/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Spack on Tursa
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#resources" class="md-nav__link">
    <span class="md-ellipsis">
      Resources
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Resources">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gpuh" class="md-nav__link">
    <span class="md-ellipsis">
      GPUh
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#checking-available-budget" class="md-nav__link">
    <span class="md-ellipsis">
      Checking available budget
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#charging" class="md-nav__link">
    <span class="md-ellipsis">
      Charging
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#basic-slurm-commands" class="md-nav__link">
    <span class="md-ellipsis">
      Basic Slurm commands
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Basic Slurm commands">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sinfo-information-on-resources" class="md-nav__link">
    <span class="md-ellipsis">
      sinfo: information on resources
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sbatch-submitting-jobs" class="md-nav__link">
    <span class="md-ellipsis">
      sbatch: submitting jobs
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#squeue-monitoring-jobs" class="md-nav__link">
    <span class="md-ellipsis">
      squeue: monitoring jobs
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scancel-deleting-jobs" class="md-nav__link">
    <span class="md-ellipsis">
      scancel: deleting jobs
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#resource-limits" class="md-nav__link">
    <span class="md-ellipsis">
      Resource Limits
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Resource Limits">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#primary-resource" class="md-nav__link">
    <span class="md-ellipsis">
      Primary resource
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#partitions" class="md-nav__link">
    <span class="md-ellipsis">
      Partitions
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quality-of-service-qos" class="md-nav__link">
    <span class="md-ellipsis">
      Quality of Service (QoS)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#priority" class="md-nav__link">
    <span class="md-ellipsis">
      Priority
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#troubleshooting" class="md-nav__link">
    <span class="md-ellipsis">
      Troubleshooting
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Troubleshooting">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slurm-error-messages" class="md-nav__link">
    <span class="md-ellipsis">
      Slurm error messages
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slurm-queued-reasons" class="md-nav__link">
    <span class="md-ellipsis">
      Slurm queued reasons
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#output-from-slurm-jobs" class="md-nav__link">
    <span class="md-ellipsis">
      Output from Slurm jobs
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#specifying-resources-in-job-scripts" class="md-nav__link">
    <span class="md-ellipsis">
      Specifying resources in job scripts
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Specifying resources in job scripts">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#resources-for-gpu-jobs" class="md-nav__link">
    <span class="md-ellipsis">
      Resources for GPU jobs
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Resources for GPU jobs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gpu-frequency" class="md-nav__link">
    <span class="md-ellipsis">
      GPU frequency
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#resources-for-cpu-jobs" class="md-nav__link">
    <span class="md-ellipsis">
      Resources for CPU jobs
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#srun-launching-parallel-jobs" class="md-nav__link">
    <span class="md-ellipsis">
      srun: Launching parallel jobs
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#example-job-submission-scripts" class="md-nav__link">
    <span class="md-ellipsis">
      Example job submission scripts
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Example job submission scripts">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#example-job-submission-script-for-a-parallel-gpu-job" class="md-nav__link">
    <span class="md-ellipsis">
      Example: job submission script for a parallel GPU job
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Example: job submission script for a parallel GPU job">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gpu_launchsh-wrapper-script" class="md-nav__link">
    <span class="md-ellipsis">
      gpu_launch.sh wrapper script
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-job-submission-script-for-a-parallel-cpu-job" class="md-nav__link">
    <span class="md-ellipsis">
      Example: job submission script for a parallel CPU job
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#using-the-dev-qos-for-short-gpu-jobs" class="md-nav__link">
    <span class="md-ellipsis">
      Using the dev QoS for short GPU jobs
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


<h1 id="running-jobs-on-tursa">Running jobs on Tursa</h1>
<p>As with most HPC services, Tursa uses a scheduler to manage access to
resources and ensure that the thousands of different users of system are
able to share the system and all get access to the resources they
require. Tursa uses the Slurm software to schedule jobs.</p>
<p>Writing a submission script is typically the most convenient way to
submit your job to the scheduler. Example submission scripts (with
explanations) for the most common job types are provided below.</p>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>If you have any questions on how to run jobs on Tursa do not hesitate
to contact the <a href="mailto:dirac-support@epcc.ed.ac.uk">DiRAC Service Desk</a>.</p>
</div>
<p>You typically interact with Slurm by issuing Slurm commands from the
login nodes (to submit, check and cancel jobs), and by specifying Slurm
directives that describe the resources required for your jobs in job
submission scripts.</p>
<h2 id="resources">Resources</h2>
<h3 id="gpuh">GPUh</h3>
<p>Time used on Tursa nodes is measured in GPUh.<br />
1 GPUh = 1 GPU for 1 hour. So a Tursa compute node with 4 GPUs would cost
4 GPUh per hour.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The minimum resource request on Tursa is one full node which is charged 
at a rate of 4 GPUh per hour.</p>
</div>
<h3 id="checking-available-budget">Checking available budget</h3>
<p>You can check in <a href="https://safe.epcc.ed.ac.uk">SAFE</a> by selecting <code>Login accounts</code> from the menu, select the login account you want to query.</p>
<p>Under <code>Login account details</code> you will see each of the budget codes you have access to listed e.g.
<code>dp123 resources</code> and then under Resource Pool to the right of this, a note of the remaining budgets. </p>
<p>When logged in to the machine you can also use the command </p>
<pre><code>sacctmgr show assoc where user=$LOGNAME format=account,user,maxtresmins%75
</code></pre>
<p>This will list all the budget codes that you have access to e.g.</p>
<div class="highlight"><pre><span></span><code>Account       User                                                                 MaxTRESMins 
---------- ---------- --------------------------------------------------------------------------- 
       t01   dc-user1                           gres/cpu-low=0,gres/cpu-standard=0,gres/gpu-low=0 
       z01   dc-user1   
</code></pre></div>
<p>This shows that <code>dc-user1</code> is a member of budgets <code>t01</code> and <code>z01</code>.  However, the <code>gres/cpu-low=0,gres/cpu-standard=0,gres/gpu-low=0</code> indicates that the <code>t01</code> budget can only run GPU jobs in standard (charged) partitions (all other options are disabled, indicated by <code>=0</code> for CPU standard, CPU low and GPU low).  This user can also submit jobs to any partition using the <code>z01</code> budget.</p>
<p>To see the number of coreh or GPUh remaining you must check in <a href="https://safe.epcc.ed.ac.uk/dirac">SAFE</a>.</p>
<h3 id="charging">Charging</h3>
<p>Jobs run on Tursa are charged for the time they use i.e. from the time the job begins to run until the time the job ends (not the full wall time requested).</p>
<p>Jobs are charged for the full number of nodes which are requested, even if they are not all used.</p>
<p>Charging takes place at the time the job ends, and the job is charged in full to the budget which is live at the end time.</p>
<h2 id="basic-slurm-commands">Basic Slurm commands</h2>
<p>There are three key commands used to interact with the Slurm on the
command line:</p>
<ul>
<li><code>sinfo</code> - Get information on the partitions and resources available</li>
<li><code>sbatch jobscript.slurm</code> - Submit a job submission script (in this
    case called: <code>jobscript.slurm</code>) to the scheduler</li>
<li><code>squeue</code> - Get the current status of jobs submitted to the scheduler</li>
<li><code>scancel 12345</code> - Cancel a job (in this case with the job ID
    <code>12345</code>)</li>
</ul>
<p>We cover each of these commands in more detail below.</p>
<h3 id="sinfo-information-on-resources"><code>sinfo</code>: information on resources</h3>
<p><code>sinfo</code> is used to query information about available resources and
partitions. Without any options, <code>sinfo</code> lists the status of all
resources and partitions, e.g.</p>
<div class="highlight"><pre><span></span><code>[dc-user1@tursa-login1 ~]$ sinfo 

PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
cpu          up 2-00:00:00      4  alloc tu-c0r0n[66-69]
cpu          up 2-00:00:00      2   idle tu-c0r0n[70-71]
gpu          up 2-00:00:00      1   plnd tu-c0r2n93
gpu          up 2-00:00:00     11  drain tu-c0r0n75,tu-c0r5n[48,51,54,57],tu-c0r6n[48,51,54,57],tu-c0r7n[00,48]
gpu          up 2-00:00:00    112    mix tu-c0r0n[00,03,06,09,12,15,18,21,24,27,30,33,36,39,42,45,72,87,90],tu-c0r1n[00,03,06,09,12,15,18,21,24,27,30,33,60,63,66,69,72,75,78,81,84,87,90,93],tu-c0r2n[00,03,06,09,12,15,18,21,24,27,30,33,60,63,66,69,72,75,78,81,84,87,90],tu-c0r3n[00,03,06,09,12,15,18,21,24,27,30,33,60,63,66,69,72,75,78,81,84,90,93],tu-c0r4n[00,03,06,09,12,15,18,21,24,27,30,33,60,63,66,69,72,75,81,84,87,90,93]
gpu          up 2-00:00:00     56   resv tu-c0r0n93,tu-c0r4n78,tu-c0r5n[00,03,06,09,12,15,18,21,24,27,30,33,36,39,42,45],tu-c0r6n[00,03,06,09,12,15,18,21,24,27,30,33,36,39,42,45,60,63,66,69],tu-c0r7n[03,06,09,12,15,18,21,24,27,30,33,36,39,42,45,51,54,57]
gpu          up 2-00:00:00      1   idle tu-c0r3n87
</code></pre></div>
<ul>
<li><code>alloc</code> nodes are those that are running jobs</li>
<li><code>idle</code> nodes are empty</li>
<li><code>drain</code>, <code>down</code>, <code>maint</code> nodes are unavailable to users</li>
<li><code>plnd</code> nodes are reserved for future jobs</li>
</ul>
<h3 id="sbatch-submitting-jobs"><code>sbatch</code>: submitting jobs</h3>
<p><code>sbatch</code> is used to submit a job script to the job submission system.
The script will typically contain one or more <code>mpirun</code> commands to launch
parallel tasks.</p>
<p>When you submit the job, the scheduler provides the job ID, which is
used to identify this job in other Slurm commands and when looking at
resource usage in SAFE.</p>
<pre><code>sbatch test-job.slurm
Submitted batch job 12345
</code></pre>
<h3 id="squeue-monitoring-jobs"><code>squeue</code>: monitoring jobs</h3>
<p><code>squeue</code> without any options or arguments shows the current status of
all jobs known to the scheduler. For example:</p>
<pre><code>squeue
</code></pre>
<p>will list all jobs on Tursa.</p>
<p>The output of this is often large. You can restrict the
output to just your jobs by adding the <code>--me</code> option:</p>
<pre><code>squeue --me
</code></pre>
<h3 id="scancel-deleting-jobs"><code>scancel</code>: deleting jobs</h3>
<p><code>scancel</code> is used to delete a jobs from the scheduler. If the job is
waiting to run it is simply cancelled, if it is a running job then it is
stopped immediately. You need to provide the job ID of the job you wish
to cancel/stop. For example:</p>
<pre><code>scancel 12345
</code></pre>
<p>will cancel (if waiting) or stop (if running) the job with ID <code>12345</code>.</p>
<h2 id="resource-limits">Resource Limits</h2>
<p>The Tursa resource limits for any given job are covered by three
separate attributes.</p>
<ul>
<li>The amount of <em>primary resource</em> you require, i.e., number of
    compute nodes.</li>
<li>The <em>partition</em> that you want to use - this specifies the nodes that
    are eligible to run your job.</li>
<li>The <em>Quality of Service (QoS)</em> that you want to use - this specifies
    the job limits that apply.</li>
</ul>
<h3 id="primary-resource">Primary resource</h3>
<p>The <em>primary resource</em> you can request for your job is the compute node.</p>
<div class="admonition information">
<p class="admonition-title">Information</p>
<p>The <code>--exclusive</code> option is enforced on Tursa which means you will
always have access to all of the memory on the compute node regardless
of how many processes are actually running on the node.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You will not generally have access to the full amount of memory resource
on the the node as some is retained for running the operating system and
other system processes.</p>
</div>
<h3 id="partitions">Partitions</h3>
<p>On Tursa, compute nodes are grouped into partitions. You will have to
specify a partition using the <code>--partition</code> option in your Slurm
submission script. The following table has a list of active partitions
on Tursa.</p>
<table>
<thead>
<tr>
<th>Partition</th>
<th>Description</th>
<th>Max nodes available</th>
</tr>
</thead>
<tbody>
<tr>
<td>cpu</td>
<td>CPU nodes with 2 AMD EPYC 64-core processor</td>
<td>6</td>
</tr>
<tr>
<td>gpu</td>
<td>GPU nodes with 2 AMD EPYC processor (16-core or 24-core) and NVIDIA A100 GPU &times; 4 (this includes both A100-40 and A100-80 GPU)</td>
<td>181</td>
</tr>
<tr>
<td>gpu-a100-40</td>
<td>GPU nodes with 2 AMD EPYC 16-core processors and NVIDIA A100-40 GPU &times; 4</td>
<td>114</td>
</tr>
<tr>
<td>gpu-a100-80</td>
<td>GPU nodes with 2 AMD EPYC 24-core processor (3 nodes have 2 AMD EPYC 16-core processors) and NVIDIA A100-80 GPU &times; 4</td>
<td>67</td>
</tr>
</tbody>
</table>
<p>You can list the active partitions by running <code>sinfo</code>.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>You may not have access to all the available partitions.</p>
</div>
<h3 id="quality-of-service-qos">Quality of Service (QoS)</h3>
<p>On Tursa, job limits are defined by the requested Quality of Service
(QoS), as specified by the <code>--qos</code> Slurm directive. The following table
lists the active QoS on Tursa.</p>
<table>
<thead>
<tr>
<th>QoS</th>
<th>Max Nodes Per Job</th>
<th>Max Walltime</th>
<th>Queued</th>
<th>Running</th>
<th>Partition(s)</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>standard</td>
<td>128</td>
<td>48 hrs</td>
<td>Max. 128 jobs per user</td>
<td>Max. 128 nodes per user, max. 32 jobs per user</td>
<td>gpu, gpu-a100-40, gpu-a100-80, cpu</td>
<td>Only jobs sizes that are powers of 2 nodes are allowed (i.e. 1, 2, 4, 8, 16, 32 nodes), only available when your budget is positive.</td>
</tr>
<tr>
<td>low</td>
<td>32</td>
<td>24 hrs</td>
<td>4</td>
<td>4</td>
<td>gpu, gpu-a100-40, gpu-a100-40, cpu</td>
<td>Only jobs sizes that are powers of 2 nodes are allowed (i.e. 1, 2, 4, 8, 16, 32 nodes), only available when your budget is zero or negative</td>
</tr>
<tr>
<td>high</td>
<td>128</td>
<td>48 hrs</td>
<td>Max. 128 jobs per user</td>
<td>Max. 128 nodes per user, max. 32 jobs per user</td>
<td>gpu, gpu-a100-40, gpu-a100-80</td>
<td>Only jobs sizes that are powers of 2 nodes are allowed (i.e. 1, 2, 4, 8, 16, 32 nodes), only available when you have access to "dpXYZ-high" budget and the budget is positive. Only available to RAC projects. High priority jobs are prioritised above other jobs on the system.</td>
</tr>
<tr>
<td>dev</td>
<td>2</td>
<td>4 hrs</td>
<td>2</td>
<td>1</td>
<td>gpu</td>
<td>For faster turnaround for development jobs and interactive sessions, only available when your budget is positive. The dev QoS includes 2x A100-40 GPU nodes and 3x A100-80 GPU nodes.</td>
</tr>
</tbody>
</table>
<p>You can find out the QoS that you can use by running the following
command:</p>
<pre><code>sacctmgr show assoc user=$USER cluster=tursa format=cluster,account,user,qos%50
</code></pre>
<p>As long as you have a positive budget, you should use the <code>standard</code> QoS. Once you have exhausted your
budget you can use the <code>low</code> QoS to continue to run jobs at a lower priority than jobs in the 
<code>standard</code> QoS.</p>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>If you have needs which do not fit within the current QoS, please
<a href="https://www.archer2.ac.uk/support-access/servicedesk.html">contact the Service
Desk</a> and we
can discuss how to accommodate your requirements.</p>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Only jobs sizes that are powers of 2 nodes
are allowed. i.e. 1, 2, 4, 8, 16, 32 nodes on the <code>gpu</code> partition and 
1, 2, 4 nodes on the <code>cpu</code> partition. There is a discussion of why this is enforced in
the <a href="../hardware/">Hardware</a> section of the User Guide.</p>
</div>
<h3 id="priority">Priority</h3>
<p>Job priority on Tursa depends on a number of different factors:</p>
<ul>
<li>The QoS your job has specified</li>
<li>The amount of time you have been queuing for</li>
<li>Your current fairshare factor</li>
</ul>
<p>Each of these factors is normalised to a value between 0 and 1, is multiplied
with a weight and the resulting values combined to produce a priority for the job. 
The current job priority formula on Tursa is:</p>
<div class="highlight"><pre><span></span><code>Priority = [10000 * P(QoS)] + [500 * P(Age)] + [300 * P(Fairshare)]
</code></pre></div>
<p>The priority factors are:</p>
<ul>
<li>P(QoS) - The QoS priority normalised to a value between 0 and 1. The maximum raw
  value is 10000 and the minimum is 0. <code>standard</code> QoS has a value of 5000 and <code>low</code>
  QoS a value of 1.</li>
<li>P(Age) - The priority based on the job age normalised to a value between 0 and 1.
  The maximum raw value is 14 days (where P(Age) = 1).</li>
<li>P(Fairshare) - The fairshare priority normalised to a value between 0 and 1. Your
  fairshare priority is determined by a combination of your budget code fairshare 
  value and your user fairshare value within that budget code. The more use that 
  the budget code you are using has made of the system recently relative to other 
  budget codes on the system, the lower the budget code fairshare value will be; and the more
  use you have made of the system recently relative to other users within your
  budget code, the lower your user fairshare value will be. The decay half life 
  for fairshare on Tursa is set to 14 days. <a href="https://slurm.schedmd.com/fair_tree.html">More information on the Slurm fairshare
  algorithm</a>.</li>
</ul>
<p>You can view the priorities for current queued jobs on the system with the <code>sprio</code>
command:</p>
<div class="highlight"><pre><span></span><code>[dc-user1@tursa-login1 ~]$ sprio 
          JOBID PARTITION   PRIORITY       SITE        AGE  FAIRSHARE        QOS
          43963 gpu             5055          0         51          5       5000
          43975 gpu             5061          0         41         20       5000
          43976 gpu             5061          0         41         20       5000
          43982 gpu             5046          0         26         20       5000
          43986 gpu             5011          0          6          5       5000
          43996 gpu             5020          0          0         20       5000
          43997 gpu             5020          0          0         20       5000
</code></pre></div>
<h2 id="troubleshooting">Troubleshooting</h2>
<h3 id="slurm-error-messages">Slurm error messages</h3>
<p>An incorrect submission will cause Slurm to return an error.
Some common problems are listed below, with a suggestion about
the likely cause:</p>
<ul>
<li>
<p><code>sbatch: unrecognized option &lt;text&gt;</code></p>
<p>One of your options is invalid or has a typo. <code>man sbatch</code> to help.</p>
</li>
<li>
<p><code>error: Batch job submission failed: No partition specified or system default partition</code></p>
<p>A <code>--partition=</code> option is missing. You must specify the partition
(see the list above). This is most often <code>--partition=standard</code>.</p>
</li>
<li>
<p><code>error: invalid partition specified: &lt;partition&gt;</code></p>
<p><code>error: Batch job submission failed: Invalid partition name specified</code></p>
<p>Check the partition exists and check the spelling is correct.</p>
</li>
<li>
<p><code>error: Batch job submission failed: Invalid account or account/partition combination specified</code></p>
<p>This probably means an invalid account has been given. Check the
<code>--account=</code> options against valid accounts in SAFE.</p>
</li>
<li>
<p><code>error: Batch job submission failed: Invalid qos specification</code></p>
<p>A QoS option is either missing or invalid. Check the script has a
<code>--qos=</code> option and that the option is a valid one from the
table above. (Check the spelling of the QoS is correct.)</p>
</li>
<li>
<p><code>error: Your job has no time specification (--time=)...</code></p>
<p>Add an option of the form <code>--time=hours:minutes:seconds</code> to the
submission script. E.g., <code>--time=01:30:00</code> gives a time limit of
90 minutes.</p>
</li>
<li>
<p><code>error: QOSMaxWallDurationPerJobLimit</code>
    <code>error: Batch job submission failed: Job violates accounting/QOS policy</code>
    <code>(job submit limit, user's size and/or time limits)</code></p>
<p>The script has probably specified a time limit which is too long for
the corresponding QoS. E.g., the time limit for the short QoS
is 20 minutes.</p>
</li>
</ul>
<h3 id="slurm-queued-reasons">Slurm queued reasons</h3>
<p>The <code>squeue</code> command allows users to view information for jobs managed by Slurm. Jobs
typically go through the following states: PENDING, RUNNING, COMPLETING, and COMPLETED.
The first table provides a description of some job state codes. The second table provides a description
of the reasons that cause a job to be in a state.</p>
<table>
<thead>
<tr>
<th>Status</th>
<th>Code</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>PENDING</td>
<td>PD</td>
<td>Job is awaiting resource allocation.</td>
</tr>
<tr>
<td>RUNNING</td>
<td>R</td>
<td>Job currently has an allocation.</td>
</tr>
<tr>
<td>SUSPENDED</td>
<td>S</td>
<td>Job currently has an allocation.</td>
</tr>
<tr>
<td>COMPLETING</td>
<td>CG</td>
<td>Job is in the process of completing. Some processes on some nodes may still be active.</td>
</tr>
<tr>
<td>COMPLETED</td>
<td>CD</td>
<td>Job has terminated all processes on all nodes with an exit code of zero.</td>
</tr>
<tr>
<td>TIMEOUT</td>
<td>TO</td>
<td>Job terminated upon reaching its time limit.</td>
</tr>
<tr>
<td>STOPPED</td>
<td>ST</td>
<td>Job has an allocation, but execution has been stopped with SIGSTOP signal. CPUS have been retained by this job.</td>
</tr>
<tr>
<td>OUT_OF_MEMORY</td>
<td>OOM</td>
<td>Job experienced out of memory error.</td>
</tr>
<tr>
<td>FAILED</td>
<td>F</td>
<td>Job terminated with non-zero exit code or other failure condition.</td>
</tr>
<tr>
<td>NODE_FAIL</td>
<td>NF</td>
<td>Job terminated due to failure of one or more allocated nodes.</td>
</tr>
<tr>
<td>CANCELLED</td>
<td>CA</td>
<td>Job was explicitly cancelled by the user or system administrator. The job may or may not have been initiated.</td>
</tr>
</tbody>
</table>
<p>For a full list of see <a href="https://slurm.schedmd.com/squeue.html#lbAG">Job State Codes</a>.</p>
<table>
<thead>
<tr>
<th>Reason</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Priority</td>
<td>One or more higher priority jobs exist for this partition or advanced reservation.</td>
</tr>
<tr>
<td>Resources</td>
<td>The job is waiting for resources to become available.</td>
</tr>
<tr>
<td>BadConstraints</td>
<td>The job's constraints can not be satisfied.</td>
</tr>
<tr>
<td>BeginTime</td>
<td>The job's earliest start time has not yet been reached.</td>
</tr>
<tr>
<td>Dependency</td>
<td>This job is waiting for a dependent job to complete.</td>
</tr>
<tr>
<td>Licenses</td>
<td>The job is waiting for a license.</td>
</tr>
<tr>
<td>WaitingForScheduling</td>
<td>No reason has been set for this job yet. Waiting for the scheduler to determine the appropriate reason.</td>
</tr>
<tr>
<td>Prolog</td>
<td>Its PrologSlurmctld program is still running.</td>
</tr>
<tr>
<td>JobHeldAdmin</td>
<td>The job is held by a system administrator.</td>
</tr>
<tr>
<td>JobHeldUser</td>
<td>The job is held by the user.</td>
</tr>
<tr>
<td>JobLaunchFailure</td>
<td>The job could not be launched. This may be due to a file system problem, invalid program name, etc.</td>
</tr>
<tr>
<td>NonZeroExitCode</td>
<td>The job terminated with a non-zero exit code.</td>
</tr>
<tr>
<td>InvalidAccount</td>
<td>The job's account is invalid.</td>
</tr>
<tr>
<td>InvalidQOS</td>
<td>The job's QOS is invalid.</td>
</tr>
<tr>
<td>QOSUsageThreshold</td>
<td>Required QOS threshold has been breached.</td>
</tr>
<tr>
<td>QOSJobLimit</td>
<td>The job's QOS has reached its maximum job count.</td>
</tr>
<tr>
<td>QOSResourceLimit</td>
<td>The job's QOS has reached some resource limit.</td>
</tr>
<tr>
<td>QOSTimeLimit</td>
<td>The job's QOS has reached its time limit.</td>
</tr>
<tr>
<td>NodeDown</td>
<td>A node required by the job is down.</td>
</tr>
<tr>
<td>TimeLimit</td>
<td>The job exhausted its time limit.</td>
</tr>
<tr>
<td>ReqNodeNotAvail</td>
<td>Some node specifically required by the job is not currently available. The node may currently be in use, reserved for another job, in an advanced reservation, DOWN, DRAINED, or not responding. Nodes which are DOWN, DRAINED, or not responding will be identified as part of the job's "reason" field as "UnavailableNodes". Such nodes will typically require the intervention of a system administrator to make available.</td>
</tr>
</tbody>
</table>
<p>For a full list of see <a href="https://slurm.schedmd.com/squeue.html#lbAF">Job Reasons</a>.</p>
<h2 id="output-from-slurm-jobs">Output from Slurm jobs</h2>
<p>Slurm places standard output (STDOUT) and standard error (STDERR) for
each job in the file <code>slurm_&lt;JobID&gt;.out</code>. This file appears in the job's
working directory once your job starts running.</p>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>Output may be buffered - to enable live output, e.g. for monitoring
job status, add <code>--unbuffered</code> to the <code>srun</code> command in your SLURM
script.</p>
</div>
<h2 id="specifying-resources-in-job-scripts">Specifying resources in job scripts</h2>
<p>You specify the resources you require for your job using directives at
the top of your job submission script using lines that start with the
directive <code>#SBATCH</code>.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>You should always ask for the full node resources in your sbatch options
with <code>tasks-per-node</code> equal to the number of CPU cores on the node and
<code>cpus-per-task</code> equal to 1 and then specify the process and thread 
pinning using <code>srun</code> options. You cannot specify process/thread pinning
options in <code>sbatch</code> options - if you try to do so you will run into
problems when launching the parallel executable using <code>srun</code>.</p>
</div>
<p>If you do not specify any options, then the default for each option will
be applied. As a minimum, all job submissions must specify the budget
that they wish to charge the job too with the option:</p>
<ul>
<li><code>--account=&lt;budgetID&gt;</code> your budget ID is usually something like
     <code>t01</code> or <code>t01-test</code>. You can see which budget codes you can charge
     to in SAFE.</li>
</ul>
<p>Other common options that are used are:</p>
<ul>
<li><code>--time=&lt;hh:mm:ss&gt;</code> the maximum walltime for your job. <em>e.g.</em> For
     a 6.5 hour walltime, you would use <code>--time=6:30:0</code>.</li>
<li><code>--job-name=&lt;jobname&gt;</code> set a name for the job to help identify it
     in</li>
</ul>
<p>To prevent the behaviour of batch scripts being dependent on the user
environment at the point of submission, the option</p>
<ul>
<li><code>--export=none</code> prevents the user environment from being exported
     to the batch system.</li>
</ul>
<p>Using the <code>--export=none</code> means that the behaviour of batch submissions
should be repeatable. We strongly recommend its use, although see
<a href="./#using-modules-in-the-batch-system-the-epcc-job-env-module">the following section</a>
to enable access to the usual modules.</p>
<h3 id="resources-for-gpu-jobs">Resources for GPU jobs</h3>
<p>In addition, parallel GPU jobs will also need to specify how many nodes,
parallel processes and threads they require.</p>
<ul>
<li><code>--nodes=&lt;nodes&gt;</code> the number of nodes to use for the job.</li>
<li><code>--tasks-per-node=&lt;processes per node&gt;</code> this should be set to either
      <code>32</code> (A100-40 or unspecified nodes) or <code>48</code> (A100-80 nodes)</li>
<li><code>--cpus-per-task=1</code> this should always be set to <code>1</code> </li>
<li><code>--gres=gpu:4</code> the number of GPU to use per node. This will almost always
     be 4 to use all GPUs on a node.</li>
</ul>
<p>If you are happy to have any GPU type for your job (A100-40 or A100-80) then you
select the <code>gpu</code> partition:</p>
<ul>
<li><code>--partition=gpu</code></li>
</ul>
<p>If you wish to use just the A100-80 GPU nodes which have higher memory, you add the
following option:</p>
<ul>
<li><code>--partition=gpu-a100-80</code> request the job is placed on nodes with high-memory
   (80 GB) GPUs with 48 cores per node - there are 64 high memory GPU nodes on the system. </li>
</ul>
<p>To just use the A100-40 GPU nodes:</p>
<ul>
<li><code>--partition=gpu-a100-40</code> request the job is placed on nodes with standard memory
   (40 GB) GPUs with 32 cores per node.</li>
</ul>
<p>If you do not specfy a partition, the scheduler may use any available node types for 
the job (equivalent of <code>--partition=gpu</code>).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For parallel jobs, Tursa operates in a <em>node exclusive</em> way. This
means that you are assigned resources in the units of full compute nodes
for your jobs (<em>i.e.</em> 32 cores and 4 GPU on GPU A100-40 nodes, 48 cores and 4 GPU on A100-80 nodes)
and that no other user can share those compute nodes with you. Hence,
the minimum amount of resource you can request for a parallel job is 1 node
(or 32 cores and 4 GPU on GPU A100-40 nodes, 48 cores and 4 GPU on A100-80 nodes).</p>
</div>
<h4 id="gpu-frequency">GPU frequency</h4>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>The default GPU frequency on Tursa compute nodes was changed from 1410 MHz
to 1040 MHz on Thursday 15 Dec 2022 to improve the energy efficiency of the
service.</p>
</div>
<p>Users can control the GPU frequency in their job submission scripts:</p>
<ul>
<li><code>--gpu-freq=&lt;desired GPU freq in MHz&gt;</code> allows users to set the GPU frequency 
     on a per job basis. The frequency can be set in the range 210 - 1410 MHz in steps
     of 15 MHz.</li>
</ul>
<div class="admonition bug">
<p class="admonition-title">Bug</p>
<p>When setting the GPU frequency you will see an error in the output from the job 
that says <code>control disabled</code>. This is an incorrect message due to an issue with 
how Slurm sets the GPU frequency and can be safely ignored.</p>
</div>
<h3 id="resources-for-cpu-jobs">Resources for CPU jobs</h3>
<p>Parallel CPU node jobs are specified in a similar way to GPU jobs</p>
<ul>
<li><code>--nodes=&lt;nodes&gt;</code> the number of nodes to use for the job.</li>
<li><code>--tasks-per-node=128</code> this should always be set to <code>128</code></li>
<li><code>--cpus-per-task=1</code> this should always be set to <code>1</code> </li>
<li><code>--partition=cpu</code> this will always be set to <code>cpu</code> for CPU jobs</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For parallel jobs, Tursa operates in a <em>node exclusive</em> way. This
means that you are assigned resources in the units of full compute nodes
for your jobs (<em>i.e.</em> 128 cores on CPU nodes)
and that no other user can share those compute nodes with you. Hence,
the minimum amount of resource you can request for a parallel job is 1 node
(128 cores on CPU nodes).</p>
</div>
<h2 id="srun-launching-parallel-jobs"><code>srun</code>: Launching parallel jobs</h2>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Only OpenMPI 4.1.5 and later versions of OpenMPI support parallel job lauch
using <code>srun</code> on Tursa. If you try to use older versions of OpenMPI via modules
on Tursa you will see errors when using <code>srun</code>. The old modules are only kept
for backwards compatibility, you should always compile and run software using
OpenMPI 4.1.5 or newer if possible.</p>
</div>
<p>If you are running parallel jobs, your job submission script should contain one or
more srun commands to launch the parallel executable across the compute nodes. In
most cases you will want to add the following options to <code>srun</code>:</p>
<ul>
<li><code>--nodes=[number of nodes]</code> - Set the number of compute nodes for this job step</li>
<li><code>--tasks-per-node=[MPI processes per node]</code> - This will usually be <code>4</code> for GPU jobs 
  as you usually have 1 MPI process per GPU</li>
<li><code>--cpus-per-task=[stride between MPI processes]</code> - This will usually be either <code>8</code>
   (for A100-40 nodes) or <code>12</code> (for A100-80 nodes). If you are using the <code>gpu</code> QoS
   where you can get any type of GPU node, you will usually se this to <code>8</code>.</li>
<li><code>--hint=nomultithread</code> - do not use hyperthreads/SMP</li>
<li><code>--distribution=block:block</code>- the first <code>block</code> means use a block distribution
   of processes across nodes (i.e. fill nodes before moving onto the next one) and
   the second <code>block</code> means use a block distribution of processes across "sockets"
   within a node (i.e. fill a "socket" before moving on to the next one).</li>
</ul>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>The Slurm definition of a "socket" does not usually correspond to a physical CPU socket.
On Tursa GPU nodes it corresponds to half the cores on a socket as the GPU nodes
are configured with NPS2.</p>
<p>On the Tursa CPU nodes, the Slurm definition of a socket does correspond to a physical
CPU socket (64 cores) as the CPU nodes are configured with NPS1.</p>
</div>
<h2 id="example-job-submission-scripts">Example job submission scripts</h2>
<p>The typical strategy for submitting josb on Tursa is for the batch script to 
request full nodes with no process/thread pinning and then the individual 
<code>srun</code> commands set the correct options for dividing up processes and threads
across nodes.</p>
<h3 id="example-job-submission-script-for-a-parallel-gpu-job">Example: job submission script for a parallel GPU job</h3>
<p>A job submission script for a parallel job that uses 4 compute nodes, 4 MPI
processes per node and 4 GPUs per node. It does not restrict what type of
GPU the job can run on so both A100-40 and A100-80 can be used.</p>
<div class="highlight"><pre><span></span><code><span class="ch">#!/bin/bash</span>

<span class="c1"># Slurm job options</span>
<span class="kp">#SBATCH --job-name=Example_GPU_MPI_job</span>
<span class="kp">#SBATCH --time=12:0:0</span>
<span class="kp">#SBATCH --partition=gpu</span>
<span class="kp">#SBATCH --qos=standard</span>
<span class="c1"># Replace [budget code] below with your budget code (e.g. t01)</span>
<span class="kp">#SBATCH --account=[budget code]  </span>

<span class="c1"># Request right number of full nodes (32 cores by node fits any GPU compute nodes))</span>
<span class="kp">#SBATCH --nodes=4</span>
<span class="kp">#SBATCH --ntasks-per-node=32</span>
<span class="kp">#SBATCH --cpus-per-task=1</span>
<span class="kp">#SBATCH --gres=gpu:4</span>

<span class="c1"># Load the correct modules</span>
module<span class="w"> </span>load<span class="w"> </span>gcc/9.3.0
module<span class="w"> </span>load<span class="w"> </span>cuda/12.3
module<span class="w"> </span>load<span class="w"> </span>openmpi/4.1.5-cuda12.3<span class="w"> </span>

<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">8</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_PLACES</span><span class="o">=</span>cores

<span class="c1"># These will need to be changed to match the actual application you are running</span>
<span class="nv">application</span><span class="o">=</span><span class="s2">&quot;my_mpi_openmp_app.x&quot;</span>
<span class="nv">options</span><span class="o">=</span><span class="s2">&quot;arg 1 arg2&quot;</span>

<span class="c1"># We have reserved the full nodes, now distribute the processes as</span>
<span class="c1"># required: 4 MPI processes per node, stride of 8 cores between </span>
<span class="c1"># MPI processes</span>
<span class="c1"># </span>
<span class="c1"># Note use of gpu_launch.sh wrapper script for GPU and NIC pinning </span>
<span class="nb">srun</span><span class="w"> </span>--nodes<span class="o">=</span><span class="m">4</span><span class="w"> </span>--ntasks-per-node<span class="o">=</span><span class="m">4</span><span class="w"> </span>--cpus-per-task<span class="o">=</span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--hint<span class="o">=</span>nomultithread<span class="w"> </span>--distribution<span class="o">=</span>block:block<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>gpu_launch.sh<span class="w"> </span><span class="se">\</span>
<span class="w">     </span><span class="si">${</span><span class="nv">application</span><span class="si">}</span><span class="w"> </span><span class="si">${</span><span class="nv">options</span><span class="si">}</span>
</code></pre></div>
<p>This will run your executable "my_mpi_opnemp_app.x" in parallel usimg 16
MPI processes on 4 nodes. 4 GPUs will be used per node.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>You must use the <code>gpu_launch.sh</code> wrapper script to get the correct biniding
of GPU to MPI processes and of network interface to GPU and MPI process.
This script is described in more detail below.</p>
</div>
<h4 id="gpu_launchsh-wrapper-script"><code>gpu_launch.sh</code> wrapper script</h4>
<p>The <code>gpu_launch.sh</code> wrapper script is required to set the correct binding of
GPU to MPI processes and the correct binding of interconnect interfaces to 
MPI process and GPU. We provide this centrally for convenience but its contents
are simple:</p>
<div class="highlight"><pre><span></span><code>#!/bin/bash

# Compute the raw process ID for binding to GPU and NIC
lrank=$((SLURM_PROCID % SLURM_NTASKS_PER_NODE))

# Bind the process to the correct GPU and NIC
export CUDA_VISIBLE_DEVICES=${lrank}
export UCX_NET_DEVICES=mlx5_${lrank}:1

$@
</code></pre></div>
<h3 id="example-job-submission-script-for-a-parallel-cpu-job">Example: job submission script for a parallel CPU job</h3>
<p>A job submission script for a parallel job that uses 1 compute node, 128 MPI
processes per node.</p>
<div class="highlight"><pre><span></span><code><span class="ch">#!/bin/bash</span>

<span class="c1"># Slurm job options</span>
<span class="kp">#SBATCH --job-name=Example_CPU_MPI_job</span>
<span class="kp">#SBATCH --time=1:0:0</span>
<span class="kp">#SBATCH --partition=cpu</span>
<span class="kp">#SBATCH --qos=standard</span>
<span class="c1"># Replace [budget code] below with your budget code (e.g. t01)</span>
<span class="kp">#SBATCH --account=[budget code]  </span>

<span class="c1"># Request right number of full nodes (32 cores by node fits any GPU compute nodes))</span>
<span class="kp">#SBATCH --nodes=1</span>
<span class="kp">#SBATCH --ntasks-per-node=128</span>
<span class="kp">#SBATCH --cpus-per-task=1</span>

<span class="c1"># Load the correct modules</span>
module<span class="w"> </span>load<span class="w"> </span>gcc/9.3.0
module<span class="w"> </span>load<span class="w"> </span>openmpi/4.1.5

<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">1</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_PLACES</span><span class="o">=</span>cores

<span class="c1"># These will need to be changed to match the actual application you are running</span>
<span class="nv">application</span><span class="o">=</span><span class="s2">&quot;my_mpi_app.x&quot;</span>
<span class="nv">options</span><span class="o">=</span><span class="s2">&quot;arg 1 arg2&quot;</span>

<span class="c1"># We have reserved the full nodes, now distribute the processes as</span>
<span class="c1"># required: 128 MPI processes per node</span>
<span class="nb">srun</span><span class="w"> </span>--nodes<span class="o">=</span><span class="m">1</span><span class="w"> </span>--ntasks-per-node<span class="o">=</span><span class="m">128</span><span class="w"> </span>--cpus-per-task<span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--hint<span class="o">=</span>nomultithread<span class="w"> </span>--distribution<span class="o">=</span>block:block<span class="w"> </span><span class="se">\</span>
<span class="w">     </span><span class="si">${</span><span class="nv">application</span><span class="si">}</span><span class="w"> </span><span class="si">${</span><span class="nv">options</span><span class="si">}</span>
</code></pre></div>
<p>This will run your executable "my_mpi_app.x" in parallel usimg 128
MPI processes on 1 node.</p>
<h2 id="using-the-dev-qos-for-short-gpu-jobs">Using the <code>dev</code> QoS for short GPU jobs</h2>
<p>The <code>dev</code> QoS is designed for faster turnaround of short GPU jobs than is usually available through
the production QoS. It is subject to a number of restrictions:</p>
<ul>
<li>4 hour maximum walltime</li>
<li>Maximum job size:<ul>
<li>2 nodes for <code>gpu-a100-80</code> partition</li>
<li>1 node for <code>gpu-a100-40</code> partition</li>
</ul>
</li>
<li>Maximum 1 job running per user</li>
<li>Maximum 2 jobs queued per user</li>
<li>Only available to projects with a positive budget</li>
</ul>
<p>In addtion, you <em>must</em> specify either the <code>gpu-a100-80</code> or <code>gpu-a100-40</code> partitions when using the
<code>dev</code> QoS.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The generic <code>gpu</code> partition will not work consistently when using the <code>dev</code> QoS.</p>
</div>
<p>Here is an example job submission script for a 2-node job in the <code>dev</code> QoS using the <code>gpu-a100-80</code> 
partition. Note the use of the <code>gpu_launch.sh</code> wrapper script to get correct GPU and NIC
binding.</p>
<div class="highlight"><pre><span></span><code><span class="ch">#!/bin/bash</span>

<span class="c1"># Slurm job options</span>
<span class="kp">#SBATCH --job-name=Example_MPI_job</span>
<span class="kp">#SBATCH --time=12:0:0</span>
<span class="kp">#SBATCH --partition=gpu-a100-80</span>
<span class="kp">#SBATCH --qos=dev</span>
<span class="c1"># Replace [budget code] below with your budget code (e.g. t01)</span>
<span class="kp">#SBATCH --account=[budget code]  </span>

<span class="c1"># Request right number of full nodes (48 cores by node for A100-80 GPU nodes))</span>
<span class="kp">#SBATCH --nodes=4</span>
<span class="kp">#SBATCH --ntasks-per-node=48</span>
<span class="kp">#SBATCH --cpus-per-task=1</span>
<span class="kp">#SBATCH --gres=gpu:4</span>

<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">1</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_PLACES</span><span class="o">=</span>cores

<span class="c1"># Load the correct modules</span>
module<span class="w"> </span>load<span class="w"> </span>gcc/9.3.0
module<span class="w"> </span>load<span class="w"> </span>cuda/12.3
module<span class="w"> </span>load<span class="w"> </span>openmpi/4.1.5-cuda12.3<span class="w"> </span>

<span class="c1"># These will need to be changed to match the actual application you are running</span>
<span class="nv">application</span><span class="o">=</span><span class="s2">&quot;my_mpi_openmp_app.x&quot;</span>
<span class="nv">options</span><span class="o">=</span><span class="s2">&quot;arg 1 arg2&quot;</span>

<span class="c1"># We have reserved the full nodes, now distribute the processes as</span>
<span class="c1"># required: 4 MPI processes per node, stride of 12 cores between </span>
<span class="c1"># MPI processes</span>
<span class="c1"># </span>
<span class="c1"># Note use of gpu_launch.sh wrapper script for GPU and NIC pinning </span>
<span class="nb">srun</span><span class="w"> </span>--nodes<span class="o">=</span><span class="m">4</span><span class="w"> </span>--ntasks-per-node<span class="o">=</span><span class="m">4</span><span class="w"> </span>--cpus-per-task<span class="o">=</span><span class="m">12</span><span class="w"> </span><span class="se">\</span>
<span class="w">     </span>--hint<span class="o">=</span>nomultithread<span class="w"> </span>--distribution<span class="o">=</span>block:block<span class="w"> </span><span class="se">\</span>
<span class="w">     </span>gpu_launch.sh<span class="w"> </span><span class="se">\</span>
<span class="w">     </span><span class="si">${</span><span class="nv">application</span><span class="si">}</span><span class="w"> </span><span class="si">${</span><span class="nv">options</span><span class="si">}</span>
</code></pre></div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.expand", "navigation.top"], "search": "../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.83f73b43.min.js"></script>
      
    
  </body>
</html>
{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"DiRAC Extreme Scaling User Documentation DiRAC Extreme Scaling is part of the DiRAC National HPC Service . You can find more information on the service and the research it supports on the DiRAC website . The DiRAC Extreme Scaling service is an HPC resource for UK researchers. DiRAC Extreme Scaling is provided by UKRI , EPCC and the University of Edinburgh . The hardware is provided by HPE and ATOS . DiRAC Extreme Scaling currently provides two HPC resources: Tesseract: a CPU-based system provided by HPE. Tursa: a GPU-based system provided by ATOS. What the documentation covers This is the documentation for the ARCHER2 service and includes: Tesseract User Guide Covers all aspects of use of the Tursa resource. This includes fundamentals (required by all users to use the system effectively), best practice for getting the most out of Tursa, and other advanced technical topics. Tursa User Guide Covers all aspects of use of the Tursa resource. This includes fundamentals (required by all users to use the system effectively), best practice for getting the most out of Tursa, and other advanced technical topics. Contributing to the documentation The source for this documentation is publicly available in the ARCHER2 documentation Github repository so that anyone can contribute to improve the documentation for the service. Contributions can be in the form of improvements or addtions to the content and/or addtion of Issues providing suggestions for how it can be improved. Full details of how to contribute can be found in the README.md file of the repository.","title":"Documentation overview"},{"location":"#dirac-extreme-scaling-user-documentation","text":"DiRAC Extreme Scaling is part of the DiRAC National HPC Service . You can find more information on the service and the research it supports on the DiRAC website . The DiRAC Extreme Scaling service is an HPC resource for UK researchers. DiRAC Extreme Scaling is provided by UKRI , EPCC and the University of Edinburgh . The hardware is provided by HPE and ATOS . DiRAC Extreme Scaling currently provides two HPC resources: Tesseract: a CPU-based system provided by HPE. Tursa: a GPU-based system provided by ATOS.","title":"DiRAC Extreme Scaling User Documentation"},{"location":"#what-the-documentation-covers","text":"This is the documentation for the ARCHER2 service and includes: Tesseract User Guide Covers all aspects of use of the Tursa resource. This includes fundamentals (required by all users to use the system effectively), best practice for getting the most out of Tursa, and other advanced technical topics. Tursa User Guide Covers all aspects of use of the Tursa resource. This includes fundamentals (required by all users to use the system effectively), best practice for getting the most out of Tursa, and other advanced technical topics.","title":"What the documentation covers"},{"location":"#contributing-to-the-documentation","text":"The source for this documentation is publicly available in the ARCHER2 documentation Github repository so that anyone can contribute to improve the documentation for the service. Contributions can be in the form of improvements or addtions to the content and/or addtion of Issues providing suggestions for how it can be improved. Full details of how to contribute can be found in the README.md file of the repository.","title":"Contributing to the documentation"},{"location":"tesseract-user-guide/","text":"Tesseract User Guide The Tesseract User Guide covers all aspects of use of the Tesseract resource. This includes fundamentals (required by all users to use the system effectively), best practice for getting the most out of Tursa and more technical topics. The Tesseract User Guide is currently hosted on a separate website: Tesseract User Guide","title":"Overview"},{"location":"tesseract-user-guide/#tesseract-user-guide","text":"The Tesseract User Guide covers all aspects of use of the Tesseract resource. This includes fundamentals (required by all users to use the system effectively), best practice for getting the most out of Tursa and more technical topics. The Tesseract User Guide is currently hosted on a separate website: Tesseract User Guide","title":"Tesseract User Guide"},{"location":"tursa-user-guide/","text":"Tursa User Guide The Tursa User Guide covers all aspects of use of the Tursa resource. This includes fundamentals (required by all users to use the system effectively), best practice for getting the most out of Tursa and more technical topics. The Tursa User Guide contains the following sections: Connecting to Tursa Data management and transfer Software environment Running jobs on Tursa Application development environment","title":"Overview"},{"location":"tursa-user-guide/#tursa-user-guide","text":"The Tursa User Guide covers all aspects of use of the Tursa resource. This includes fundamentals (required by all users to use the system effectively), best practice for getting the most out of Tursa and more technical topics. The Tursa User Guide contains the following sections: Connecting to Tursa Data management and transfer Software environment Running jobs on Tursa Application development environment","title":"Tursa User Guide"},{"location":"tursa-user-guide/connecting/","text":"Connecting to Tursa On the Tursa system, interactive access can be achieved via SSH, either directly from a command line terminal or using an SSH client. In addition data can be transferred to and from the Tursa system using scp from the command line or by using a file transfer client. This section covers the basic connection methods. Before following the process below, we assume you have setup an account on Tursa through the DiRAC SAFE. Documentation on how to do this can be found at: DiRAC SAFE Guide for Users Command line terminal Linux Linux distributions and MacOS each come installed with a terminal application that can be used for SSH access to the login nodes. Linux users will have different terminals depending on their distribution and window manager (e.g. GNOME Terminal in GNOME, Konsole in KDE). Consult your Linux distribution's documentation for details on how to load a terminal. MacOS MacOS users can use the Terminal application, located in the Utilities folder within the Applications folder. Windows A typical Windows installation will not include a terminal client, though there are various clients available. We recommend all our Windows users to download and install MobaXterm to access Tursa. It is very easy to use and includes an integrated X server with SSH client to run any graphical applications on Tursa. You can download MobaXterm Home Edition (Installer Edition) from the following link: Install MobaXterm Double-click the downloaded Microsoft Installer file (.msi), and the Windows wizard will automatically guides you through the installation process. Note, you might need to have administrator rights to install on some Windows OS. Also make sure to check whether Windows Firewall hasn't blocked any features of this program after installation. Start MobaXterm and then click \"Start local terminal\" Tips If you download the .zip file rather than the .msi, make sure you unzip before attempting to run the installer. If you are using a \"managed desktop\" machine, so do not have admin rights, you can use the Portable edition of MobaXterm which doesn't need install privilages. If this is your first time using MobaXterm, check that a permanent /home directory has been set up (or all saved info will be lost from session to session). Go to \"Settings\" -> \"Configuration\"-> check path to \"Persistent home directory\" is set and make sure path is \"private\" if prompted. Any ssh key generated in MobaXterm will, by default, be stored in the permanent /home directory (see above) i.e. if your /home directory is _MyDocuments_\\MobaXterm\\home then within that folder you will find _MyDocuments_\\MobaXterm\\home\\.ssh with your keys. This folder will be 'hidden' by default so you may need to tick 'Hidden items' under 'View' in Windows Explorer to see it. MobaXterm also allows you to set up ssh sessions with the username, login host and key details saved. You are welcome to use this, rather than using the \"Local terminal\" but we are not able to assist with debugging connection issues if you choose this method. We recommend sticking to command line terminal access. Access credentials To access Tursa, you need to use two credentials: your password and an SSH key pair protected by a passphrase. You can find more detailed instructions on how to set up your credentials to access Tursa from Windows, macOS and Linux below. SSH Key Pairs You will need to generate an SSH key pair protected by a passphrase to access Tursa. Using a terminal (the command line), set up a key pair that contains your e-mail address and enter a passphrase you will use to unlock the key: $ ssh-keygen -t rsa -C \"your@email.com\" ... -bash-4.1$ ssh-keygen -t rsa -C \"your@email.com\" Generating public/private rsa key pair. Enter file in which to save the key (/Home/user/.ssh/id_rsa): [Enter] Enter passphrase (empty for no passphrase): [Passphrase] Enter same passphrase again: [Passphrase] Your identification has been saved in /Home/user/.ssh/id_rsa. Your public key has been saved in /Home/user/.ssh/id_rsa.pub. The key fingerprint is: 03:d4:c4:6d:58:0a:e2:4a:f8:73:9a:e8:e3:07:16:c8 your@email.com The key's randomart image is: +--[ RSA 2048]----+ | . ...+o++++. | | . . . =o.. | |+ . . .......o o | |oE . . | |o = . S | |. +.+ . | |. oo | |. . | | .. | +-----------------+ (remember to replace \"your@email.com\" with your e-mail address). Upload public part of key pair to SAFE You should now upload the public part of your SSH key pair to the SAFE by following the instructions at: Login to SAFE . Then: Go to the Menu Login accounts and select the Tursa account you want to add the SSH key to On the subsequent Login account details page click the Add Credential button Select SSH public key as the Credential Type and click Next Either copy and paste the public part of your SSH key into the SSH Public key box or use the button to select the public key file on your computer. Click Add to associate the public SSH key part with your account Once you have done this, your SSH key will be added to your Tursa account. Remember, you will need to use both an SSH key and password to log into Tursa so you will also need to collect your initial password before you can log into Tursa. We cover this next. Note If you want to connect to Tursa from more than one machine, e.g. from your home laptop as well as your work laptop, you should generate an ssh key on each machine, and add each of the public keys into SAFE. Initial passwords The SAFE web interface is used to provide your initial password for logging onto Tursa (see the SAFE Documentation for more details on requesting accounts and picking up passwords). Note You may now change your password on the Tursa machine itself using the passwd command or when you are prompted the first time you login. This change will not be reflected in the SAFE. If you forget your password, you should use the SAFE to request a new one-shot password. SSH Clients Interaction with Tursa is done remotely, over an encrypted communication channel, Secure Shell version 2 (SSH-2). This allows command-line access to one of the login nodes of a Tursa, from which you can run commands or use a command-line text editor to edit files. SSH can also be used to run graphical programs such as GUI text editors and debuggers when used in conjunction with an X client. Logging in You can use the following command from the terminal window to login into Tursa: ssh username@login.tursa.ac.uk You will first be prompted for your machine account password. Once you have entered your password successfully, you will then be prompted for the passphrase associated with your SSH key pair. You need to enter both credentials correctly to be able to access Tursa. Tip If your SSH key pair is not stored in the default location (usually ~/.ssh/id_rsa ) on your local system, you may need to specify the path to the private part of the key wih the -i option to ssh . For example, if your key is in a file called keys/id_rsa_Tursa you would use the command ssh -i keys/id_rsa_Tursa username@login.tursa.ac.uk to log in. Tip When you first log into Tursa, you will be prompted to change your initial password. This is a three step process: When promoted to enter your ldap password : Re-enter the password you retrieved from SAFE When prompted to enter your new password: type in a new password When prompted to re-enter the new password: re-enter the new password Your password has now been changed To allow remote programs, especially graphical applications to control your local display, such as being able to open up a new GUI window (such as for a debugger), use: ssh -X username@login.tursa.ac.uk Some sites recommend using the -Y flag. While this can fix some compatibility issues, the -X flag is more secure. Current MacOS systems do not have an X window system. Users should install the XQuartz package to allow for SSH with X11 forwarding on MacOS systems: XQuartz website Making access more convenient using the SSH configuration file Typing in the full command to login or transfer data to Tursa can become tedious as it often has to be repeated many times. You can use the SSH configuration file, usually located on your local machine at .ssh/config to make things a bit more convenient. Each remote site (or group of sites) can have an entry in this file which may look something like: Host tursa HostName login.tursa.ac.uk User username (remember to replace username with your actual username!). The Host tursa line defines a short name for the entry. In this case, instead of typing ssh username@login.tursa.ac.uk to access the Tursa login nodes, you could use ssh tursa instead. The remaining lines define the options for the tursa host. Hostname login.tursa.ac.uk - defines the full address of the host User username - defines the username to use by default for this host (replace username with your own username on the remote host) Now you can use SSH to access Tursa without needing to enter your username or the full hostname every time: $ ssh tursa You can set up as many of these entries as you need in your local configuration file. Other options are available. See the ssh_config man page (or man ssh_config on any machine with SSH installed) for a description of the SSH configuration file. You may find the IdentityFile option useful if you have to manage multiple SSH key pairs for different systems as this allows you to specify which SSH key to use for each system. Bug There is a known bug with Windows ssh-agent. If you get the error message: Warning: agent returned different signature type ssh-rsa (expected rsa-sha2-512) , you will need to either specify the path to your ssh key in the command line (using the -i option as described above) or add the path to your SSH config file by using the IdentityFile option. SSH debugging tips If you find you are unable to connect via SSH there are a number of ways you can try and diagnose the issue. Some of these are collected below - if you are having difficulties connecting we suggest trying these before contacting the Tursa service desk. Use the user@login.tursa.ac.uk syntax rather than -l user login.tursa.ac.uk We have seen a number of instances where people using the syntax ssh -l user login.tursa.ac.uk have not been able to connect properly and get prompted for a password many times. We have found that using the alternative syntax: ssh user@login.tursa.ac.uk works more reliably. If you are using the -l user option to connect and are seeing issues, then try using user@login.tursa.ac.uk instead. Can you connect to the login node? Try the command ping -c 3 login.tursa.ac.uk . If you successfully connect to the login node, the output should include: --- login.tursa.ac.uk ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 38ms (the ping time '38ms' is not important). If not all packets are received there could be a problem with your internet connection, or the login node could be unavailable. Password If you are having trouble entering your password consider using a password manager, from which you can copy and paste it. This will also help you generate a secure password. If you need to reset your password, instructions for doing so can be found in the SAFE documentation Windows users please note that Ctrl+V does not work to paste in to PuTTY, MobaXterm, or PowerShell. Instead use Shift+Ins to paste. Alternatively, right-click and select 'Paste' in PuTTY and MobaXterm, or simply right-click to paste in PowerShell. SSH key If you get the error message Permission denied (publickey) this can indicate a problem with your SSH key. Some things to check: Have you uploaded the key to SAFE? Please note that if the same key is reuploaded SAFE will not map the \"new\" key to Tursa. If for some reason this is required, please delete the key first, then reupload. Is ssh using the correct key? You can check which keys are being found and offered by ssh using ssh -vvv . If your private key has a non-default name you can use the -i flag to provide it to ssh, i.e. ssh -i path/to/key username@login.tursa.ac.uk . Are you entering the passphrase correctly? You will be asked for your private key's passphrase first. If you enter it incorrectly you will usually be asked to enter it again, and usually up to three times in total, after which ssh will fail with Permission denied (publickey) . If you would like to confirm your passphrase without attempting to connect, you can use ssh-keygen -y -f /path/to/private/key . If successful, this command will print the corresponding public key. You can also use this to check it is the one uploaded to SAFE. Are permissions correct on the ssh key? One common issue is that the permissions are incorrect on the either the key file, or the directory it's contained in. On Linux/MacOS for example, if your private keys are held in ~/.ssh/ you can check this with ls -al ~/.ssh . This should give something similar to the following output: $ ls -al ~/.ssh/ drwx------. 2 user group 48 Jul 15 20:24 . drwx------. 12 user group 4096 Oct 13 12:11 .. -rw-------. 1 user group 113 Jul 15 20:23 authorized_keys -rw-------. 1 user group 12686 Jul 15 20:23 id_rsa -rw-r--r--. 1 user group 2785 Jul 15 20:23 id_rsa.pub -rw-r--r--. 1 user group 1967 Oct 13 14:11 known_hosts The important section here is the string of letters and dashes at the start, for the lines ending in . , id_rsa , and id_rsa.pub , which indicate permissions on the containing directory, private key, and public key respectively. If your permissions are not correct, they can be set with chmod . Consult the table below for the relevant chmod command. On Windows, permissions are handled differently but can be set by right-clicking on the file and selecting Properties > Security > Advanced. The user, SYSTEM, and Administrators should have Full control , and no other permissions should exist for both public and private key files, and the containing folder. Target Permissions chmod Code Directory drwx------ 700 Private Key -rw------- 600 Public Key -rw-r--r-- 644 chmod can be used to set permissions on the target in the following way: chmod <code> <target> . So for example to set correct permissions on the private key file id_rsa_Tursa one would use the command chmod 600 id_rsa_Tursa . Tip Unix file permissions can be understood in the following way. There are three groups that can have file permissions: (owning) users , (owning) groups , and others . The available permissions are read , write , and execute . The first character indicates whether the target is a file - , or directory d . The next three characters indicate the owning user's permissions. The first character is r if they have read permission, - if they don't, the second character is w if they have write permission, - if they don't, the third character is x if they have execute permission, - if they don't. This pattern is then repeated for group , and other permissions. For example the pattern -rw-r--r-- indicates that the owning user can read and write the file, members of the owning group can read it, and anyone else can also read it. The chmod codes are constructed by treating the user, group, and owner permission strings as binary numbers, then converting them to decimal. For example the permission string -rwx------ becomes 111 000 000 -> 700 . SSH verbose output Verbose debugging output from ssh can be very useful for diagnosing the issue. In particular, it can be used to distinguish between problems with the SSH key and password - further details are given below. To enable verbose output add the -vvv flag to your SSH command. For example: ssh -vvv username@login.tursa.ac.uk The output is lengthy, but somewhere in there you should see lines similar to the following: debug1: Next authentication method: keyboard-interactive debug2: userauth_kbdint debug3: send packet: type 50 debug2: we sent a keyboard-interactive packet, wait for reply debug3: receive packet: type 60 debug2: input_userauth_info_req debug2: input_userauth_info_req: num_prompts 1 Password: debug3: send packet: type 61 debug3: receive packet: type 60 debug2: input_userauth_info_req debug2: input_userauth_info_req: num_prompts 0 debug3: send packet: type 61 debug3: receive packet: type 51 Authenticated with partial success. debug1: Authentications that can continue: publickey,password If you do not see the Password: prompt you may have connection issues, or there could be a problem with the Tursa login nodes. If you do not see Authenticated with partial success it means your password was not accepted. You will be asked to re-enter your password, usually two more times before the connection will be rejected. Consider the suggestions under Password above. If you do see Authenticated with partial success , it means your password was accepted, and your SSH key will now be checked. You should next see something similiar to: debug1: Next authentication method: publickey debug1: Offering public key: RSA SHA256:<key_hash> <path_to_private_key> debug3: send_pubkey_test debug3: send packet: type 50 debug2: we sent a publickey packet, wait for reply debug3: receive packet: type 60 debug1: Server accepts key: pkalg rsa-sha2-512 blen 2071 debug2: input_userauth_pk_ok: fp SHA256:<key_hash> debug3: sign_and_send_pubkey: RSA SHA256:<key_hash> Enter passphrase for key '<path_to_private_key>': debug3: send packet: type 50 debug3: receive packet: type 52 debug1: Authentication succeeded (publickey). Most importantly, you can see which files ssh has checked for private keys, and you can see if any key is accepted. The line Authenticated succeeded indicates that the SSH key has been accepted. By default ssh will go through a list of standard private key files, as well as any you have specified with -i or a config file. This is fine, as long as one of the files mentioned is the one that matches the public key uploaded to SAFE. If your SSH key passphrase is incorrect, you will be asked to try again up to three times in total, before being disconnected with Permission denied (publickey) . If you enter your passphrase correctly, but still see this error message, please consider the advice under SSH key above. The equivalent information can be obtained in PuTTY by enabling all logging in settings.","title":"Connecting to Tursa"},{"location":"tursa-user-guide/connecting/#connecting-to-tursa","text":"On the Tursa system, interactive access can be achieved via SSH, either directly from a command line terminal or using an SSH client. In addition data can be transferred to and from the Tursa system using scp from the command line or by using a file transfer client. This section covers the basic connection methods. Before following the process below, we assume you have setup an account on Tursa through the DiRAC SAFE. Documentation on how to do this can be found at: DiRAC SAFE Guide for Users","title":"Connecting to Tursa"},{"location":"tursa-user-guide/connecting/#command-line-terminal","text":"","title":"Command line terminal"},{"location":"tursa-user-guide/connecting/#linux","text":"Linux distributions and MacOS each come installed with a terminal application that can be used for SSH access to the login nodes. Linux users will have different terminals depending on their distribution and window manager (e.g. GNOME Terminal in GNOME, Konsole in KDE). Consult your Linux distribution's documentation for details on how to load a terminal.","title":"Linux"},{"location":"tursa-user-guide/connecting/#macos","text":"MacOS users can use the Terminal application, located in the Utilities folder within the Applications folder.","title":"MacOS"},{"location":"tursa-user-guide/connecting/#windows","text":"A typical Windows installation will not include a terminal client, though there are various clients available. We recommend all our Windows users to download and install MobaXterm to access Tursa. It is very easy to use and includes an integrated X server with SSH client to run any graphical applications on Tursa. You can download MobaXterm Home Edition (Installer Edition) from the following link: Install MobaXterm Double-click the downloaded Microsoft Installer file (.msi), and the Windows wizard will automatically guides you through the installation process. Note, you might need to have administrator rights to install on some Windows OS. Also make sure to check whether Windows Firewall hasn't blocked any features of this program after installation. Start MobaXterm and then click \"Start local terminal\" Tips If you download the .zip file rather than the .msi, make sure you unzip before attempting to run the installer. If you are using a \"managed desktop\" machine, so do not have admin rights, you can use the Portable edition of MobaXterm which doesn't need install privilages. If this is your first time using MobaXterm, check that a permanent /home directory has been set up (or all saved info will be lost from session to session). Go to \"Settings\" -> \"Configuration\"-> check path to \"Persistent home directory\" is set and make sure path is \"private\" if prompted. Any ssh key generated in MobaXterm will, by default, be stored in the permanent /home directory (see above) i.e. if your /home directory is _MyDocuments_\\MobaXterm\\home then within that folder you will find _MyDocuments_\\MobaXterm\\home\\.ssh with your keys. This folder will be 'hidden' by default so you may need to tick 'Hidden items' under 'View' in Windows Explorer to see it. MobaXterm also allows you to set up ssh sessions with the username, login host and key details saved. You are welcome to use this, rather than using the \"Local terminal\" but we are not able to assist with debugging connection issues if you choose this method. We recommend sticking to command line terminal access.","title":"Windows"},{"location":"tursa-user-guide/connecting/#access-credentials","text":"To access Tursa, you need to use two credentials: your password and an SSH key pair protected by a passphrase. You can find more detailed instructions on how to set up your credentials to access Tursa from Windows, macOS and Linux below.","title":"Access credentials"},{"location":"tursa-user-guide/connecting/#ssh-key-pairs","text":"You will need to generate an SSH key pair protected by a passphrase to access Tursa. Using a terminal (the command line), set up a key pair that contains your e-mail address and enter a passphrase you will use to unlock the key: $ ssh-keygen -t rsa -C \"your@email.com\" ... -bash-4.1$ ssh-keygen -t rsa -C \"your@email.com\" Generating public/private rsa key pair. Enter file in which to save the key (/Home/user/.ssh/id_rsa): [Enter] Enter passphrase (empty for no passphrase): [Passphrase] Enter same passphrase again: [Passphrase] Your identification has been saved in /Home/user/.ssh/id_rsa. Your public key has been saved in /Home/user/.ssh/id_rsa.pub. The key fingerprint is: 03:d4:c4:6d:58:0a:e2:4a:f8:73:9a:e8:e3:07:16:c8 your@email.com The key's randomart image is: +--[ RSA 2048]----+ | . ...+o++++. | | . . . =o.. | |+ . . .......o o | |oE . . | |o = . S | |. +.+ . | |. oo | |. . | | .. | +-----------------+ (remember to replace \"your@email.com\" with your e-mail address).","title":"SSH Key Pairs"},{"location":"tursa-user-guide/connecting/#upload-public-part-of-key-pair-to-safe","text":"You should now upload the public part of your SSH key pair to the SAFE by following the instructions at: Login to SAFE . Then: Go to the Menu Login accounts and select the Tursa account you want to add the SSH key to On the subsequent Login account details page click the Add Credential button Select SSH public key as the Credential Type and click Next Either copy and paste the public part of your SSH key into the SSH Public key box or use the button to select the public key file on your computer. Click Add to associate the public SSH key part with your account Once you have done this, your SSH key will be added to your Tursa account. Remember, you will need to use both an SSH key and password to log into Tursa so you will also need to collect your initial password before you can log into Tursa. We cover this next. Note If you want to connect to Tursa from more than one machine, e.g. from your home laptop as well as your work laptop, you should generate an ssh key on each machine, and add each of the public keys into SAFE.","title":"Upload public part of key pair to SAFE"},{"location":"tursa-user-guide/connecting/#initial-passwords","text":"The SAFE web interface is used to provide your initial password for logging onto Tursa (see the SAFE Documentation for more details on requesting accounts and picking up passwords). Note You may now change your password on the Tursa machine itself using the passwd command or when you are prompted the first time you login. This change will not be reflected in the SAFE. If you forget your password, you should use the SAFE to request a new one-shot password.","title":"Initial passwords"},{"location":"tursa-user-guide/connecting/#ssh-clients","text":"Interaction with Tursa is done remotely, over an encrypted communication channel, Secure Shell version 2 (SSH-2). This allows command-line access to one of the login nodes of a Tursa, from which you can run commands or use a command-line text editor to edit files. SSH can also be used to run graphical programs such as GUI text editors and debuggers when used in conjunction with an X client.","title":"SSH Clients"},{"location":"tursa-user-guide/connecting/#logging-in","text":"You can use the following command from the terminal window to login into Tursa: ssh username@login.tursa.ac.uk You will first be prompted for your machine account password. Once you have entered your password successfully, you will then be prompted for the passphrase associated with your SSH key pair. You need to enter both credentials correctly to be able to access Tursa. Tip If your SSH key pair is not stored in the default location (usually ~/.ssh/id_rsa ) on your local system, you may need to specify the path to the private part of the key wih the -i option to ssh . For example, if your key is in a file called keys/id_rsa_Tursa you would use the command ssh -i keys/id_rsa_Tursa username@login.tursa.ac.uk to log in. Tip When you first log into Tursa, you will be prompted to change your initial password. This is a three step process: When promoted to enter your ldap password : Re-enter the password you retrieved from SAFE When prompted to enter your new password: type in a new password When prompted to re-enter the new password: re-enter the new password Your password has now been changed To allow remote programs, especially graphical applications to control your local display, such as being able to open up a new GUI window (such as for a debugger), use: ssh -X username@login.tursa.ac.uk Some sites recommend using the -Y flag. While this can fix some compatibility issues, the -X flag is more secure. Current MacOS systems do not have an X window system. Users should install the XQuartz package to allow for SSH with X11 forwarding on MacOS systems: XQuartz website","title":"Logging in"},{"location":"tursa-user-guide/connecting/#making-access-more-convenient-using-the-ssh-configuration-file","text":"Typing in the full command to login or transfer data to Tursa can become tedious as it often has to be repeated many times. You can use the SSH configuration file, usually located on your local machine at .ssh/config to make things a bit more convenient. Each remote site (or group of sites) can have an entry in this file which may look something like: Host tursa HostName login.tursa.ac.uk User username (remember to replace username with your actual username!). The Host tursa line defines a short name for the entry. In this case, instead of typing ssh username@login.tursa.ac.uk to access the Tursa login nodes, you could use ssh tursa instead. The remaining lines define the options for the tursa host. Hostname login.tursa.ac.uk - defines the full address of the host User username - defines the username to use by default for this host (replace username with your own username on the remote host) Now you can use SSH to access Tursa without needing to enter your username or the full hostname every time: $ ssh tursa You can set up as many of these entries as you need in your local configuration file. Other options are available. See the ssh_config man page (or man ssh_config on any machine with SSH installed) for a description of the SSH configuration file. You may find the IdentityFile option useful if you have to manage multiple SSH key pairs for different systems as this allows you to specify which SSH key to use for each system. Bug There is a known bug with Windows ssh-agent. If you get the error message: Warning: agent returned different signature type ssh-rsa (expected rsa-sha2-512) , you will need to either specify the path to your ssh key in the command line (using the -i option as described above) or add the path to your SSH config file by using the IdentityFile option.","title":"Making access more convenient using the SSH configuration file"},{"location":"tursa-user-guide/connecting/#ssh-debugging-tips","text":"If you find you are unable to connect via SSH there are a number of ways you can try and diagnose the issue. Some of these are collected below - if you are having difficulties connecting we suggest trying these before contacting the Tursa service desk.","title":"SSH debugging tips"},{"location":"tursa-user-guide/connecting/#use-the-userlogintursaacuk-syntax-rather-than-l-user-logintursaacuk","text":"We have seen a number of instances where people using the syntax ssh -l user login.tursa.ac.uk have not been able to connect properly and get prompted for a password many times. We have found that using the alternative syntax: ssh user@login.tursa.ac.uk works more reliably. If you are using the -l user option to connect and are seeing issues, then try using user@login.tursa.ac.uk instead.","title":"Use the user@login.tursa.ac.uk syntax rather than -l user login.tursa.ac.uk"},{"location":"tursa-user-guide/connecting/#can-you-connect-to-the-login-node","text":"Try the command ping -c 3 login.tursa.ac.uk . If you successfully connect to the login node, the output should include: --- login.tursa.ac.uk ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 38ms (the ping time '38ms' is not important). If not all packets are received there could be a problem with your internet connection, or the login node could be unavailable.","title":"Can you connect to the login node?"},{"location":"tursa-user-guide/connecting/#password","text":"If you are having trouble entering your password consider using a password manager, from which you can copy and paste it. This will also help you generate a secure password. If you need to reset your password, instructions for doing so can be found in the SAFE documentation Windows users please note that Ctrl+V does not work to paste in to PuTTY, MobaXterm, or PowerShell. Instead use Shift+Ins to paste. Alternatively, right-click and select 'Paste' in PuTTY and MobaXterm, or simply right-click to paste in PowerShell.","title":"Password"},{"location":"tursa-user-guide/connecting/#ssh-key","text":"If you get the error message Permission denied (publickey) this can indicate a problem with your SSH key. Some things to check: Have you uploaded the key to SAFE? Please note that if the same key is reuploaded SAFE will not map the \"new\" key to Tursa. If for some reason this is required, please delete the key first, then reupload. Is ssh using the correct key? You can check which keys are being found and offered by ssh using ssh -vvv . If your private key has a non-default name you can use the -i flag to provide it to ssh, i.e. ssh -i path/to/key username@login.tursa.ac.uk . Are you entering the passphrase correctly? You will be asked for your private key's passphrase first. If you enter it incorrectly you will usually be asked to enter it again, and usually up to three times in total, after which ssh will fail with Permission denied (publickey) . If you would like to confirm your passphrase without attempting to connect, you can use ssh-keygen -y -f /path/to/private/key . If successful, this command will print the corresponding public key. You can also use this to check it is the one uploaded to SAFE. Are permissions correct on the ssh key? One common issue is that the permissions are incorrect on the either the key file, or the directory it's contained in. On Linux/MacOS for example, if your private keys are held in ~/.ssh/ you can check this with ls -al ~/.ssh . This should give something similar to the following output: $ ls -al ~/.ssh/ drwx------. 2 user group 48 Jul 15 20:24 . drwx------. 12 user group 4096 Oct 13 12:11 .. -rw-------. 1 user group 113 Jul 15 20:23 authorized_keys -rw-------. 1 user group 12686 Jul 15 20:23 id_rsa -rw-r--r--. 1 user group 2785 Jul 15 20:23 id_rsa.pub -rw-r--r--. 1 user group 1967 Oct 13 14:11 known_hosts The important section here is the string of letters and dashes at the start, for the lines ending in . , id_rsa , and id_rsa.pub , which indicate permissions on the containing directory, private key, and public key respectively. If your permissions are not correct, they can be set with chmod . Consult the table below for the relevant chmod command. On Windows, permissions are handled differently but can be set by right-clicking on the file and selecting Properties > Security > Advanced. The user, SYSTEM, and Administrators should have Full control , and no other permissions should exist for both public and private key files, and the containing folder. Target Permissions chmod Code Directory drwx------ 700 Private Key -rw------- 600 Public Key -rw-r--r-- 644 chmod can be used to set permissions on the target in the following way: chmod <code> <target> . So for example to set correct permissions on the private key file id_rsa_Tursa one would use the command chmod 600 id_rsa_Tursa . Tip Unix file permissions can be understood in the following way. There are three groups that can have file permissions: (owning) users , (owning) groups , and others . The available permissions are read , write , and execute . The first character indicates whether the target is a file - , or directory d . The next three characters indicate the owning user's permissions. The first character is r if they have read permission, - if they don't, the second character is w if they have write permission, - if they don't, the third character is x if they have execute permission, - if they don't. This pattern is then repeated for group , and other permissions. For example the pattern -rw-r--r-- indicates that the owning user can read and write the file, members of the owning group can read it, and anyone else can also read it. The chmod codes are constructed by treating the user, group, and owner permission strings as binary numbers, then converting them to decimal. For example the permission string -rwx------ becomes 111 000 000 -> 700 .","title":"SSH key"},{"location":"tursa-user-guide/connecting/#ssh-verbose-output","text":"Verbose debugging output from ssh can be very useful for diagnosing the issue. In particular, it can be used to distinguish between problems with the SSH key and password - further details are given below. To enable verbose output add the -vvv flag to your SSH command. For example: ssh -vvv username@login.tursa.ac.uk The output is lengthy, but somewhere in there you should see lines similar to the following: debug1: Next authentication method: keyboard-interactive debug2: userauth_kbdint debug3: send packet: type 50 debug2: we sent a keyboard-interactive packet, wait for reply debug3: receive packet: type 60 debug2: input_userauth_info_req debug2: input_userauth_info_req: num_prompts 1 Password: debug3: send packet: type 61 debug3: receive packet: type 60 debug2: input_userauth_info_req debug2: input_userauth_info_req: num_prompts 0 debug3: send packet: type 61 debug3: receive packet: type 51 Authenticated with partial success. debug1: Authentications that can continue: publickey,password If you do not see the Password: prompt you may have connection issues, or there could be a problem with the Tursa login nodes. If you do not see Authenticated with partial success it means your password was not accepted. You will be asked to re-enter your password, usually two more times before the connection will be rejected. Consider the suggestions under Password above. If you do see Authenticated with partial success , it means your password was accepted, and your SSH key will now be checked. You should next see something similiar to: debug1: Next authentication method: publickey debug1: Offering public key: RSA SHA256:<key_hash> <path_to_private_key> debug3: send_pubkey_test debug3: send packet: type 50 debug2: we sent a publickey packet, wait for reply debug3: receive packet: type 60 debug1: Server accepts key: pkalg rsa-sha2-512 blen 2071 debug2: input_userauth_pk_ok: fp SHA256:<key_hash> debug3: sign_and_send_pubkey: RSA SHA256:<key_hash> Enter passphrase for key '<path_to_private_key>': debug3: send packet: type 50 debug3: receive packet: type 52 debug1: Authentication succeeded (publickey). Most importantly, you can see which files ssh has checked for private keys, and you can see if any key is accepted. The line Authenticated succeeded indicates that the SSH key has been accepted. By default ssh will go through a list of standard private key files, as well as any you have specified with -i or a config file. This is fine, as long as one of the files mentioned is the one that matches the public key uploaded to SAFE. If your SSH key passphrase is incorrect, you will be asked to try again up to three times in total, before being disconnected with Permission denied (publickey) . If you enter your passphrase correctly, but still see this error message, please consider the advice under SSH key above. The equivalent information can be obtained in PuTTY by enabling all logging in settings.","title":"SSH verbose output"},{"location":"tursa-user-guide/data/","text":"Data management and transfer This section covers best practice and tools for data management on Tursa. Information If you have any questions on data management and transfer please do not hesitate to contact the DiRAC service desk at dirac-support@epcc.ed.ac.uk . Useful resources and links Harry Mangalam's guide on How to transfer large amounts of data via network . This provides lots of useful advice on transferring data. Data management We strongly recommend that you give some thought to how you use the various data storage facilities that are part of the Tursa service. This will not only allow you to use the machine more effectively but also to ensure that your valuable data is protected. Tursa storage The Tursa service, like many HPC systems, has a complex structure. There are a number of different data storage types available to users: Home file systems Work file systems Each type of storage has different characteristics and policies, and is suitable for different types of use. There are also two different types of node available to users: Login nodes Compute nodes Each type of node sees a different combination of the storage types. The following table shows which storage options are avalable on different node types: Storage Login Nodes Compute Nodes Notes /home yes no Backed up /work yes yes Not backed up, high performance Home file systems There are four independent home file-systems. Every project has an allocation on one of the four. You do not need to know which one your project uses as your projects space can always be accessed via the path /home/project-code . Each home file-system is approximately 60TB in size and is implemented using standard Network Attached Storage (NAS) technology. This means that these disks are not particularly high performance but are well suited to standard operations like compilation and file editing. These file systems are visible from the Tursa login nodes. The home file systems are fully backed up . Full backups are taken weekly (for each of the past two weeks), daily (for each of the past two days) and hourly (for each of the last 6 hours). You can access the snapshots at the /home1/.snapshot , /home2/.snapshot , /home3/.snapshot and /home4/.snapshot depending on which of the file systems you have your home directories on. You can find out which file system your home directory is on with the command: readlink -f $HOME These file-systems are a good location to keep source-code, copies of scripts and compiled binaries. Small amounts of important data can also be copied here for safe keeping though the file systems are not fast enough to manipulate large datasets effectively. Warning Files with filenames that contain non-ascii characters and/or non-printable characters cannot be backed up using our automated process and so will be omitted from all backups. Work file systems There is one work file-system: /work 3.4 PB Every project has an allocation on the file system. This is a high-performance, Lustre parallel file system. It is designed to support data in large files. The performance for data stored in large numbers of small files is probably not going to be as good. This is the only file system that is available on the compute nodes so all data read or written by jobs running on the compute nodes has to be hosted here. Warning There are no backups of any data on the work file system. You should not rely on these file systems for long term storage. Ideally, this file system should only contain data that is: actively in use; recently generated and in the process of being saved elsewhere; or being made ready for up-coming work. In practice it may be convenient to keep copies of datasets on the work file system that you know will be needed at a later date. However, make sure that important data is always backed up elsewhere and that your work would not be significantly impacted if the data on the work file system was lost. Large data sets can be moved to the RDF storage or transferred off the Tursa service entirely. If you have data on the work file system that you are not going to need in the future please delete it. Subprojects Some large projects may choose to split their resources into multiple subprojects. These subprojects will have identifiers appended to the main project ID. For example, the rse subgroup of the z19 project would have the ID z19-rse . If the main project has allocated storage quotas to the subproject the directories for this storage will be found at, for example: /home/z19/z19-rse/auser Your Linux home directory will generally not be changed when you are made a member of a subproject so you must change directories manually (or change the ownership of files) to make use of this different storage quota allocation. Sharing data with other Tursa users How you share data with other Tursa users depends on whether or not they belong to the same project as you. Each project has two shared folders that can be used for sharing data. Sharing data with Tursa users in your project Each project has an inner shared folder. /work/[project code]/[project code]/shared This folder has read/write permissions for all project members. You can place any data you wish to share with other project members in this directory. For example, if your project code is x01 the inner shared folder would be located at /work/x01/x01/shared . Sharing data with all Tursa users Each project also has an outer shared folder.: /work/[project code]/shared It is writable by all project members and readable by any user on the system. You can place any data you wish to share with other Tursa users who are not members of your project in this directory. For example, if your project code is x01 the outer shared folder would be located at /work/x01/shared . Permissions You should check the permissions of any files that you place in the shared area, especially if those files were created in your own Tursa account. Files of the latter type are likely to be readable by you only. The chmod command below shows how to make sure that a file placed in the outer shared folder is also readable by all Tursa users. chmod a+r /work/x01/shared/your-shared-file.txt Similarly, for the inner shared folder, chmod can be called such that read permission is granted to all users within the x01 project. chmod g+r /work/x01/x01/shared/your-shared-file.txt If you're sharing a set of files stored within a folder hierarchy the chmod is slightly more complicated. chmod -R a+Xr /work/x01/shared/my-shared-folder chmod -R g+Xr /work/x01/x01/shared/my-shared-folder The -R option ensures that the read permission is enabled recursively and the +X guarantees that the user(s) you're sharing the folder with can access the subdirectories below my-shared-folder . Sharing data between projects and subprojects Every file has an owner group that specifies access permissions for users belonging to that group. It's usually the case that the group id is synonymous with the project code. Somewhat confusingly however, projects can contain groups of their own, called subprojects, which can be assigned disk space quotas distinct from the project. chown -R $USER:x01-subproject /work/x01/x01-subproject/$USER/my-folder The chown command above changes the owning group for all the files within my-folder to the x01-subproject group. This might be necessary if previously those files were owned by the x01 group and thereby using some of the x01 disk quota. Archiving and data transfer Data transfer speed may be limited by many different factors so the best data transfer mechanism to use depends on the type of data being transferred and where the data is going. Disk speed - The Tursa /work file system is highly parallel, consisting of a very large number of high performance disk drives. This allows it to support a very high data bandwidth. Unless the remote system has a similar parallel file-system you may find your transfer speed limited by disk performance. Meta-data performance - Meta-data operations such as opening and closing files or listing the owner or size of a file are much less parallel than read/write operations. If your data consists of a very large number of small files you may find your transfer speed is limited by meta-data operations. Meta-data operations performed by other users of the system will interact strongly with those you perform so reducing the number of such operations you use, may reduce variability in your IO timings. Network speed - Data transfer performance can be limited by network speed. More importantly it is limited by the slowest section of the network between source and destination. Firewall speed - Most modern networks are protected by some form of firewall that filters out malicious traffic. This filtering has some overhead and can result in a reduction in data transfer performance. The needs of a general purpose network that hosts email/web-servers and desktop machines are quite different from a research network that needs to support high volume data transfers. If you are trying to transfer data to or from a host on a general purpose network you may find the firewall for that network will limit the transfer rate you can achieve. The method you use to transfer data to/from Tursa will depend on how much you want to transfer and where to. The methods we cover in this guide are: scp/sftp/rsync - These are the simplest methods of transferring data and can be used up to moderate amounts of data. If you are transferring data to your workstation/laptop then this is the method you will use. GridFTP - It is sometimes more convenient to transfer large amounts of data (> 100 GBs) using GridFTP servers. Before discussing specific data transfer methods, we cover archiving which is an essential process for transferring data efficiently. Archiving If you have related data that consists of a large number of small files it is strongly recommended to pack the files into a larger \"archive\" file for ease of transfer and manipulation. A single large file makes more efficient use of the file system and is easier to move and copy and transfer because significantly fewer meta-data operations are required. Archive files can be created using tools like tar and zip . tar The tar command packs files into a \"tape archive\" format. The command has general form: tar [options] [file(s)] Common options include: -c create a new archive -v verbosely list files processed -W verify the archive after writing -l confirm all file hard links are included in the archive -f use an archive file (for historical reasons, tar writes its output to stdout by default rather than a file). Putting these together: tar -cvWlf mydata.tar mydata will create and verify an archive. To extract files from a tar file, the option -x is used. For example: tar -xf mydata.tar will recover the contents of mydata.tar to the current working directory. To verify an existing tar file against a set of data, the -d (diff) option can be used. By default, no output will be given if a verification succeeds and an example of a failed verification follows: $> tar -df mydata.tar mydata/* mydata/damaged_file: Mod time differs mydata/damaged_file: Size differs Note tar files do not store checksums with their data, requiring the original data to be present during verification. Tip Further information on using tar can be found in the tar manual (accessed via man tar or at man tar ). zip The zip file format is widely used for archiving files and is supported by most major operating systems. The utility to create zip files can be run from the command line as: zip [options] mydata.zip [file(s)] Common options are: -r used to zip up a directory -# where \"#\" represents a digit ranging from 0 to 9 to specify compression level, 0 being the least and 9 the most. Default compression is -6 but we recommend using -0 to speed up the archiving process. Together: zip -0r mydata.zip mydata will create an archive. Note Unlike tar, zip files do not preserve hard links. File data will be copied on archive creation, e.g. an uncompressed zip archive of a 100MB file and a hard link to that file will be approximately 200MB in size. This makes zip an unsuitable format if you wish to precisely reproduce the file system layout. The corresponding unzip command is used to extract data from the archive. The simplest use case is: unzip mydata.zip which recovers the contents of the archive to the current working directory. Files in a zip archive are stored with a CRC checksum to help detect data loss. unzip provides options for verifying this checksum against the stored files. The relevant flag is -t and is used as follows: $> unzip -t mydata.zip Archive: mydata.zip testing: mydata/ OK testing: mydata/file OK No errors detected in compressed data of mydata.zip. Tip Further information on using zip can be found in the zip manual (accessed via man zip or at man zip ). Data transfer via SSH The easiest way of transferring data to/from Tursa is to use one of the standard programs based on the SSH protocol such as scp , sftp or rsync . These all use the same underlying mechanism (SSH) as you normally use to log-in to Tursa. So, once the the command has been executed via the command line, you will be prompted for your password for the specified account on the remote machine (Tursa in this case). To avoid having to type in your password multiple times you can set up a SSH key pair and use an SSH agent as documented in the User Guide at connecting . SSH data transfer performance considerations The SSH protocol encrypts all traffic it sends. This means that file transfer using SSH consumes a relatively large amount of CPU time at both ends of the transfer (for encryption and decryption). The Tursa login nodes have fairly fast processors that can sustain about 100 MB/s transfer. The encryption algorithm used is negotiated between the SSH client and the SSH server. There are command line flags that allow you to specify a preference for which encryption algorithm should be used. You may be able to improve transfer speeds by requesting a different algorithm than the default. The aes128-ctr or aes256-ctr algorithms are well supported and fast as they are implemented in hardware. These are not usually the default choice when using scp so you will need to manually specify them. A single SSH based transfer will usually not be able to saturate the available network bandwidth or the available disk bandwidth so you may see an overall improvement by running several data transfer operations in parallel. To reduce metadata interactions it is a good idea to overlap transfers of files from different directories. In addition, you should consider the following when transferring data: Only transfer those files that are required. Consider which data you really need to keep. Combine lots of small files into a single tar archive, to reduce the overheads associated in initiating many separate data transfers (over SSH, each file counts as an individual transfer). Compress data before transferring it, e.g. using gzip . scp The scp command creates a copy of a file, or if given the -r flag, a directory either from a local machine onto a remote machine or from a remote machine onto a local machine. For example, to transfer files to Tursa from a local machine: scp [options] source user@login.tursa.ac.uk:[destination] (Remember to replace user with your Tursa username in the example above.) In the above example, the [destination] is optional, as when left out scp will copy the source into your home directory. Also, the source should be the absolute path of the file/directory being copied or the command should be executed in the directory containing the source file/directory. If you want to request a different encryption algorithm add the -c [algorithm-name] flag to the scp options. For example, to use the (usually faster) arcfour encryption algorithm you would use: scp [options] -c aes128-ctr source user@login.tursa.ac.uk:[destination] (Remember to replace user with your Tursa username in the example above.) rsync The rsync command can also transfer data between hosts using a ssh connection. It creates a copy of a file or, if given the -r flag, a directory at the given destination, similar to scp above. Given the -a option rsync can also make exact copies (including permissions), this is referred to as mirroring . In this case the rsync command is executed with ssh to create the copy on a remote machine. To transfer files to Tursa using rsync with ssh the command has the form: rsync [options] -e ssh source user@login.tursa.ac.uk:[destination] (Remember to replace user with your Tursa username in the example above.) In the above example, the [destination] is optional, as when left out rsync will copy the source into your home directory. Also the source should be the absolute path of the file/directory being copied or the command should be executed in the directory containing the source file/directory. Additional flags can be specified for the underlying ssh command by using a quoted string as the argument of the -e flag. e.g. rsync [options] -e \"ssh -c arcfour\" source user@login.tursa.ac.uk:[destination] (Remember to replace user with your Tursa username in the example above.) Tip Further information on using rsync can be found in the rsync manual (accessed via man rsync or at man rsync ). Data transfer via GridFTP Tursa provides a module for grid computing, gct/6.2 , otherwise known as the Globus Grid Community Toolkit v6.2.20201212. This toolkit provides a command line interface for moving data to and from GridFTP servers. Data transfers are managed by the globus-url-copy command. Full details concerning this command's use can be found in the GCT 6.2 GridFTP User's Guide . Please note, the GCT module does not yet support parallel streams. We anticipate having this feature available soon. Please consult the module help ( module help gct/6.2 ) for confirmation of when this work has been completed. SSH data transfer example: laptop/workstation to Tursa Here we have a short example demonstrating transfer of data directly from a laptop/workstation to Tursa. Note This guide assumes you are using a command line interface to transfer data. This means the terminal on Linux or macOS, MobaXterm local terminal on Windows or Powershell. Before we can transfer of data to Tursa we need to make sure we have an SSH key setup to access Tursa from the system we are transferring data from. If you are using the same system that you use to log into Tursa then you should be all set. If you want to use a different system you will need to generate a new SSH key there (or use SSH key forwarding) to allow you to connect to Tursa. Tip Remember that you will need to use both a key and your password to transfer data to Tursa. Once we know our keys are setup correctly, we are now ready to transfer data directly between the two machines. We begin by combining our important research data in to a single archive file using the following command: tar -czf all_my_files.tar.gz file1.txt file2.txt file3.txt We then initiate the data transfer from our system to Tursa, here using rsync to allow the transfer to be recommenced without needing to start again, in the event of a loss of connection or other failure. For example, using the SSH key in the file ~/.ssh/id_RSA_A2 on our local system: rsync -Pv -e\"ssh -c aes128-gcm@openssh.com -i $HOME/.ssh/id_RSA_A2\" ./all_my_files.tar.gz otbz19@login.tursa.ac.uk:/work/z19/z19/otbz19/ Note the use of the -P flag to allow partial transfer -- the same command could be used to restart the transfer after a loss of connection. The -e flag allows specification of the ssh command - we have used this to add the location of the identity file. The -c option specifies the cipher to be used as aes128-gcm which has been found to increase performance Unfortunately the ~ shortcut is not correctly expanded, so we have specified the full path. We move our research archive to our project work directory on Tursa. Note Remember to replace otbz19 with your username on Tursa. If we were unconcerned about being able to restart an interrupted transfer, we could instead use the scp command, scp -c aes128-gcm@openssh.com -i ~/.ssh/id_RSA_A2 all_my_files.tar.gz otbz19@transfer.dyn.tursa.ac.uk:/work/z19/z19/otbz19/ but rsync is recommended for larger transfers.","title":"Data Management and transfer"},{"location":"tursa-user-guide/data/#data-management-and-transfer","text":"This section covers best practice and tools for data management on Tursa. Information If you have any questions on data management and transfer please do not hesitate to contact the DiRAC service desk at dirac-support@epcc.ed.ac.uk .","title":"Data management and transfer"},{"location":"tursa-user-guide/data/#useful-resources-and-links","text":"Harry Mangalam's guide on How to transfer large amounts of data via network . This provides lots of useful advice on transferring data.","title":"Useful resources and links"},{"location":"tursa-user-guide/data/#data-management","text":"We strongly recommend that you give some thought to how you use the various data storage facilities that are part of the Tursa service. This will not only allow you to use the machine more effectively but also to ensure that your valuable data is protected.","title":"Data management"},{"location":"tursa-user-guide/data/#tursa-storage","text":"The Tursa service, like many HPC systems, has a complex structure. There are a number of different data storage types available to users: Home file systems Work file systems Each type of storage has different characteristics and policies, and is suitable for different types of use. There are also two different types of node available to users: Login nodes Compute nodes Each type of node sees a different combination of the storage types. The following table shows which storage options are avalable on different node types: Storage Login Nodes Compute Nodes Notes /home yes no Backed up /work yes yes Not backed up, high performance","title":"Tursa storage"},{"location":"tursa-user-guide/data/#home-file-systems","text":"There are four independent home file-systems. Every project has an allocation on one of the four. You do not need to know which one your project uses as your projects space can always be accessed via the path /home/project-code . Each home file-system is approximately 60TB in size and is implemented using standard Network Attached Storage (NAS) technology. This means that these disks are not particularly high performance but are well suited to standard operations like compilation and file editing. These file systems are visible from the Tursa login nodes. The home file systems are fully backed up . Full backups are taken weekly (for each of the past two weeks), daily (for each of the past two days) and hourly (for each of the last 6 hours). You can access the snapshots at the /home1/.snapshot , /home2/.snapshot , /home3/.snapshot and /home4/.snapshot depending on which of the file systems you have your home directories on. You can find out which file system your home directory is on with the command: readlink -f $HOME These file-systems are a good location to keep source-code, copies of scripts and compiled binaries. Small amounts of important data can also be copied here for safe keeping though the file systems are not fast enough to manipulate large datasets effectively. Warning Files with filenames that contain non-ascii characters and/or non-printable characters cannot be backed up using our automated process and so will be omitted from all backups.","title":"Home file systems"},{"location":"tursa-user-guide/data/#work-file-systems","text":"There is one work file-system: /work 3.4 PB Every project has an allocation on the file system. This is a high-performance, Lustre parallel file system. It is designed to support data in large files. The performance for data stored in large numbers of small files is probably not going to be as good. This is the only file system that is available on the compute nodes so all data read or written by jobs running on the compute nodes has to be hosted here. Warning There are no backups of any data on the work file system. You should not rely on these file systems for long term storage. Ideally, this file system should only contain data that is: actively in use; recently generated and in the process of being saved elsewhere; or being made ready for up-coming work. In practice it may be convenient to keep copies of datasets on the work file system that you know will be needed at a later date. However, make sure that important data is always backed up elsewhere and that your work would not be significantly impacted if the data on the work file system was lost. Large data sets can be moved to the RDF storage or transferred off the Tursa service entirely. If you have data on the work file system that you are not going to need in the future please delete it.","title":"Work file systems"},{"location":"tursa-user-guide/data/#subprojects","text":"Some large projects may choose to split their resources into multiple subprojects. These subprojects will have identifiers appended to the main project ID. For example, the rse subgroup of the z19 project would have the ID z19-rse . If the main project has allocated storage quotas to the subproject the directories for this storage will be found at, for example: /home/z19/z19-rse/auser Your Linux home directory will generally not be changed when you are made a member of a subproject so you must change directories manually (or change the ownership of files) to make use of this different storage quota allocation.","title":"Subprojects"},{"location":"tursa-user-guide/data/#sharing-data-with-other-tursa-users","text":"How you share data with other Tursa users depends on whether or not they belong to the same project as you. Each project has two shared folders that can be used for sharing data.","title":"Sharing data with other Tursa users"},{"location":"tursa-user-guide/data/#sharing-data-with-tursa-users-in-your-project","text":"Each project has an inner shared folder. /work/[project code]/[project code]/shared This folder has read/write permissions for all project members. You can place any data you wish to share with other project members in this directory. For example, if your project code is x01 the inner shared folder would be located at /work/x01/x01/shared .","title":"Sharing data with Tursa users in your project"},{"location":"tursa-user-guide/data/#sharing-data-with-all-tursa-users","text":"Each project also has an outer shared folder.: /work/[project code]/shared It is writable by all project members and readable by any user on the system. You can place any data you wish to share with other Tursa users who are not members of your project in this directory. For example, if your project code is x01 the outer shared folder would be located at /work/x01/shared .","title":"Sharing data with all Tursa users"},{"location":"tursa-user-guide/data/#permissions","text":"You should check the permissions of any files that you place in the shared area, especially if those files were created in your own Tursa account. Files of the latter type are likely to be readable by you only. The chmod command below shows how to make sure that a file placed in the outer shared folder is also readable by all Tursa users. chmod a+r /work/x01/shared/your-shared-file.txt Similarly, for the inner shared folder, chmod can be called such that read permission is granted to all users within the x01 project. chmod g+r /work/x01/x01/shared/your-shared-file.txt If you're sharing a set of files stored within a folder hierarchy the chmod is slightly more complicated. chmod -R a+Xr /work/x01/shared/my-shared-folder chmod -R g+Xr /work/x01/x01/shared/my-shared-folder The -R option ensures that the read permission is enabled recursively and the +X guarantees that the user(s) you're sharing the folder with can access the subdirectories below my-shared-folder .","title":"Permissions"},{"location":"tursa-user-guide/data/#sharing-data-between-projects-and-subprojects","text":"Every file has an owner group that specifies access permissions for users belonging to that group. It's usually the case that the group id is synonymous with the project code. Somewhat confusingly however, projects can contain groups of their own, called subprojects, which can be assigned disk space quotas distinct from the project. chown -R $USER:x01-subproject /work/x01/x01-subproject/$USER/my-folder The chown command above changes the owning group for all the files within my-folder to the x01-subproject group. This might be necessary if previously those files were owned by the x01 group and thereby using some of the x01 disk quota.","title":"Sharing data between projects and subprojects"},{"location":"tursa-user-guide/data/#archiving-and-data-transfer","text":"Data transfer speed may be limited by many different factors so the best data transfer mechanism to use depends on the type of data being transferred and where the data is going. Disk speed - The Tursa /work file system is highly parallel, consisting of a very large number of high performance disk drives. This allows it to support a very high data bandwidth. Unless the remote system has a similar parallel file-system you may find your transfer speed limited by disk performance. Meta-data performance - Meta-data operations such as opening and closing files or listing the owner or size of a file are much less parallel than read/write operations. If your data consists of a very large number of small files you may find your transfer speed is limited by meta-data operations. Meta-data operations performed by other users of the system will interact strongly with those you perform so reducing the number of such operations you use, may reduce variability in your IO timings. Network speed - Data transfer performance can be limited by network speed. More importantly it is limited by the slowest section of the network between source and destination. Firewall speed - Most modern networks are protected by some form of firewall that filters out malicious traffic. This filtering has some overhead and can result in a reduction in data transfer performance. The needs of a general purpose network that hosts email/web-servers and desktop machines are quite different from a research network that needs to support high volume data transfers. If you are trying to transfer data to or from a host on a general purpose network you may find the firewall for that network will limit the transfer rate you can achieve. The method you use to transfer data to/from Tursa will depend on how much you want to transfer and where to. The methods we cover in this guide are: scp/sftp/rsync - These are the simplest methods of transferring data and can be used up to moderate amounts of data. If you are transferring data to your workstation/laptop then this is the method you will use. GridFTP - It is sometimes more convenient to transfer large amounts of data (> 100 GBs) using GridFTP servers. Before discussing specific data transfer methods, we cover archiving which is an essential process for transferring data efficiently.","title":"Archiving and data transfer"},{"location":"tursa-user-guide/data/#archiving","text":"If you have related data that consists of a large number of small files it is strongly recommended to pack the files into a larger \"archive\" file for ease of transfer and manipulation. A single large file makes more efficient use of the file system and is easier to move and copy and transfer because significantly fewer meta-data operations are required. Archive files can be created using tools like tar and zip .","title":"Archiving"},{"location":"tursa-user-guide/data/#tar","text":"The tar command packs files into a \"tape archive\" format. The command has general form: tar [options] [file(s)] Common options include: -c create a new archive -v verbosely list files processed -W verify the archive after writing -l confirm all file hard links are included in the archive -f use an archive file (for historical reasons, tar writes its output to stdout by default rather than a file). Putting these together: tar -cvWlf mydata.tar mydata will create and verify an archive. To extract files from a tar file, the option -x is used. For example: tar -xf mydata.tar will recover the contents of mydata.tar to the current working directory. To verify an existing tar file against a set of data, the -d (diff) option can be used. By default, no output will be given if a verification succeeds and an example of a failed verification follows: $> tar -df mydata.tar mydata/* mydata/damaged_file: Mod time differs mydata/damaged_file: Size differs Note tar files do not store checksums with their data, requiring the original data to be present during verification. Tip Further information on using tar can be found in the tar manual (accessed via man tar or at man tar ).","title":"tar"},{"location":"tursa-user-guide/data/#zip","text":"The zip file format is widely used for archiving files and is supported by most major operating systems. The utility to create zip files can be run from the command line as: zip [options] mydata.zip [file(s)] Common options are: -r used to zip up a directory -# where \"#\" represents a digit ranging from 0 to 9 to specify compression level, 0 being the least and 9 the most. Default compression is -6 but we recommend using -0 to speed up the archiving process. Together: zip -0r mydata.zip mydata will create an archive. Note Unlike tar, zip files do not preserve hard links. File data will be copied on archive creation, e.g. an uncompressed zip archive of a 100MB file and a hard link to that file will be approximately 200MB in size. This makes zip an unsuitable format if you wish to precisely reproduce the file system layout. The corresponding unzip command is used to extract data from the archive. The simplest use case is: unzip mydata.zip which recovers the contents of the archive to the current working directory. Files in a zip archive are stored with a CRC checksum to help detect data loss. unzip provides options for verifying this checksum against the stored files. The relevant flag is -t and is used as follows: $> unzip -t mydata.zip Archive: mydata.zip testing: mydata/ OK testing: mydata/file OK No errors detected in compressed data of mydata.zip. Tip Further information on using zip can be found in the zip manual (accessed via man zip or at man zip ).","title":"zip"},{"location":"tursa-user-guide/data/#data-transfer-via-ssh","text":"The easiest way of transferring data to/from Tursa is to use one of the standard programs based on the SSH protocol such as scp , sftp or rsync . These all use the same underlying mechanism (SSH) as you normally use to log-in to Tursa. So, once the the command has been executed via the command line, you will be prompted for your password for the specified account on the remote machine (Tursa in this case). To avoid having to type in your password multiple times you can set up a SSH key pair and use an SSH agent as documented in the User Guide at connecting .","title":"Data transfer via SSH"},{"location":"tursa-user-guide/data/#ssh-data-transfer-performance-considerations","text":"The SSH protocol encrypts all traffic it sends. This means that file transfer using SSH consumes a relatively large amount of CPU time at both ends of the transfer (for encryption and decryption). The Tursa login nodes have fairly fast processors that can sustain about 100 MB/s transfer. The encryption algorithm used is negotiated between the SSH client and the SSH server. There are command line flags that allow you to specify a preference for which encryption algorithm should be used. You may be able to improve transfer speeds by requesting a different algorithm than the default. The aes128-ctr or aes256-ctr algorithms are well supported and fast as they are implemented in hardware. These are not usually the default choice when using scp so you will need to manually specify them. A single SSH based transfer will usually not be able to saturate the available network bandwidth or the available disk bandwidth so you may see an overall improvement by running several data transfer operations in parallel. To reduce metadata interactions it is a good idea to overlap transfers of files from different directories. In addition, you should consider the following when transferring data: Only transfer those files that are required. Consider which data you really need to keep. Combine lots of small files into a single tar archive, to reduce the overheads associated in initiating many separate data transfers (over SSH, each file counts as an individual transfer). Compress data before transferring it, e.g. using gzip .","title":"SSH data transfer performance considerations"},{"location":"tursa-user-guide/data/#scp","text":"The scp command creates a copy of a file, or if given the -r flag, a directory either from a local machine onto a remote machine or from a remote machine onto a local machine. For example, to transfer files to Tursa from a local machine: scp [options] source user@login.tursa.ac.uk:[destination] (Remember to replace user with your Tursa username in the example above.) In the above example, the [destination] is optional, as when left out scp will copy the source into your home directory. Also, the source should be the absolute path of the file/directory being copied or the command should be executed in the directory containing the source file/directory. If you want to request a different encryption algorithm add the -c [algorithm-name] flag to the scp options. For example, to use the (usually faster) arcfour encryption algorithm you would use: scp [options] -c aes128-ctr source user@login.tursa.ac.uk:[destination] (Remember to replace user with your Tursa username in the example above.)","title":"scp"},{"location":"tursa-user-guide/data/#rsync","text":"The rsync command can also transfer data between hosts using a ssh connection. It creates a copy of a file or, if given the -r flag, a directory at the given destination, similar to scp above. Given the -a option rsync can also make exact copies (including permissions), this is referred to as mirroring . In this case the rsync command is executed with ssh to create the copy on a remote machine. To transfer files to Tursa using rsync with ssh the command has the form: rsync [options] -e ssh source user@login.tursa.ac.uk:[destination] (Remember to replace user with your Tursa username in the example above.) In the above example, the [destination] is optional, as when left out rsync will copy the source into your home directory. Also the source should be the absolute path of the file/directory being copied or the command should be executed in the directory containing the source file/directory. Additional flags can be specified for the underlying ssh command by using a quoted string as the argument of the -e flag. e.g. rsync [options] -e \"ssh -c arcfour\" source user@login.tursa.ac.uk:[destination] (Remember to replace user with your Tursa username in the example above.) Tip Further information on using rsync can be found in the rsync manual (accessed via man rsync or at man rsync ).","title":"rsync"},{"location":"tursa-user-guide/data/#data-transfer-via-gridftp","text":"Tursa provides a module for grid computing, gct/6.2 , otherwise known as the Globus Grid Community Toolkit v6.2.20201212. This toolkit provides a command line interface for moving data to and from GridFTP servers. Data transfers are managed by the globus-url-copy command. Full details concerning this command's use can be found in the GCT 6.2 GridFTP User's Guide . Please note, the GCT module does not yet support parallel streams. We anticipate having this feature available soon. Please consult the module help ( module help gct/6.2 ) for confirmation of when this work has been completed.","title":"Data transfer via GridFTP"},{"location":"tursa-user-guide/data/#ssh-data-transfer-example-laptopworkstation-to-tursa","text":"Here we have a short example demonstrating transfer of data directly from a laptop/workstation to Tursa. Note This guide assumes you are using a command line interface to transfer data. This means the terminal on Linux or macOS, MobaXterm local terminal on Windows or Powershell. Before we can transfer of data to Tursa we need to make sure we have an SSH key setup to access Tursa from the system we are transferring data from. If you are using the same system that you use to log into Tursa then you should be all set. If you want to use a different system you will need to generate a new SSH key there (or use SSH key forwarding) to allow you to connect to Tursa. Tip Remember that you will need to use both a key and your password to transfer data to Tursa. Once we know our keys are setup correctly, we are now ready to transfer data directly between the two machines. We begin by combining our important research data in to a single archive file using the following command: tar -czf all_my_files.tar.gz file1.txt file2.txt file3.txt We then initiate the data transfer from our system to Tursa, here using rsync to allow the transfer to be recommenced without needing to start again, in the event of a loss of connection or other failure. For example, using the SSH key in the file ~/.ssh/id_RSA_A2 on our local system: rsync -Pv -e\"ssh -c aes128-gcm@openssh.com -i $HOME/.ssh/id_RSA_A2\" ./all_my_files.tar.gz otbz19@login.tursa.ac.uk:/work/z19/z19/otbz19/ Note the use of the -P flag to allow partial transfer -- the same command could be used to restart the transfer after a loss of connection. The -e flag allows specification of the ssh command - we have used this to add the location of the identity file. The -c option specifies the cipher to be used as aes128-gcm which has been found to increase performance Unfortunately the ~ shortcut is not correctly expanded, so we have specified the full path. We move our research archive to our project work directory on Tursa. Note Remember to replace otbz19 with your username on Tursa. If we were unconcerned about being able to restart an interrupted transfer, we could instead use the scp command, scp -c aes128-gcm@openssh.com -i ~/.ssh/id_RSA_A2 all_my_files.tar.gz otbz19@transfer.dyn.tursa.ac.uk:/work/z19/z19/otbz19/ but rsync is recommended for larger transfers.","title":"SSH data transfer example: laptop/workstation to Tursa"},{"location":"tursa-user-guide/dev-environment/","text":"Application development environment What's available Tursa runs on the Cray Linux Environment (a version of SUSE Linux), and provides a development environment which includes: Software modules via a standard module framework Three different compiler environments (AMD, Cray, and GNU) MPI, OpenMP, and SHMEM Scientific and numerical libraries Parallel Python and R Parallel debugging and profiling Singularity containers Access to particular software, and particular versions, is managed by a standard TCL module framework. Most software is available via standard software modules and the different programming environments are available via module collections. You can see what programming environments are available with: auser@login01:~> module savelist Named collection list: 1) PrgEnv-aocc 2) PrgEnv-cray 3) PrgEnv-gnu Other software modules can be listed with auser@login01:~> module avail ------------------------------- /opt/cray/pe/perftools/20.09.0/modulefiles -------------------------------- perftools perftools-lite-events perftools-lite-hbm perftools-nwpc perftools-lite perftools-lite-gpu perftools-lite-loops perftools-preload ---------------------------------- /opt/cray/pe/craype/2.7.0/modulefiles ---------------------------------- craype-hugepages1G craype-hugepages8M craype-hugepages128M craype-network-ofi craype-hugepages2G craype-hugepages16M craype-hugepages256M craype-network-slingshot10 craype-hugepages2M craype-hugepages32M craype-hugepages512M craype-x86-rome craype-hugepages4M craype-hugepages64M craype-network-none ------------------------------------- /usr/local/Modules/modulefiles -------------------------------------- dot module-git module-info modules null use.own -------------------------------------- /opt/cray/pe/cpe-prgenv/7.0.0 -------------------------------------- cpe-aocc cpe-cray cpe-gnu -------------------------------------------- /opt/modulefiles --------------------------------------------- aocc/2.1.0.3(default) cray-R/4.0.2.0(default) gcc/8.1.0 gcc/9.3.0 gcc/10.1.0(default) ---------------------------------------- /opt/cray/pe/modulefiles ----------------------------------------- atp/3.7.4(default) cray-mpich-abi/8.0.15 craype-dl-plugin-py3/20.06.1(default) cce/10.0.3(default) cray-mpich-ucx/8.0.15 craype/2.7.0(default) cray-ccdb/4.7.1(default) cray-mpich/8.0.15(default) craypkg-gen/1.3.10(default) cray-cti/2.7.3(default) cray-netcdf-hdf5parallel/4.7.4.0 gdb4hpc/4.7.3(default) cray-dsmml/0.1.2(default) cray-netcdf/4.7.4.0 iobuf/2.0.10(default) cray-fftw/3.3.8.7(default) cray-openshmemx/11.1.1(default) papi/6.0.0.2(default) cray-ga/5.7.0.3 cray-parallel-netcdf/1.12.1.0 perftools-base/20.09.0(default) cray-hdf5-parallel/1.12.0.0 cray-pmi-lib/6.0.6(default) valgrind4hpc/2.7.2(default) cray-hdf5/1.12.0.0 cray-pmi/6.0.6(default) cray-libsci/20.08.1.2(default) cray-python/3.8.5.0(default) A full discussion of the module system is available in the Software environment section . A consistent set of modules is loaded on login to the machine (currently PrgEnv-cray , see below). Developing applications then means selecting and loading the appropriate set of modules before starting work. This section is aimed at code developers and will concentrate on the compilation environment and building libraries and executables, and specifically parallel executables. Other topics such as Python and Containers are covered in more detail in separate sections of the documentation. Managing development Tursa supports common revision control software such as git . Standard GNU autoconf tools are available, along with make (which is GNU Make). Versions of cmake are available. Note Some of these tools are part of the system software, and typically reside in /usr/bin , while others are provided as part of the module system. Some tools may be available in different versions via both /usr/bin and via the module system. Compilation environment There are three different compiler environments available on Tursa: AMD (AOCC), Cray (CCE), and GNU (GCC). The current compiler suite is selected via the programming environment, while the specific compiler versions are determined by the relevant compiler module. A summary is: Suite name Module Programming environment collection CCE cce PrgEnv-cray GCC gcc PrgEnv-gnu AOCC aocc PrgEnv-aocc For example, at login, the default set of modules are: Currently Loaded Modulefiles: 1) cpe-cray 7) cray-dsmml/0.1.2(default) 2) cce/10.0.3(default) 8) perftools-base/20.09.0(default) 3) craype/2.7.0(default) 9) xpmem/2.2.35-7.0.1.0_1.3__gd50fabf.shasta(default) 4) craype-x86-rome 10) cray-mpich/8.0.15(default) 5) libfabric/1.11.0.0.233(default) 11) cray-libsci/20.08.1.2(default) 6) craype-network-ofi from which we see the default programming environment is Cray (indicated by cpe-cray (at 1 in the list above) and the default compiler module is cce/10.0.3 (at 2 in the list above). The programming environment will give access to a consistent set of compiler, MPI library via cray-mpich (at 10), and other libraries e.g., cray-libsci (at 11 in the list above) infrastructure. Within a given programming environment, it is possible to swap to a different compiler version by swapping the relevant compiler module. To ensure consistent behaviour, compilation of C, C++, and Fortran source code should then take place using the appropriate compiler wrapper: cc , CC , and ftn , respectively. The wrapper will automatically call the relevant underlying compiler and add the appropriate include directories and library locations to the invocation. This typically eliminates the need to specify this additional information explicitly in the configuration stage. To see the details of the exact compiler invocation use the -craype-verbose flag to the compiler wrapper. The default link time behaviour is also related to the current programming environment. See the section below on Linking and libraries . Users should not, in general, invoke specific compilers at compile/link stages. In particular, gcc , which may default to /usr/bin/gcc , should not be used. The compiler wrappers cc , CC , and ftn should be used via the appropriate module. Other common MPI compiler wrappers e.g., mpicc should also be replaced by the relevant wrapper cc ( mpicc etc are not available). Important Always use the compiler wrappers cc , CC , and/or ftn and not a specific compiler invocation. This will ensure consistent compile/link time behaviour. Compiler man pages and help Further information on both the compiler wrappers, and the individual compilers themselves are available via the command line, and via standard man pages. The man page for the compiler wrappers is common to all programming environments, while the man page for individual compilers depends on the currently loaded programming environment. The following table summarises options for obtaining information on the compiler and compile options: Compiler suite C C++ Fortran Cray man craycc man crayCC man crayftn GNU man gcc man g++ man gfortran Wrappers man cc man CC man ftn Tip You can also pass the --help option to any of the compilers or wrappers to get a summary of how to use them. The Cray Fortran compiler uses ftn --craype-help to access the help options. Tip There are no man pages for the AOCC compilers at the moment. Tip Cray C/C++ is based on Clang and therefore supports similar options to clang/gcc ( man clang is in fact equivalent to man craycc ). clang --help will produce a full summary of options with Cray-specific options marked \"Cray\". The craycc man page concentrates on these Cray extensions to the clang front end and does not provide an exhaustive description of all clang options. Cray Fortran is not based on Flang and so takes different options from flang/gfortran. Dynamic Linking Executables on Tursa link dynamically, and the Cray Programming Environment does not currently support static linking. This is in contrast to ARCHER where the default was to build statically. If you attempt to link statically, you will see errors similar to: /usr/bin/ld: cannot find -lpmi /usr/bin/ld: cannot find -lpmi2 collect2: error: ld returned 1 exit status The compiler wrapper scripts on ARCHER link runtime libraries in using the runpath by default. This means that the paths to the runtime libraries are encoded into the executable so you do not need to load the compiler environment in your job submission scripts. Which compiler environment? If you are unsure which compiler you should choose, we suggest the starting point should be the GNU compiler collection (GCC, PrgEnv-gnu ); this is perhaps the most commonly used by code developers, particularly in the open source software domain. A portable, standard-conforming code should (in principle) compile in any of the three programming environments. For users requiring specific compiler features, such as co-array Fortran, the recommended starting point would be Cray. The following sections provide further details of the different programming environments. Warning Intel compilers are not available on Tursa. AMD Optimizing C/C++ Compiler (AOCC) The AMD Optimizing C/++ Compiler (AOCC) is a clang-based optimising compiler. AOCC (despite its name) includes a flang-based Fortran compiler. Switch the the AOCC programming environment via $ module restore PrgEnv-aocc Note Further details on AOCC will appear here as they become available. AOCC reference material AMD website https://developer.amd.com/amd-aocc/ Cray compiler environment (CCE) The Cray compiler environment (CCE) is the default compiler at the point of login. CCE supports C/C++ (along with unified parallel C UPC), and Fortran (including co-array Fortran). Support for OpenMP parallelism is available for both C/C++ and Fortran (currently OpenMP 4.5, with a number of exceptions). The Cray C/C++ compiler is based on a clang front end, and so compiler options are similar to those for gcc/clang. However, the Fortran compiler remains based around Cray-specific options. Be sure to separate C/C++ compiler options and Fortran compiler options (typically CFLAGS and FFLAGS ) if compiling mixed C/Fortran applications. Switch the the Cray programming environment via $ module restore PrgEnv-cray Useful CCE C/C++ options When using the compiler wrappers cc or CC , some of the following options may be useful: Language, warning, Debugging options: Option Comment -std=<standard> Default is -std=gnu11 ( gnu++14 for C++) [1] Performance options: Option Comment -Ofast Optimisation levels: -O0, -O1, -O2, -O3, -Ofast -ffp=level Floating point maths optimisations levels 0-4 [2] -flto Link time optimisation Miscellaneous options: Option Comment -fopenmp Compile OpenMP (default is off) -v Display verbose output from compiler stages Notes Option -std=gnu11 gives c11 plus GNU extensions (likewise c++14 plus GNU extensions). See https://gcc.gnu.org/onlinedocs/gcc-4.8.2/gcc/C-Extensions.html Option -ffp=3 is implied by -Ofast or -ffast-math Useful CCE Fortran options Language, Warning, Debugging options: Option Comment -m <level> Message level (default -m 3 errors and warnings) Performance options: Option Comment -O <level> Optimisation levels: -O0 to -O3 (default -O2) -h fp<level> Floating point maths optimisations levels 0-3 -h ipa Inter-procedural analysis Miscellaneous options: Option Comment -h omp Compile OpenMP (default is -hnoomp ) -v Display verbose output from compiler stages GNU compiler collection (GCC) The commonly used open source GNU compiler collection is available and provides C/C++ and Fortran compilers. The GNU compiler collection is loaded by switching to the GNU programming environment: $ module restore PrgEnv-gnu Bug The gcc/8.1.0 module is available on Tursa but cannot be used as the supporting scientific and system libraries are not available. You should not use this version of GCC. Warning If you want to use GCC version 10 or greater to compile Fortran code, with the old MPI interfaces (i.e. use mpi or INCLUDE 'mpif.h' ) you must add the -fallow-argument-mismatch option (or equivalent) when compiling otherwise you will see compile errors associated with MPI functions. The reason for this is that past versions of gfortran have allowed mismatched arguments to external procedures (e.g., where an explicit interface is not available). This is often the case for MPI routines using the old MPI interfaces where arrays of different types are passed to, for example, MPI_Send() . This will now generate an error as not standard conforming. The -fallow-argument-mismatch option is used to reduce the error to a warning. The same effect may be achieved via -std=legacy . If you use the Fortran 2008 MPI interface (i.e. use mpi_f08 ) then you should not need to add this option. Fortran language MPI bindings are described in more detail at in the MPI Standard documentation . Useful Gnu Fortran options Option Comment -std=<standard> Default is gnu -fallow-argument-mismatch Allow mismatched procedure arguments. This argument is required for compiling MPI Fortran code with GCC version 10 or greater if you are using the older MPI interfaces (see warning above) -fbounds-check Use runtime checking of array indices -fopenmp Compile OpenMP (default is no OpenMP) -v Display verbose output from compiler stages Tip The standard in -std may be one of f95 f2003 , f2008 or f2018 . The default option -std=gnu is the latest Fortran standard plus gnu extensions. Warning Past versions of gfortran have allowed mismatched arguments to external procedures (e.g., where an explicit interface is not available). This is often the case for MPI routines where arrays of different types are passed to MPI_Send() and so on. This will now generate an error as not standard conforming. Use -fallow-argument-mismatch to reduce the error to a warning. The same effect may be achieved via -std=legacy . Reference material C/C++ documentation https://gcc.gnu.org/onlinedocs/gcc-9.3.0/gcc/ Fortran documentation https://gcc.gnu.org/onlinedocs/gcc-9.3.0/gfortran/ Message passing interface (MPI) HPE Cray MPICH HPE Cray provide, as standard, an MPICH implementation of the message passing interface which is specifically optimised for the Tursa network. The current implementation supports MPI standard version 3.1. The HPE Cray MPICH implementation is linked into software by default when compiling using the standard wrapper scripts: cc , CC and ftn . MPI reference material MPI standard documents: https://www.mpi-forum.org/docs/ Linking and libraries Linking to libraries is performed dynamically on Tursa. One can use the -craype-verbose flag to the compiler wrapper to check exactly what linker arguments are invoked. The compiler wrapper scripts encode the paths to the programming environment system libraries using RUNPATH. This ensures that the executable can find the correct runtime libraries without the matching software modules loaded. The library RUNPATH associated with an executable can be inspected via, e.g., $ readelf -d ./a.out (swap a.out for the name of the executable you are querying). Commonly used libraries Modules with names prefixed by cray- are provided by HPE Cray, and are supported to be consistent with any of the programming environments and associated compilers. These modules should be the first choice for access to software libraries if available. Tip More information on the different software libraries on Tursa can be found in the Software libraries section of the user guide. Switching to a different HPE Cray Programming Environment release Important See the section below on using non-default versions of HPE Cray libraries below as this process will generally need to be followed when using software from non-default PE installs. Access to non-default PE environments is controlled by the use of the cpe modules. These modules are typically loaded after you have restored a PrgEnv and loaded all the other modules you need and will set your compile environment to match that in the other PE release. This means: The compiler version will be switched to the one from the selected PE HPE Cray provided libraries (or modules) that are loaded before you switch to the new programming environment are switched to those from the programming environment that you select. For example, if you have a code that uses the Gnu programming environment, FFTW and NetCDF parallel libraries and you want to compile in the (non-default) 21.03 programming environment, you would do the following: First, restore the Gnu programming environment and load the required library modules (FFTW and NetCDF HDF5 parallel). The loaded module list shows they are the versions from the default (20.10) programming environment): auser@login02:/work/t01/t01/auser> module restore -s PrgEnv-gnu auser@login02:/work/t01/t01/auser> module load cray-fftw auser@login02:/work/t01/t01/auser> module load cray-netcdf auser@login02:/work/t01/t01/auser> module load cray-netcdf-hdf5parallel auser@login02:/work/t01/t01/auser> module list Currently Loaded Modulefiles: 1) cpe-gnu 9) xpmem/2.2.35-7.0.1.0_1.9__gd50fabf.shasta(default) 2) gcc/10.1.0(default) 10) cray-mpich/8.0.16(default) 3) craype/2.7.2(default) 11) cray-libsci/20.10.1.2(default) 4) craype-x86-rome 12) bolt/0.7 5) libfabric/1.11.0.0.233(default) 13) /work/y07/shared/tursa-modules/modulefiles-cse/epcc-setup-env 6) craype-network-ofi 14) /usr/local/share/epcc-module/epcc-module-loader 7) cray-dsmml/0.1.2(default) 15) cray-fftw/3.3.8.8(default) 8) perftools-base/20.10.0(default) 16) cray-netcdf-hdf5parallel/4.7.4.2(default) Now, load the cpe/21.03 programming environment module to switch all the currently loaded HPE Cray modules from the default (20.10) programming environment version to the 21.03 programming environment versions: auser@login02:/work/t01/t01/auser> module load cpe/21.03 Switching to cray-dsmml/0.1.3. Switching to cray-fftw/3.3.8.9. Switching to cray-libsci/21.03.1.1. Switching to cray-mpich/8.1.3. Switching to cray-netcdf-hdf5parallel/4.7.4.3. Switching to craype/2.7.5. Switching to gcc/9.3.0. Switching to perftools-base/21.02.0. Loading cpe/21.03 Unloading conflict: cray-dsmml/0.1.2 cray-fftw/3.3.8.8 cray-libsci/20.10.1.2 cray-mpich/8.0.16 cray-netcdf-hdf5parallel/4.7.4.2 craype/2.7.2 gcc/10.1.0 perftools-base/20.10.0 Loading requirement: cray-dsmml/0.1.3 cray-fftw/3.3.8.9 cray-libsci/21.03.1.1 cray-mpich/8.1.3 cray-netcdf-hdf5parallel/4.7.4.3 craype/2.7.5 gcc/9.3.0 perftools-base/21.02.0 auser@login02:/work/t01/t01/auser> module list Currently Loaded Modulefiles: 1) cpe-gnu 9) cray-dsmml/0.1.3 17) cpe/21.03(default) 2) craype-x86-rome 10) cray-fftw/3.3.8.9 3) libfabric/1.11.0.0.233(default) 11) cray-libsci/21.03.1.1 4) craype-network-ofi 12) cray-mpich/8.1.3 5) xpmem/2.2.35-7.0.1.0_1.9__gd50fabf.shasta(default) 13) cray-netcdf-hdf5parallel/4.7.4.3 6) bolt/0.7 14) craype/2.7.5 7) /work/y07/shared/tursa-modules/modulefiles-cse/epcc-setup-env 15) gcc/9.3.0 8) /usr/local/share/epcc-module/epcc-module-loader 16) perftools-base/21.02.0 Finally (as noted above), you will need to modify the value of LD_LIBRARY_PATH before you compile your software to ensure it picks up the non-default versions of libraries: auser@login02:/work/t01/t01/auser> export LD_LIBRARY_PATH=$CRAY_LD_LIBRARY_PATH:$LD_LIBRARY_PATH Now you can go ahead and compile your software with the new programming environment. Important The cpe modules only change the versions of software modules provided as part of the HPE Cray programming environments. Any modules provided by the Tursa service will need to be loaded manually after you have completed the process described above. Note Unloading the cpe module does not restore the original programming environment release. To restore the default programming environment release you should log out and then log back in to Tursa. Bug The cpe/21.03 module has a known issue with PrgEnv-gnu where it loads an old version of GCC (9.3.0) rather than the correct, newer version (10.2.0). You can resolve this by using the sequence: module restore -s PrgEnv-gnu ...load any other modules you need... module load cpe/21.03 module unload cpe/21.03 module swap gcc gcc/10.2.0 Available HPE Cray Programming Environment releases on Tursa Tursa currently has the following HPE Cray Programming Environment releases available: 20.08: not available via cpe module 20.10: Current default 21.03: available via cpe/21.03 module Tip You can see which programming environment release you currently have loaded by using module list and looking at the version number of the cray-libsci module you have loaded. The first two numbers indicate the version of the PE you have loaded. For example, if you have cray-libsci/20.10.1.2 loaded then you are using the 20.10 PE release. Using non-default versions of HPE Cray libraries on Tursa If you wish to make use of non-default versions of libraries provided by HPE Cray (usually because they are part of a non-default PE release: either old or new) then you need to make changes at both compile and runtime. In summary, you need to load the correct module and also make changes to the LD_LIBRARY_PATH environment variable. At compile time you need to load the version of the library module before you compile and set the LD_LIBRARY_PATH environment variable to include the contencts of $CRAY_LD_LIBRARY_PATH as the first entry. For example, to use the, non-default, 20.08.1.2 version of HPE Cray LibSci in the default programming environment (Cray Compiler Environment, CCE) you would first setup the environment to compile with: auser@login01:~/test/libsci> module swap cray-libsci cray-libsci/20.08.1.2 auser@login01:~/test/libsci> export LD_LIBRARY_PATH=$CRAY_LD_LIBRARY_PATH:$LD_LIBRARY_PATH The order is important here: every time you change a module, you will need to reset the value of LD_LIBRARY_PATH for the process to work (it will not be updated automatically). Now you can compile your code. You can check that the executable is using the correct version of LibSci with the ldd command and look for the line beginning libsci_cray.so.5 , you should see the version in the path to the library file: auser@login01:~/test/libsci> ldd dgemv.x linux-vdso.so.1 (0x00007ffe4a7d2000) libsci_cray.so.5 => /opt/cray/pe/libsci/20.08.1.2/CRAY/9.0/x86_64/lib/libsci_cray.so.5 (0x00007fafd6a43000) libdl.so.2 => /lib64/libdl.so.2 (0x00007fafd683f000) libxpmem.so.0 => /opt/cray/xpmem/default/lib64/libxpmem.so.0 (0x00007fafd663c000) libquadmath.so.0 => /opt/cray/pe/cce/10.0.4/cce/x86_64/lib/libquadmath.so.0 (0x00007fafd63fc000) libmodules.so.1 => /opt/cray/pe/cce/10.0.4/cce/x86_64/lib/libmodules.so.1 (0x00007fafd61e0000) libfi.so.1 => /opt/cray/pe/cce/10.0.4/cce/x86_64/lib/libfi.so.1 (0x00007fafd5abe000) libcraymath.so.1 => /opt/cray/pe/cce/10.0.4/cce/x86_64/lib/libcraymath.so.1 (0x00007fafd57e2000) libf.so.1 => /opt/cray/pe/cce/10.0.4/cce/x86_64/lib/libf.so.1 (0x00007fafd554f000) libu.so.1 => /opt/cray/pe/cce/10.0.4/cce/x86_64/lib/libu.so.1 (0x00007fafd523b000) libcsup.so.1 => /opt/cray/pe/cce/10.0.4/cce/x86_64/lib/libcsup.so.1 (0x00007fafd5035000) libstdc++.so.6 => /opt/cray/pe/gcc-libs/libstdc++.so.6 (0x00007fafd4c62000) libpthread.so.0 => /lib64/libpthread.so.0 (0x00007fafd4a43000) libc.so.6 => /lib64/libc.so.6 (0x00007fafd4688000) libm.so.6 => /lib64/libm.so.6 (0x00007fafd4350000) /lib64/ld-linux-x86-64.so.2 (0x00007fafda988000) librt.so.1 => /lib64/librt.so.1 (0x00007fafd4148000) libgfortran.so.5 => /opt/cray/pe/gcc-libs/libgfortran.so.5 (0x00007fafd3c92000) libgcc_s.so.1 => /opt/cray/pe/gcc-libs/libgcc_s.so.1 (0x00007fafd3a7a000) Tip If any of the libraries point to versions in the /opt/cray/pe/lib64 directory then these are using the default versions of the libraries rather than the specific versions. This happens at compile time if you have forgotton to load the right module and set $LD_LIBRARY_PATH afterwards. At run time (typically in your job script) you need to repeat the environment setup steps (you can also use the ldd command in your job submission script to check the library is pointing to the correct version). For example, a job submission script to run our dgemv.x executable with the non-default version of LibSci could look like: #!/bin/bash #SBATCH --job-name=dgemv #SBATCH --time=0:20:0 #SBATCH --nodes=1 #SBATCH --tasks-per-node=1 #SBATCH --cpus-per-task=1 # Replace the account code, partition and QoS with those you wish to use #SBATCH --account=t01 #SBATCH --partition=standard #SBATCH --qos=short #SBATCH --reservation=shortqos # Load the standard environment module module load epcc-job-env # Setup up the environment to use the non-default version of LibSci # We use \"module swap\" as the \"cray-libsci\" is loaded by default. # This must be done after loading the \"epcc-job-env\" module module swap cray-libsci cray-libsci/20.08.1.2 export LD_LIBRARY_PATH=$CRAY_LD_LIBRARY_PATH:$LD_LIBRARY_PATH # Check which library versions the executable is pointing too ldd dgemv.x export OMP_NUM_THREADS=1 srun --hint=nomultithread --distribution=block:block dgemv.x Tip As when compiling, the order of commands matters. Setting the value of LD_LIBRARY_PATH must happen after you have finished all your module commands for it to have the correct effect. Important You must setup the environment at both compile and run time otherwise you will end up using the default version of the library. Compiling in compute nodes Sometimes you may wish to compile in a batch job. For example, the compile process may take a long time or the compile process is part of the research workflow and can be coupled to the production job. Unlike login nodes, the /home file system is not available. An example job submission script for a compile job using make (assuming the Makefile is in the same directory as the job submission script) would be: #!/bin/bash #SBATCH --job-name=compile #SBATCH --time=00:20:00 #SBATCH --nodes=1 #SBATCH --tasks-per-node=1 #SBATCH --cpus-per-task=1 # Replace the account code, partition and QoS with those you wish to use #SBATCH --account=t01 #SBATCH --partition=standard #SBATCH --qos=standard # Load the compilation environment (cray, gnu or aocc) module restore /etc/cray-pe.d/PrgEnv-cray make clean make Warning Do not forget to include the full path when the compilation environment is restored. For instance: module restore /etc/cray-pe.d/PrgEnv-cray You can also use a compute node in an interactive way using salloc . Please see Section Using salloc to reserve resources for further details. Once your interactive session is ready, you can load the compilation environment and compile the code. Build instructions for software on Tursa The Tursa CSE team at EPCC and other contributors provide build configurations ando instructions for a range of research software, software libraries and tools on a variety of HPC systems (including Tursa) in a public Github repository. See: Build instructions repository The repository always welcomes contributions from the Tursa user community. Support for building software on Tursa If you run into issues building software on Tursa or the software you require is not available then please contact the Tursa Service Desk with any questions you have.","title":"Application development environment"},{"location":"tursa-user-guide/dev-environment/#application-development-environment","text":"","title":"Application development environment"},{"location":"tursa-user-guide/dev-environment/#whats-available","text":"Tursa runs on the Cray Linux Environment (a version of SUSE Linux), and provides a development environment which includes: Software modules via a standard module framework Three different compiler environments (AMD, Cray, and GNU) MPI, OpenMP, and SHMEM Scientific and numerical libraries Parallel Python and R Parallel debugging and profiling Singularity containers Access to particular software, and particular versions, is managed by a standard TCL module framework. Most software is available via standard software modules and the different programming environments are available via module collections. You can see what programming environments are available with: auser@login01:~> module savelist Named collection list: 1) PrgEnv-aocc 2) PrgEnv-cray 3) PrgEnv-gnu Other software modules can be listed with auser@login01:~> module avail ------------------------------- /opt/cray/pe/perftools/20.09.0/modulefiles -------------------------------- perftools perftools-lite-events perftools-lite-hbm perftools-nwpc perftools-lite perftools-lite-gpu perftools-lite-loops perftools-preload ---------------------------------- /opt/cray/pe/craype/2.7.0/modulefiles ---------------------------------- craype-hugepages1G craype-hugepages8M craype-hugepages128M craype-network-ofi craype-hugepages2G craype-hugepages16M craype-hugepages256M craype-network-slingshot10 craype-hugepages2M craype-hugepages32M craype-hugepages512M craype-x86-rome craype-hugepages4M craype-hugepages64M craype-network-none ------------------------------------- /usr/local/Modules/modulefiles -------------------------------------- dot module-git module-info modules null use.own -------------------------------------- /opt/cray/pe/cpe-prgenv/7.0.0 -------------------------------------- cpe-aocc cpe-cray cpe-gnu -------------------------------------------- /opt/modulefiles --------------------------------------------- aocc/2.1.0.3(default) cray-R/4.0.2.0(default) gcc/8.1.0 gcc/9.3.0 gcc/10.1.0(default) ---------------------------------------- /opt/cray/pe/modulefiles ----------------------------------------- atp/3.7.4(default) cray-mpich-abi/8.0.15 craype-dl-plugin-py3/20.06.1(default) cce/10.0.3(default) cray-mpich-ucx/8.0.15 craype/2.7.0(default) cray-ccdb/4.7.1(default) cray-mpich/8.0.15(default) craypkg-gen/1.3.10(default) cray-cti/2.7.3(default) cray-netcdf-hdf5parallel/4.7.4.0 gdb4hpc/4.7.3(default) cray-dsmml/0.1.2(default) cray-netcdf/4.7.4.0 iobuf/2.0.10(default) cray-fftw/3.3.8.7(default) cray-openshmemx/11.1.1(default) papi/6.0.0.2(default) cray-ga/5.7.0.3 cray-parallel-netcdf/1.12.1.0 perftools-base/20.09.0(default) cray-hdf5-parallel/1.12.0.0 cray-pmi-lib/6.0.6(default) valgrind4hpc/2.7.2(default) cray-hdf5/1.12.0.0 cray-pmi/6.0.6(default) cray-libsci/20.08.1.2(default) cray-python/3.8.5.0(default) A full discussion of the module system is available in the Software environment section . A consistent set of modules is loaded on login to the machine (currently PrgEnv-cray , see below). Developing applications then means selecting and loading the appropriate set of modules before starting work. This section is aimed at code developers and will concentrate on the compilation environment and building libraries and executables, and specifically parallel executables. Other topics such as Python and Containers are covered in more detail in separate sections of the documentation.","title":"What's available"},{"location":"tursa-user-guide/dev-environment/#managing-development","text":"Tursa supports common revision control software such as git . Standard GNU autoconf tools are available, along with make (which is GNU Make). Versions of cmake are available. Note Some of these tools are part of the system software, and typically reside in /usr/bin , while others are provided as part of the module system. Some tools may be available in different versions via both /usr/bin and via the module system.","title":"Managing development"},{"location":"tursa-user-guide/dev-environment/#compilation-environment","text":"There are three different compiler environments available on Tursa: AMD (AOCC), Cray (CCE), and GNU (GCC). The current compiler suite is selected via the programming environment, while the specific compiler versions are determined by the relevant compiler module. A summary is: Suite name Module Programming environment collection CCE cce PrgEnv-cray GCC gcc PrgEnv-gnu AOCC aocc PrgEnv-aocc For example, at login, the default set of modules are: Currently Loaded Modulefiles: 1) cpe-cray 7) cray-dsmml/0.1.2(default) 2) cce/10.0.3(default) 8) perftools-base/20.09.0(default) 3) craype/2.7.0(default) 9) xpmem/2.2.35-7.0.1.0_1.3__gd50fabf.shasta(default) 4) craype-x86-rome 10) cray-mpich/8.0.15(default) 5) libfabric/1.11.0.0.233(default) 11) cray-libsci/20.08.1.2(default) 6) craype-network-ofi from which we see the default programming environment is Cray (indicated by cpe-cray (at 1 in the list above) and the default compiler module is cce/10.0.3 (at 2 in the list above). The programming environment will give access to a consistent set of compiler, MPI library via cray-mpich (at 10), and other libraries e.g., cray-libsci (at 11 in the list above) infrastructure. Within a given programming environment, it is possible to swap to a different compiler version by swapping the relevant compiler module. To ensure consistent behaviour, compilation of C, C++, and Fortran source code should then take place using the appropriate compiler wrapper: cc , CC , and ftn , respectively. The wrapper will automatically call the relevant underlying compiler and add the appropriate include directories and library locations to the invocation. This typically eliminates the need to specify this additional information explicitly in the configuration stage. To see the details of the exact compiler invocation use the -craype-verbose flag to the compiler wrapper. The default link time behaviour is also related to the current programming environment. See the section below on Linking and libraries . Users should not, in general, invoke specific compilers at compile/link stages. In particular, gcc , which may default to /usr/bin/gcc , should not be used. The compiler wrappers cc , CC , and ftn should be used via the appropriate module. Other common MPI compiler wrappers e.g., mpicc should also be replaced by the relevant wrapper cc ( mpicc etc are not available). Important Always use the compiler wrappers cc , CC , and/or ftn and not a specific compiler invocation. This will ensure consistent compile/link time behaviour.","title":"Compilation environment"},{"location":"tursa-user-guide/dev-environment/#compiler-man-pages-and-help","text":"Further information on both the compiler wrappers, and the individual compilers themselves are available via the command line, and via standard man pages. The man page for the compiler wrappers is common to all programming environments, while the man page for individual compilers depends on the currently loaded programming environment. The following table summarises options for obtaining information on the compiler and compile options: Compiler suite C C++ Fortran Cray man craycc man crayCC man crayftn GNU man gcc man g++ man gfortran Wrappers man cc man CC man ftn Tip You can also pass the --help option to any of the compilers or wrappers to get a summary of how to use them. The Cray Fortran compiler uses ftn --craype-help to access the help options. Tip There are no man pages for the AOCC compilers at the moment. Tip Cray C/C++ is based on Clang and therefore supports similar options to clang/gcc ( man clang is in fact equivalent to man craycc ). clang --help will produce a full summary of options with Cray-specific options marked \"Cray\". The craycc man page concentrates on these Cray extensions to the clang front end and does not provide an exhaustive description of all clang options. Cray Fortran is not based on Flang and so takes different options from flang/gfortran.","title":"Compiler man pages and help"},{"location":"tursa-user-guide/dev-environment/#dynamic-linking","text":"Executables on Tursa link dynamically, and the Cray Programming Environment does not currently support static linking. This is in contrast to ARCHER where the default was to build statically. If you attempt to link statically, you will see errors similar to: /usr/bin/ld: cannot find -lpmi /usr/bin/ld: cannot find -lpmi2 collect2: error: ld returned 1 exit status The compiler wrapper scripts on ARCHER link runtime libraries in using the runpath by default. This means that the paths to the runtime libraries are encoded into the executable so you do not need to load the compiler environment in your job submission scripts.","title":"Dynamic Linking"},{"location":"tursa-user-guide/dev-environment/#which-compiler-environment","text":"If you are unsure which compiler you should choose, we suggest the starting point should be the GNU compiler collection (GCC, PrgEnv-gnu ); this is perhaps the most commonly used by code developers, particularly in the open source software domain. A portable, standard-conforming code should (in principle) compile in any of the three programming environments. For users requiring specific compiler features, such as co-array Fortran, the recommended starting point would be Cray. The following sections provide further details of the different programming environments. Warning Intel compilers are not available on Tursa.","title":"Which compiler environment?"},{"location":"tursa-user-guide/dev-environment/#amd-optimizing-cc-compiler-aocc","text":"The AMD Optimizing C/++ Compiler (AOCC) is a clang-based optimising compiler. AOCC (despite its name) includes a flang-based Fortran compiler. Switch the the AOCC programming environment via $ module restore PrgEnv-aocc Note Further details on AOCC will appear here as they become available.","title":"AMD Optimizing C/C++ Compiler (AOCC)"},{"location":"tursa-user-guide/dev-environment/#aocc-reference-material","text":"AMD website https://developer.amd.com/amd-aocc/","title":"AOCC reference material"},{"location":"tursa-user-guide/dev-environment/#cray-compiler-environment-cce","text":"The Cray compiler environment (CCE) is the default compiler at the point of login. CCE supports C/C++ (along with unified parallel C UPC), and Fortran (including co-array Fortran). Support for OpenMP parallelism is available for both C/C++ and Fortran (currently OpenMP 4.5, with a number of exceptions). The Cray C/C++ compiler is based on a clang front end, and so compiler options are similar to those for gcc/clang. However, the Fortran compiler remains based around Cray-specific options. Be sure to separate C/C++ compiler options and Fortran compiler options (typically CFLAGS and FFLAGS ) if compiling mixed C/Fortran applications. Switch the the Cray programming environment via $ module restore PrgEnv-cray","title":"Cray compiler environment (CCE)"},{"location":"tursa-user-guide/dev-environment/#useful-cce-cc-options","text":"When using the compiler wrappers cc or CC , some of the following options may be useful: Language, warning, Debugging options: Option Comment -std=<standard> Default is -std=gnu11 ( gnu++14 for C++) [1] Performance options: Option Comment -Ofast Optimisation levels: -O0, -O1, -O2, -O3, -Ofast -ffp=level Floating point maths optimisations levels 0-4 [2] -flto Link time optimisation Miscellaneous options: Option Comment -fopenmp Compile OpenMP (default is off) -v Display verbose output from compiler stages Notes Option -std=gnu11 gives c11 plus GNU extensions (likewise c++14 plus GNU extensions). See https://gcc.gnu.org/onlinedocs/gcc-4.8.2/gcc/C-Extensions.html Option -ffp=3 is implied by -Ofast or -ffast-math","title":"Useful CCE C/C++ options"},{"location":"tursa-user-guide/dev-environment/#useful-cce-fortran-options","text":"Language, Warning, Debugging options: Option Comment -m <level> Message level (default -m 3 errors and warnings) Performance options: Option Comment -O <level> Optimisation levels: -O0 to -O3 (default -O2) -h fp<level> Floating point maths optimisations levels 0-3 -h ipa Inter-procedural analysis Miscellaneous options: Option Comment -h omp Compile OpenMP (default is -hnoomp ) -v Display verbose output from compiler stages","title":"Useful CCE Fortran options"},{"location":"tursa-user-guide/dev-environment/#gnu-compiler-collection-gcc","text":"The commonly used open source GNU compiler collection is available and provides C/C++ and Fortran compilers. The GNU compiler collection is loaded by switching to the GNU programming environment: $ module restore PrgEnv-gnu Bug The gcc/8.1.0 module is available on Tursa but cannot be used as the supporting scientific and system libraries are not available. You should not use this version of GCC. Warning If you want to use GCC version 10 or greater to compile Fortran code, with the old MPI interfaces (i.e. use mpi or INCLUDE 'mpif.h' ) you must add the -fallow-argument-mismatch option (or equivalent) when compiling otherwise you will see compile errors associated with MPI functions. The reason for this is that past versions of gfortran have allowed mismatched arguments to external procedures (e.g., where an explicit interface is not available). This is often the case for MPI routines using the old MPI interfaces where arrays of different types are passed to, for example, MPI_Send() . This will now generate an error as not standard conforming. The -fallow-argument-mismatch option is used to reduce the error to a warning. The same effect may be achieved via -std=legacy . If you use the Fortran 2008 MPI interface (i.e. use mpi_f08 ) then you should not need to add this option. Fortran language MPI bindings are described in more detail at in the MPI Standard documentation .","title":"GNU compiler collection (GCC)"},{"location":"tursa-user-guide/dev-environment/#useful-gnu-fortran-options","text":"Option Comment -std=<standard> Default is gnu -fallow-argument-mismatch Allow mismatched procedure arguments. This argument is required for compiling MPI Fortran code with GCC version 10 or greater if you are using the older MPI interfaces (see warning above) -fbounds-check Use runtime checking of array indices -fopenmp Compile OpenMP (default is no OpenMP) -v Display verbose output from compiler stages Tip The standard in -std may be one of f95 f2003 , f2008 or f2018 . The default option -std=gnu is the latest Fortran standard plus gnu extensions. Warning Past versions of gfortran have allowed mismatched arguments to external procedures (e.g., where an explicit interface is not available). This is often the case for MPI routines where arrays of different types are passed to MPI_Send() and so on. This will now generate an error as not standard conforming. Use -fallow-argument-mismatch to reduce the error to a warning. The same effect may be achieved via -std=legacy .","title":"Useful Gnu Fortran options"},{"location":"tursa-user-guide/dev-environment/#reference-material","text":"C/C++ documentation https://gcc.gnu.org/onlinedocs/gcc-9.3.0/gcc/ Fortran documentation https://gcc.gnu.org/onlinedocs/gcc-9.3.0/gfortran/","title":"Reference material"},{"location":"tursa-user-guide/dev-environment/#message-passing-interface-mpi","text":"","title":"Message passing interface (MPI)"},{"location":"tursa-user-guide/dev-environment/#hpe-cray-mpich","text":"HPE Cray provide, as standard, an MPICH implementation of the message passing interface which is specifically optimised for the Tursa network. The current implementation supports MPI standard version 3.1. The HPE Cray MPICH implementation is linked into software by default when compiling using the standard wrapper scripts: cc , CC and ftn .","title":"HPE Cray MPICH"},{"location":"tursa-user-guide/dev-environment/#mpi-reference-material","text":"MPI standard documents: https://www.mpi-forum.org/docs/","title":"MPI reference material"},{"location":"tursa-user-guide/dev-environment/#linking-and-libraries","text":"Linking to libraries is performed dynamically on Tursa. One can use the -craype-verbose flag to the compiler wrapper to check exactly what linker arguments are invoked. The compiler wrapper scripts encode the paths to the programming environment system libraries using RUNPATH. This ensures that the executable can find the correct runtime libraries without the matching software modules loaded. The library RUNPATH associated with an executable can be inspected via, e.g., $ readelf -d ./a.out (swap a.out for the name of the executable you are querying).","title":"Linking and libraries"},{"location":"tursa-user-guide/dev-environment/#commonly-used-libraries","text":"Modules with names prefixed by cray- are provided by HPE Cray, and are supported to be consistent with any of the programming environments and associated compilers. These modules should be the first choice for access to software libraries if available. Tip More information on the different software libraries on Tursa can be found in the Software libraries section of the user guide.","title":"Commonly used libraries"},{"location":"tursa-user-guide/dev-environment/#switching-to-a-different-hpe-cray-programming-environment-release","text":"Important See the section below on using non-default versions of HPE Cray libraries below as this process will generally need to be followed when using software from non-default PE installs. Access to non-default PE environments is controlled by the use of the cpe modules. These modules are typically loaded after you have restored a PrgEnv and loaded all the other modules you need and will set your compile environment to match that in the other PE release. This means: The compiler version will be switched to the one from the selected PE HPE Cray provided libraries (or modules) that are loaded before you switch to the new programming environment are switched to those from the programming environment that you select. For example, if you have a code that uses the Gnu programming environment, FFTW and NetCDF parallel libraries and you want to compile in the (non-default) 21.03 programming environment, you would do the following: First, restore the Gnu programming environment and load the required library modules (FFTW and NetCDF HDF5 parallel). The loaded module list shows they are the versions from the default (20.10) programming environment): auser@login02:/work/t01/t01/auser> module restore -s PrgEnv-gnu auser@login02:/work/t01/t01/auser> module load cray-fftw auser@login02:/work/t01/t01/auser> module load cray-netcdf auser@login02:/work/t01/t01/auser> module load cray-netcdf-hdf5parallel auser@login02:/work/t01/t01/auser> module list Currently Loaded Modulefiles: 1) cpe-gnu 9) xpmem/2.2.35-7.0.1.0_1.9__gd50fabf.shasta(default) 2) gcc/10.1.0(default) 10) cray-mpich/8.0.16(default) 3) craype/2.7.2(default) 11) cray-libsci/20.10.1.2(default) 4) craype-x86-rome 12) bolt/0.7 5) libfabric/1.11.0.0.233(default) 13) /work/y07/shared/tursa-modules/modulefiles-cse/epcc-setup-env 6) craype-network-ofi 14) /usr/local/share/epcc-module/epcc-module-loader 7) cray-dsmml/0.1.2(default) 15) cray-fftw/3.3.8.8(default) 8) perftools-base/20.10.0(default) 16) cray-netcdf-hdf5parallel/4.7.4.2(default) Now, load the cpe/21.03 programming environment module to switch all the currently loaded HPE Cray modules from the default (20.10) programming environment version to the 21.03 programming environment versions: auser@login02:/work/t01/t01/auser> module load cpe/21.03 Switching to cray-dsmml/0.1.3. Switching to cray-fftw/3.3.8.9. Switching to cray-libsci/21.03.1.1. Switching to cray-mpich/8.1.3. Switching to cray-netcdf-hdf5parallel/4.7.4.3. Switching to craype/2.7.5. Switching to gcc/9.3.0. Switching to perftools-base/21.02.0. Loading cpe/21.03 Unloading conflict: cray-dsmml/0.1.2 cray-fftw/3.3.8.8 cray-libsci/20.10.1.2 cray-mpich/8.0.16 cray-netcdf-hdf5parallel/4.7.4.2 craype/2.7.2 gcc/10.1.0 perftools-base/20.10.0 Loading requirement: cray-dsmml/0.1.3 cray-fftw/3.3.8.9 cray-libsci/21.03.1.1 cray-mpich/8.1.3 cray-netcdf-hdf5parallel/4.7.4.3 craype/2.7.5 gcc/9.3.0 perftools-base/21.02.0 auser@login02:/work/t01/t01/auser> module list Currently Loaded Modulefiles: 1) cpe-gnu 9) cray-dsmml/0.1.3 17) cpe/21.03(default) 2) craype-x86-rome 10) cray-fftw/3.3.8.9 3) libfabric/1.11.0.0.233(default) 11) cray-libsci/21.03.1.1 4) craype-network-ofi 12) cray-mpich/8.1.3 5) xpmem/2.2.35-7.0.1.0_1.9__gd50fabf.shasta(default) 13) cray-netcdf-hdf5parallel/4.7.4.3 6) bolt/0.7 14) craype/2.7.5 7) /work/y07/shared/tursa-modules/modulefiles-cse/epcc-setup-env 15) gcc/9.3.0 8) /usr/local/share/epcc-module/epcc-module-loader 16) perftools-base/21.02.0 Finally (as noted above), you will need to modify the value of LD_LIBRARY_PATH before you compile your software to ensure it picks up the non-default versions of libraries: auser@login02:/work/t01/t01/auser> export LD_LIBRARY_PATH=$CRAY_LD_LIBRARY_PATH:$LD_LIBRARY_PATH Now you can go ahead and compile your software with the new programming environment. Important The cpe modules only change the versions of software modules provided as part of the HPE Cray programming environments. Any modules provided by the Tursa service will need to be loaded manually after you have completed the process described above. Note Unloading the cpe module does not restore the original programming environment release. To restore the default programming environment release you should log out and then log back in to Tursa. Bug The cpe/21.03 module has a known issue with PrgEnv-gnu where it loads an old version of GCC (9.3.0) rather than the correct, newer version (10.2.0). You can resolve this by using the sequence: module restore -s PrgEnv-gnu ...load any other modules you need... module load cpe/21.03 module unload cpe/21.03 module swap gcc gcc/10.2.0","title":"Switching to a different HPE Cray Programming Environment release"},{"location":"tursa-user-guide/dev-environment/#available-hpe-cray-programming-environment-releases-on-tursa","text":"Tursa currently has the following HPE Cray Programming Environment releases available: 20.08: not available via cpe module 20.10: Current default 21.03: available via cpe/21.03 module Tip You can see which programming environment release you currently have loaded by using module list and looking at the version number of the cray-libsci module you have loaded. The first two numbers indicate the version of the PE you have loaded. For example, if you have cray-libsci/20.10.1.2 loaded then you are using the 20.10 PE release.","title":"Available HPE Cray Programming Environment releases on Tursa"},{"location":"tursa-user-guide/dev-environment/#using-non-default-versions-of-hpe-cray-libraries-on-tursa","text":"If you wish to make use of non-default versions of libraries provided by HPE Cray (usually because they are part of a non-default PE release: either old or new) then you need to make changes at both compile and runtime. In summary, you need to load the correct module and also make changes to the LD_LIBRARY_PATH environment variable. At compile time you need to load the version of the library module before you compile and set the LD_LIBRARY_PATH environment variable to include the contencts of $CRAY_LD_LIBRARY_PATH as the first entry. For example, to use the, non-default, 20.08.1.2 version of HPE Cray LibSci in the default programming environment (Cray Compiler Environment, CCE) you would first setup the environment to compile with: auser@login01:~/test/libsci> module swap cray-libsci cray-libsci/20.08.1.2 auser@login01:~/test/libsci> export LD_LIBRARY_PATH=$CRAY_LD_LIBRARY_PATH:$LD_LIBRARY_PATH The order is important here: every time you change a module, you will need to reset the value of LD_LIBRARY_PATH for the process to work (it will not be updated automatically). Now you can compile your code. You can check that the executable is using the correct version of LibSci with the ldd command and look for the line beginning libsci_cray.so.5 , you should see the version in the path to the library file: auser@login01:~/test/libsci> ldd dgemv.x linux-vdso.so.1 (0x00007ffe4a7d2000) libsci_cray.so.5 => /opt/cray/pe/libsci/20.08.1.2/CRAY/9.0/x86_64/lib/libsci_cray.so.5 (0x00007fafd6a43000) libdl.so.2 => /lib64/libdl.so.2 (0x00007fafd683f000) libxpmem.so.0 => /opt/cray/xpmem/default/lib64/libxpmem.so.0 (0x00007fafd663c000) libquadmath.so.0 => /opt/cray/pe/cce/10.0.4/cce/x86_64/lib/libquadmath.so.0 (0x00007fafd63fc000) libmodules.so.1 => /opt/cray/pe/cce/10.0.4/cce/x86_64/lib/libmodules.so.1 (0x00007fafd61e0000) libfi.so.1 => /opt/cray/pe/cce/10.0.4/cce/x86_64/lib/libfi.so.1 (0x00007fafd5abe000) libcraymath.so.1 => /opt/cray/pe/cce/10.0.4/cce/x86_64/lib/libcraymath.so.1 (0x00007fafd57e2000) libf.so.1 => /opt/cray/pe/cce/10.0.4/cce/x86_64/lib/libf.so.1 (0x00007fafd554f000) libu.so.1 => /opt/cray/pe/cce/10.0.4/cce/x86_64/lib/libu.so.1 (0x00007fafd523b000) libcsup.so.1 => /opt/cray/pe/cce/10.0.4/cce/x86_64/lib/libcsup.so.1 (0x00007fafd5035000) libstdc++.so.6 => /opt/cray/pe/gcc-libs/libstdc++.so.6 (0x00007fafd4c62000) libpthread.so.0 => /lib64/libpthread.so.0 (0x00007fafd4a43000) libc.so.6 => /lib64/libc.so.6 (0x00007fafd4688000) libm.so.6 => /lib64/libm.so.6 (0x00007fafd4350000) /lib64/ld-linux-x86-64.so.2 (0x00007fafda988000) librt.so.1 => /lib64/librt.so.1 (0x00007fafd4148000) libgfortran.so.5 => /opt/cray/pe/gcc-libs/libgfortran.so.5 (0x00007fafd3c92000) libgcc_s.so.1 => /opt/cray/pe/gcc-libs/libgcc_s.so.1 (0x00007fafd3a7a000) Tip If any of the libraries point to versions in the /opt/cray/pe/lib64 directory then these are using the default versions of the libraries rather than the specific versions. This happens at compile time if you have forgotton to load the right module and set $LD_LIBRARY_PATH afterwards. At run time (typically in your job script) you need to repeat the environment setup steps (you can also use the ldd command in your job submission script to check the library is pointing to the correct version). For example, a job submission script to run our dgemv.x executable with the non-default version of LibSci could look like: #!/bin/bash #SBATCH --job-name=dgemv #SBATCH --time=0:20:0 #SBATCH --nodes=1 #SBATCH --tasks-per-node=1 #SBATCH --cpus-per-task=1 # Replace the account code, partition and QoS with those you wish to use #SBATCH --account=t01 #SBATCH --partition=standard #SBATCH --qos=short #SBATCH --reservation=shortqos # Load the standard environment module module load epcc-job-env # Setup up the environment to use the non-default version of LibSci # We use \"module swap\" as the \"cray-libsci\" is loaded by default. # This must be done after loading the \"epcc-job-env\" module module swap cray-libsci cray-libsci/20.08.1.2 export LD_LIBRARY_PATH=$CRAY_LD_LIBRARY_PATH:$LD_LIBRARY_PATH # Check which library versions the executable is pointing too ldd dgemv.x export OMP_NUM_THREADS=1 srun --hint=nomultithread --distribution=block:block dgemv.x Tip As when compiling, the order of commands matters. Setting the value of LD_LIBRARY_PATH must happen after you have finished all your module commands for it to have the correct effect. Important You must setup the environment at both compile and run time otherwise you will end up using the default version of the library.","title":"Using non-default versions of HPE Cray libraries on Tursa"},{"location":"tursa-user-guide/dev-environment/#compiling-in-compute-nodes","text":"Sometimes you may wish to compile in a batch job. For example, the compile process may take a long time or the compile process is part of the research workflow and can be coupled to the production job. Unlike login nodes, the /home file system is not available. An example job submission script for a compile job using make (assuming the Makefile is in the same directory as the job submission script) would be: #!/bin/bash #SBATCH --job-name=compile #SBATCH --time=00:20:00 #SBATCH --nodes=1 #SBATCH --tasks-per-node=1 #SBATCH --cpus-per-task=1 # Replace the account code, partition and QoS with those you wish to use #SBATCH --account=t01 #SBATCH --partition=standard #SBATCH --qos=standard # Load the compilation environment (cray, gnu or aocc) module restore /etc/cray-pe.d/PrgEnv-cray make clean make Warning Do not forget to include the full path when the compilation environment is restored. For instance: module restore /etc/cray-pe.d/PrgEnv-cray You can also use a compute node in an interactive way using salloc . Please see Section Using salloc to reserve resources for further details. Once your interactive session is ready, you can load the compilation environment and compile the code.","title":"Compiling in compute nodes"},{"location":"tursa-user-guide/dev-environment/#build-instructions-for-software-on-tursa","text":"The Tursa CSE team at EPCC and other contributors provide build configurations ando instructions for a range of research software, software libraries and tools on a variety of HPC systems (including Tursa) in a public Github repository. See: Build instructions repository The repository always welcomes contributions from the Tursa user community.","title":"Build instructions for software on Tursa"},{"location":"tursa-user-guide/dev-environment/#support-for-building-software-on-tursa","text":"If you run into issues building software on Tursa or the software you require is not available then please contact the Tursa Service Desk with any questions you have.","title":"Support for building software on Tursa"},{"location":"tursa-user-guide/io/","text":"I/O and file systems Using the ARCHER2 file systems Different file systems are configured for different purposes and performance. ARCHER2 has three file systems available to users: Node type Available file systems Login /home, /work Compute /work Warning Any data used in a parallel jobs should be located on /work (Lustre). Home file systems Home directories provide a convenient means for a user to have access to files such as source files, input files or configuration files. This file system is only mounted on the login nodes. The home directory for each user is located at: /home/[project code]/[group code]/[username] where [project code] is the code for your project (e.g., x01); [group code] is the code for your project group, if your project has groups, (e.g. x01-a) or the same as the project code, if not; [username] is your login name. Each project is allocated a portion of the total storage available, and the project PI will be able to sub-divide this quota among the groups and users within the project. As is standard practice on UNIX and Linux systems, the environment variable $HOME is automatically set to point to your home directory. It should be noted that the home file system is not designed, and does not have the capacity, to act as a long term archive for large sets of results. Work file system Warning There is no backup of data on any of the work file systems, which means that in the event of a major hardware failure, or if a user accidently deletes essential data, it will not be possible to recover the lost files. A high-performance Lustre file system is mounted on the compute nodes. All parallel calculations must be run from directories on the /work file system and all files required by the calculation (apart from the executable) must reside on /work . Each project will be assigned space on a particular Lustre partition with the assignments chosen to balance the load across the available infrastructure. The work directory for each user is located at: /work/[project code]/[group code]/[username] where [project code] is the code for your project (e.g., x01); [group code] is the code for your project group, if your project has groups, (e.g. x01-a) or the same as the project code, if not; [username] is your login name. Links from the /home file system to directories or files on /work are strongly discouraged. If links are used, executables and data files on /work to be used by applications on the compute nodes (i.e. those executed via the aprun command) should be referenced directly on /work . Common I/O patterns There is a number of I/O patterns that are frequently used in applications: Single file, single writer (Serial I/O) A common approach is to funnel all the I/O through a single master process. Although this has the advantage of producing a single file, the fact that only a single client is doing all the I/O means that it gains little benefit from the parallel file system. File-per-process (FPP) One of the first parallel strategies people use for I/O is for each parallel process to write to its own file. This is a simple scheme to implement and understand but has the disadvantage that, at the end of the calculation, the data is spread across many different files and may therefore be difficult to use for further analysis without a data reconstruction stage. Single file, multiple writers without collective operations There are a number of ways to achieve this. For example, many processes can open the same file but access different parts by skipping some initial offset; parallel I/O libraries such as MPI-IO, HDF5 and NetCDF also enable this. Shared-file I/O has the advantage that all the data is organised correctly in a single file making analysis or restart more straightforward. The problem is that, with many clients all accessing the same file, there can be a lot of contention for file system resources. Single Shared File with collective writes (SSF) The problem with having many clients performing I/O at the same time is that, to prevent them clashing with each other, the I/O library may have to take a conservative approach. For example, a file may be locked while each client is accessing it which means that I/O is effectively serialised and performance may be poor. However, if I/O is done collectively where the library knows that all clients are doing I/O at the same time, then reads and writes can be explicitly coordinated to avoid clashes. It is only through collective I/O that the full bandwidth of the file system can be realised while accessing a single file. Achieving efficient I/O This section provides information on getting the best performance out of the parallel /work file systems on ARCHER2 when writing data, particularly using parallel I/O patterns. Lustre The ARCHER2 /work file systems use Lustre as a parallel file system technology. The Lustre file system provides POSIX semantics (changes on one node are immediately visible on other nodes) and can support very high data rates for appropriate I/O patterns. Striping One of the main factors leading to the high performance of Lustre file systems is the ability to stripe data across multiple Object Storage Targets (OSTs) in a round-robin fashion. Files are striped when the data is split up in chunks that will then be stored on different OSTs across the Lustre system. Striping might improve the I/O performance because it increases the available bandwith since multiple processes can read and write the same files simultaneously. However striping can also increase the overhead. Choosing the right striping configuration is key to obtain high performance results. Users have control of a number of striping settings on Lustre file systems. Although these parameters can be set on a per-file basis they are usually set on directory where your output files will be written so that all output files inherit the settings. Default configuration The /work file systems on ARCHER2 have the same default stripe settings: A default stripe count of 1 A default stripe size of 1 MiB (1048576 bytes) These settings have been chosen to provide a good compromise for the wide variety of I/O patterns that are seen on the system but are unlikely to be optimal for any one particular scenario. The Lustre command to query the stripe settings for a directory (or file) is lfs getstripe . For example, to query the stripe settings of an already created directory res_dir : [auser@archer2]$ lfs getstripe res_dir/ res_dir stripe_count: 1 stripe_size: 1048576 stripe_offset: -1 Setting Custom Striping Configurations Users can set stripe settings for a directory (or file) using the lfs setstripe command. The options for lfs setstripe are: [--stripe-count|-c] to set the stripe count; 0 means use the system default (usually 1) and -1 means stripe over all available OSTs. [--stripe-size|-s] to set the stripe size; 0 means use the system default (usually 1 MB) otherwise use k, m or g for KB, MB or GB respectively [--stripe-index|-i] to set the OST index (starting at 0) on which to start striping for this file. An index of -1 allows the MDS to choose the starting index and it is strongly recommended, as this allows space and load balancing to be done by the MDS as needed. For example, to set a stripe size of 4 MiB for the existing directory res_dir , along with maximum striping count you would use: [auser@archer2]$ lfs setstripe -s 4m -c -1 res_dir/ Recommended ARCHER2 I/O settings With the default settings, parallel I/O on multiple nodes can currently give poor perfomance. We recommend always setting the following environment variable in your SLURM batch script: export FI_OFI_RXM_SAR_LIMIT=64K Although I/O requirements vary significantly between different applications, the following settings should be good in most cases: If each process is writing to its own individual file then the default settings should give good performance. If processes are writing to a single shared file (e.g. using MPI-IO, HDF5 or NetCDF), set the appropriate directories to be fully striped: lfs setstripe -c -1 directory/ I/O Profiling Note We will add advice on I/O profiling soon.","title":"I/O and file systems"},{"location":"tursa-user-guide/io/#io-and-file-systems","text":"","title":"I/O and file systems"},{"location":"tursa-user-guide/io/#using-the-archer2-file-systems","text":"Different file systems are configured for different purposes and performance. ARCHER2 has three file systems available to users: Node type Available file systems Login /home, /work Compute /work Warning Any data used in a parallel jobs should be located on /work (Lustre).","title":"Using the ARCHER2 file systems"},{"location":"tursa-user-guide/io/#home-file-systems","text":"Home directories provide a convenient means for a user to have access to files such as source files, input files or configuration files. This file system is only mounted on the login nodes. The home directory for each user is located at: /home/[project code]/[group code]/[username] where [project code] is the code for your project (e.g., x01); [group code] is the code for your project group, if your project has groups, (e.g. x01-a) or the same as the project code, if not; [username] is your login name. Each project is allocated a portion of the total storage available, and the project PI will be able to sub-divide this quota among the groups and users within the project. As is standard practice on UNIX and Linux systems, the environment variable $HOME is automatically set to point to your home directory. It should be noted that the home file system is not designed, and does not have the capacity, to act as a long term archive for large sets of results.","title":"Home file systems"},{"location":"tursa-user-guide/io/#work-file-system","text":"Warning There is no backup of data on any of the work file systems, which means that in the event of a major hardware failure, or if a user accidently deletes essential data, it will not be possible to recover the lost files. A high-performance Lustre file system is mounted on the compute nodes. All parallel calculations must be run from directories on the /work file system and all files required by the calculation (apart from the executable) must reside on /work . Each project will be assigned space on a particular Lustre partition with the assignments chosen to balance the load across the available infrastructure. The work directory for each user is located at: /work/[project code]/[group code]/[username] where [project code] is the code for your project (e.g., x01); [group code] is the code for your project group, if your project has groups, (e.g. x01-a) or the same as the project code, if not; [username] is your login name. Links from the /home file system to directories or files on /work are strongly discouraged. If links are used, executables and data files on /work to be used by applications on the compute nodes (i.e. those executed via the aprun command) should be referenced directly on /work .","title":"Work file system"},{"location":"tursa-user-guide/io/#common-io-patterns","text":"There is a number of I/O patterns that are frequently used in applications:","title":"Common I/O patterns"},{"location":"tursa-user-guide/io/#single-file-single-writer-serial-io","text":"A common approach is to funnel all the I/O through a single master process. Although this has the advantage of producing a single file, the fact that only a single client is doing all the I/O means that it gains little benefit from the parallel file system.","title":"Single file, single writer (Serial I/O)"},{"location":"tursa-user-guide/io/#file-per-process-fpp","text":"One of the first parallel strategies people use for I/O is for each parallel process to write to its own file. This is a simple scheme to implement and understand but has the disadvantage that, at the end of the calculation, the data is spread across many different files and may therefore be difficult to use for further analysis without a data reconstruction stage.","title":"File-per-process (FPP)"},{"location":"tursa-user-guide/io/#single-file-multiple-writers-without-collective-operations","text":"There are a number of ways to achieve this. For example, many processes can open the same file but access different parts by skipping some initial offset; parallel I/O libraries such as MPI-IO, HDF5 and NetCDF also enable this. Shared-file I/O has the advantage that all the data is organised correctly in a single file making analysis or restart more straightforward. The problem is that, with many clients all accessing the same file, there can be a lot of contention for file system resources.","title":"Single file, multiple writers without collective operations"},{"location":"tursa-user-guide/io/#single-shared-file-with-collective-writes-ssf","text":"The problem with having many clients performing I/O at the same time is that, to prevent them clashing with each other, the I/O library may have to take a conservative approach. For example, a file may be locked while each client is accessing it which means that I/O is effectively serialised and performance may be poor. However, if I/O is done collectively where the library knows that all clients are doing I/O at the same time, then reads and writes can be explicitly coordinated to avoid clashes. It is only through collective I/O that the full bandwidth of the file system can be realised while accessing a single file.","title":"Single Shared File with collective writes (SSF)"},{"location":"tursa-user-guide/io/#achieving-efficient-io","text":"This section provides information on getting the best performance out of the parallel /work file systems on ARCHER2 when writing data, particularly using parallel I/O patterns.","title":"Achieving efficient I/O"},{"location":"tursa-user-guide/io/#lustre","text":"The ARCHER2 /work file systems use Lustre as a parallel file system technology. The Lustre file system provides POSIX semantics (changes on one node are immediately visible on other nodes) and can support very high data rates for appropriate I/O patterns.","title":"Lustre"},{"location":"tursa-user-guide/io/#striping","text":"One of the main factors leading to the high performance of Lustre file systems is the ability to stripe data across multiple Object Storage Targets (OSTs) in a round-robin fashion. Files are striped when the data is split up in chunks that will then be stored on different OSTs across the Lustre system. Striping might improve the I/O performance because it increases the available bandwith since multiple processes can read and write the same files simultaneously. However striping can also increase the overhead. Choosing the right striping configuration is key to obtain high performance results. Users have control of a number of striping settings on Lustre file systems. Although these parameters can be set on a per-file basis they are usually set on directory where your output files will be written so that all output files inherit the settings.","title":"Striping"},{"location":"tursa-user-guide/io/#default-configuration","text":"The /work file systems on ARCHER2 have the same default stripe settings: A default stripe count of 1 A default stripe size of 1 MiB (1048576 bytes) These settings have been chosen to provide a good compromise for the wide variety of I/O patterns that are seen on the system but are unlikely to be optimal for any one particular scenario. The Lustre command to query the stripe settings for a directory (or file) is lfs getstripe . For example, to query the stripe settings of an already created directory res_dir : [auser@archer2]$ lfs getstripe res_dir/ res_dir stripe_count: 1 stripe_size: 1048576 stripe_offset: -1","title":"Default configuration"},{"location":"tursa-user-guide/io/#setting-custom-striping-configurations","text":"Users can set stripe settings for a directory (or file) using the lfs setstripe command. The options for lfs setstripe are: [--stripe-count|-c] to set the stripe count; 0 means use the system default (usually 1) and -1 means stripe over all available OSTs. [--stripe-size|-s] to set the stripe size; 0 means use the system default (usually 1 MB) otherwise use k, m or g for KB, MB or GB respectively [--stripe-index|-i] to set the OST index (starting at 0) on which to start striping for this file. An index of -1 allows the MDS to choose the starting index and it is strongly recommended, as this allows space and load balancing to be done by the MDS as needed. For example, to set a stripe size of 4 MiB for the existing directory res_dir , along with maximum striping count you would use: [auser@archer2]$ lfs setstripe -s 4m -c -1 res_dir/","title":"Setting Custom Striping Configurations"},{"location":"tursa-user-guide/io/#recommended-archer2-io-settings","text":"With the default settings, parallel I/O on multiple nodes can currently give poor perfomance. We recommend always setting the following environment variable in your SLURM batch script: export FI_OFI_RXM_SAR_LIMIT=64K Although I/O requirements vary significantly between different applications, the following settings should be good in most cases: If each process is writing to its own individual file then the default settings should give good performance. If processes are writing to a single shared file (e.g. using MPI-IO, HDF5 or NetCDF), set the appropriate directories to be fully striped: lfs setstripe -c -1 directory/","title":"Recommended ARCHER2 I/O settings"},{"location":"tursa-user-guide/io/#io-profiling","text":"Note We will add advice on I/O profiling soon.","title":"I/O Profiling"},{"location":"tursa-user-guide/scheduler/","text":"Running jobs on Tursa As with most HPC services, Tursa uses a scheduler to manage access to resources and ensure that the thousands of different users of system are able to share the system and all get access to the resources they require. Tursa uses the Slurm software to schedule jobs. Writing a submission script is typically the most convenient way to submit your job to the scheduler. Example submission scripts (with explanations) for the most common job types are provided below. Interactive jobs are also available and can be particularly useful for developing and debugging applications. More details are available below. Hint If you have any questions on how to run jobs on Tursa do not hesitate to contact the DiRAC Service Desk . You typically interact with Slurm by issuing Slurm commands from the login nodes (to submit, check and cancel jobs), and by specifying Slurm directives that describe the resources required for your jobs in job submission scripts. Resources coreh Time used on Tursa CPU nodes is measured in coreh. 1 coreh = 1 physical core for 1 hour. So a Tursa compute node with 2, 64 core CPUs would cost 128 coreh per hour. GPUh Time used on Tursa GPU nodes is measured in GPUh. 1 GPUh = 1 GPU for 1 hour. So a Tursa compute node with 4 GPUs would cost 4 GPUh per hour. Checking available budget You can check in SAFE by selecting Login accounts from the menu, select the login account you want to query. Under Login account details you will see each of the budget codes you have access to listed e.g. e123 resources and then under Resource Pool to the right of this, a note of the remaining budgets. When logged in to the machine you can also use the command sacctmgr show assoc where user=$LOGNAME format=account,user,maxtresmins This will list all the budget codes that you have access to e.g. Account User MaxTRESMins ---------- ---------- ------------- e123 userx cpu=0 e123-test userx This shows that userx is a member of budgets e123 and e123-test . However, the cpu=0 indicates that the e123 budget is empty or disabled. This user can submit jobs using the e123-test budget. To see the number of CUs remaining you must check in SAFE . Charging Jobs run on Tursa are charged for the time they use i.e. from the time the job begins to run until the time the job ends (not the full wall time requested). Jobs are charged for the full number of nodes which are requested, even if they are not all used. Charging takes place at the time the job ends, and the job is charged in full to the budget which is live at the end time. Basic Slurm commands There are three key commands used to interact with the Slurm on the command line: sinfo - Get information on the partitions and resources available sbatch jobscript.slurm - Submit a job submission script (in this case called: jobscript.slurm ) to the scheduler squeue - Get the current status of jobs submitted to the scheduler scancel 12345 - Cancel a job (in this case with the job ID 12345 ) We cover each of these commands in more detail below. sinfo : information on resources sinfo is used to query information about available resources and partitions. Without any options, sinfo lists the status of all resources and partitions, e.g. sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST standard up 1-00:00:00 105 down* nid[001006,...,002014] standard up 1-00:00:00 12 drain nid[001016,...,001969] standard up 1-00:00:00 5 resv nid[001000,001002-001004,001114] standard up 1-00:00:00 683 alloc nid[001001,...,001970-001991] standard up 1-00:00:00 214 idle nid[001022-001023,...,002015-002023] standard up 1-00:00:00 2 down nid[001021,001050] Here we see the number of nodes in different states. For example, 683 nodes are allocated (running jobs), and 214 are idle (available to run jobs). !!! note that long lists of node IDs have been abbreviated with ... . sbatch : submitting jobs sbatch is used to submit a job script to the job submission system. The script will typically contain one or more srun commands to launch parallel tasks. When you submit the job, the scheduler provides the job ID, which is used to identify this job in other Slurm commands and when looking at resource usage in SAFE. sbatch test-job.slurm Submitted batch job 12345 squeue : monitoring jobs squeue without any options or arguments shows the current status of all jobs known to the scheduler. For example: squeue will list all jobs on Tursa. The output of this is often overwhelmingly large. You can restrict the output to just your jobs by adding the -u $USER option: squeue -u $USER scancel : deleting jobs scancel is used to delete a jobs from the scheduler. If the job is waiting to run it is simply cancelled, if it is a running job then it is stopped immediately. You need to provide the job ID of the job you wish to cancel/stop. For example: scancel 12345 will cancel (if waiting) or stop (if running) the job with ID 12345 . Resource Limits The Tursa resource limits for any given job are covered by three separate attributes. The amount of primary resource you require, i.e., number of compute nodes. The partition that you want to use - this specifies the nodes that are eligible to run your job. The Quality of Service (QoS) that you want to use - this specifies the job limits that apply. Primary resource The primary resource you can request for your job is the compute node. Information The --exclusive option is enforced on Tursa which means you will always have access to all of the memory on the compute node regardless of how many processes are actually running on the node. Note You will not generally have access to the full amount of memory resource on the the node as some is retained for running the operating system and other system processes. Partitions On Tursa, compute nodes are grouped into partitions. You will have to specify a partition using the --partition option in your Slurm submission script. The following table has a list of active partitions on Tursa. Partition Description Max nodes available standard CPU nodes with AMD EPYC 7742 64-core processor \u00d7 2 1024 Tursa Partitions You can list the active partitions by running sinfo . Tip You may not have access to all the available partitions. Quality of Service (QoS) On Tursa, job limits are defined by the requested Quality of Service (QoS), as specified by the --qos Slurm directive. The following table lists the active QoS on Tursa. QoS Max Nodes Per Job Max Walltime Jobs Queued Jobs Running Partition(s) Notes standard 256 24 hrs 64 16 standard Maximum of 256 nodes in use by any one user at any time short 8 20 mins 16 4 standard long 64 48 hrs 16 16 standard Minimum walltime of 24 hrs largescale 940 3 hrs 4 1 standard Minimum job size of 257 nodes lowpriority 256 3 hrs 4 1 standard Maximum of 256 nodes in use by any one user at any time. Jobs not charged but requires at least 1 CU in budget to use. Warning If you want to use the short QoS then you also need to add the --reservation=shortqos to your job submission command. You can find out the QoS that you can use by running the following command: sacctmgr show assoc user=$USER cluster=tursa-es format=cluster,account,user,qos%50 Hint If you have needs which do not fit within the current QoS, please contact the Service Desk and we can discuss how to accommodate your requirements. E-mail notifications E-mail notifications from the batch system are not currently available on Tursa. Troubleshooting Slurm error messages An incorrect submission will cause Slurm to return an error. Some common problems are listed below, with a suggestion about the likely cause: sbatch: unrecognized option <text> One of your options is invalid or has a typo. man sbatch to help. error: Batch job submission failed: No partition specified or system default partition A --partition= option is missing. You must specify the partition (see the list above). This is most often --partition=standard . error: invalid partition specified: <partition> error: Batch job submission failed: Invalid partition name specified Check the partition exists and check the spelling is correct. error: Batch job submission failed: Invalid account or account/partition combination specified This probably means an invalid account has been given. Check the --account= options against valid accounts in SAFE. error: Batch job submission failed: Invalid qos specification A QoS option is either missing or invalid. Check the script has a --qos= option and that the option is a valid one from the table above. (Check the spelling of the QoS is correct.) error: Your job has no time specification (--time=)... Add an option of the form --time=hours:minutes:seconds to the submission script. E.g., --time=01:30:00 gives a time limit of 90 minutes. error: QOSMaxWallDurationPerJobLimit error: Batch job submission failed: Job violates accounting/QOS policy (job submit limit, user's size and/or time limits) The script has probably specified a time limit which is too long for the corresponding QoS. E.g., the time limit for the short QoS is 20 minutes. Slurm queued reasons The squeue command allows users to view information for jobs managed by Slurm. Jobs typically go through the following states: PENDING, RUNNING, COMPLETING, and COMPLETED. The first table provides a description of some job state codes. The second table provides a description of the reasons that cause a job to be in a state. Status Code Description PENDING PD Job is awaiting resource allocation. RUNNING R Job currently has an allocation. SUSPENDED S Job currently has an allocation. COMPLETING CG Job is in the process of completing. Some processes on some nodes may still be active. COMPLETED CD Job has terminated all processes on all nodes with an exit code of zero. TIMEOUT TO Job terminated upon reaching its time limit. STOPPED ST Job has an allocation, but execution has been stopped with SIGSTOP signal. CPUS have been retained by this job. OUT_OF_MEMORY OOM Job experienced out of memory error. FAILED F Job terminated with non-zero exit code or other failure condition. NODE_FAIL NF Job terminated due to failure of one or more allocated nodes. CANCELLED CA Job was explicitly cancelled by the user or system administrator. The job may or may not have been initiated. For a full list of see Job State Codes . Reason Description Priority One or more higher priority jobs exist for this partition or advanced reservation. Resources The job is waiting for resources to become available. BadConstraints The job's constraints can not be satisfied. BeginTime The job's earliest start time has not yet been reached. Dependency This job is waiting for a dependent job to complete. Licenses The job is waiting for a license. WaitingForScheduling No reason has been set for this job yet. Waiting for the scheduler to determine the appropriate reason. Prolog Its PrologSlurmctld program is still running. JobHeldAdmin The job is held by a system administrator. JobHeldUser The job is held by the user. JobLaunchFailure The job could not be launched. This may be due to a file system problem, invalid program name, etc. NonZeroExitCode The job terminated with a non-zero exit code. InvalidAccount The job's account is invalid. InvalidQOS The job's QOS is invalid. QOSUsageThreshold Required QOS threshold has been breached. QOSJobLimit The job's QOS has reached its maximum job count. QOSResourceLimit The job's QOS has reached some resource limit. QOSTimeLimit The job's QOS has reached its time limit. NodeDown A node required by the job is down. TimeLimit The job exhausted its time limit. ReqNodeNotAvail Some node specifically required by the job is not currently available. The node may currently be in use, reserved for another job, in an advanced reservation, DOWN, DRAINED, or not responding. Nodes which are DOWN, DRAINED, or not responding will be identified as part of the job's \"reason\" field as \"UnavailableNodes\". Such nodes will typically require the intervention of a system administrator to make available. For a full list of see Job Reasons . Output from Slurm jobs Slurm places standard output (STDOUT) and standard error (STDERR) for each job in the file slurm_<JobID>.out . This file appears in the job's working directory once your job starts running. Hint Output may be buffered - to enable live output, e.g. for monitoring job status, add --unbuffered to the srun command in your SLURM script. Specifying resources in job scripts You specify the resources you require for your job using directives at the top of your job submission script using lines that start with the directive #SBATCH . Hint Most options provided using #SBATCH directives can also be specified as command line options to srun . If you do not specify any options, then the default for each option will be applied. As a minimum, all job submissions must specify the budget that they wish to charge the job too with the option: --account=<budgetID> your budget ID is usually something like t01 or t01-test . You can see which budget codes you can charge to in SAFE. Other common options that are used are: --time=<hh:mm:ss> the maximum walltime for your job. e.g. For a 6.5 hour walltime, you would use --time=6:30:0 . --job-name=<jobname> set a name for the job to help identify it in In addition, parallel jobs will also need to specify how many nodes, parallel processes and threads they require. --nodes=<nodes> the number of nodes to use for the job. --tasks-per-node=<processes per node> the number of parallel processes (e.g. MPI ranks) per node. --cpus-per-task=1 if you are using parallel processes only with no threading then you should set the number of CPUs (cores) per parallel process to 1. !!! note: if you are using threading (e.g. with OpenMP) then you will need to change this option as described below. For parallel jobs that use threading (e.g. OpenMP), you will also need to change the --cpus-per-task option. --cpus-per-task=<threads per task> the number of threads per parallel process (e.g. number of OpenMP threads per MPI task for hybrid MPI/OpenMP jobs). !!! note: you must also set the OMP_NUM_THREADS environment variable if using OpenMP in your job. Note For parallel jobs, Tursa operates in a node exclusive way. This means that you are assigned resources in the units of full compute nodes for your jobs ( i.e. 128 cores) and that no other user can share those compute nodes with you. Hence, the minimum amount of resource you can request for a parallel job is 1 node (or 128 cores). To prevent the behaviour of batch scripts being dependent on the user environment at the point of submission, the option --export=none prevents the user environment from being exported to the batch system. Using the --export=none means that the behaviour of batch submissions should be repeatable. We strongly recommend its use, although see the following section to enable access to the usual modules. srun : Launching parallel jobs If you are running parallel jobs, your job submission script should contain one or more srun commands to launch the parallel executable across the compute nodes. In most cases you will want to add the options --distribution=block:block and --hint=nomultithread to your srun command to ensure you get the correct pinning of processes to cores on a compute node. Warning If you do not add the --distribution=block:block and --hint=nomultithread options to your srun command the default process placement may lead to a drop in performance for your jobs on Tursa. A brief explanation of these options: - --hint=nomultithread - do not use hyperthreads/SMP - --distribution=block:block - the first block means use a block distribution of processes across nodes (i.e. fill nodes before moving onto the next one) and the second block means use a block distribution of processes across NUMA regions within a node (i.e. fill a NUMA region before moving on to the next one). Example job submission scripts A subset of example job submission scripts are included in full below. Example: job submission script for MPI parallel job A simple MPI job submission script to submit a job using 4 compute nodes and 128 MPI ranks per node for 20 minutes would look like: #!/bin/bash # Slurm job options (job-name, compute nodes, job time) #SBATCH --job-name=Example_MPI_Job #SBATCH --time=0:20:0 #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 # Replace [budget code] below with your budget code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the job environment (this module needs to be loaded before any other modules) module load epcc-job-env # Set the number of threads to 1 # This prevents any threaded system libraries from automatically # using threading. export OMP_NUM_THREADS=1 # Launch the parallel job # Using 512 MPI processes and 128 MPI processes per node # srun picks up the distribution from the sbatch options srun --distribution=block:block --hint=nomultithread ./my_mpi_executable.x This will run your executable \"my_mpi_executable.x\" in parallel on 512 MPI processes using 4 nodes (128 cores per node, i.e. not using hyper-threading). Slurm will allocate 4 nodes to your job and srun will place 128 MPI processes on each node (one per physical core). See above for a more detailed discussion of the different sbatch options Example: job submission script for MPI+OpenMP (mixed mode) parallel job Mixed mode codes that use both MPI (or another distributed memory parallel model) and OpenMP should take care to ensure that the shared memory portion of the process/thread placement does not span more than one NUMA region. Nodes on Tursa are made up of two sockets each containing 4 NUMA regions of 16 cores, i.e. there are 8 NUMA regions in total. Therefore the total number of threads should ideally not be greater than 16, and also needs to be a factor of 16. Sensible choices for the number of threads are therefore 1 (single-threaded), 2, 4, 8, and 16. More information about using OpenMP and MPI+OpenMP can be found in the Tuning chapter. To ensure correct placement of MPI processes the number of cpus-per-task needs to match the number of OpenMP threads, and the number of tasks-per-node should be set to ensure the entire node is filled with MPI tasks. In the example below, we are using 4 nodes for 6 hours. There are 32 MPI processes in total (8 MPI processes per node) and 16 OpenMP threads per MPI process. This results in all 128 physical cores per node being used. Hint Note the use of the export OMP_PLACES=cores environment option to generate the correct thread pinning. #!/bin/bash # Slurm job options (job-name, compute nodes, job time) #SBATCH --job-name=Example_MPI_Job #SBATCH --time=0:20:0 #SBATCH --nodes=4 #SBATCH --tasks-per-node=8 #SBATCH --cpus-per-task=16 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the job environment (this module needs to be loaded before any other modules) module load epcc-job-env # Set the number of threads to 16 and specify placement # There are 16 OpenMP threads per MPI process # We want one thread per physical core export OMP_NUM_THREADS=16 export OMP_PLACES=cores # Launch the parallel job # Using 32 MPI processes # 8 MPI processes per node # 16 OpenMP threads per MPI process # Additional srun options to pin one thread per physical core srun --hint=nomultithread --distribution=block:block ./my_mixed_executable.x arg1 arg2 Job arrays The Slurm job scheduling system offers the job array concept, for running collections of almost-identical jobs. For example, running the same program several times with different arguments or input data. Each job in a job array is called a subjob . The subjobs of a job array can be submitted and queried as a unit, making it easier and cleaner to handle the full set, compared to individual jobs. All subjobs in a job array are started by running the same job script. The job script also contains information on the number of jobs to be started, and Slurm provides a subjob index which can be passed to the individual subjobs or used to select the input data per subjob. Job script for a job array As an example, the following script runs 56 subjobs, with the subjob index as the only argument to the executable. Each subjob requests a single node and uses all 128 cores on the node by placing 1 MPI process per core and specifies 4 hours maximum runtime per subjob: #!/bin/bash # Slurm job options (job-name, compute nodes, job time) #SBATCH --job-name=Example_Array_Job #SBATCH --time=04:00:00 #SBATCH --nodes=1 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --array=0-55 # Replace [budget code] below with your budget code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the job environment (this module needs to be loaded before any other modules) module load epcc-job-env # Set the number of threads to 1 # This prevents any threaded system libraries from automatically # using threading. export OMP_NUM_THREADS=1 srun --distribution=block:block --hint=nomultithread /path/to/exe $SLURM_ARRAY_TASK_ID Submitting a job array Job arrays are submitted using sbatch in the same way as for standard jobs: sbatch job_script.pbs Job chaining Job dependencies can be used to construct complex pipelines or chain together long simulations requiring multiple steps. Hint The --parsable option to sbatch can simplify working with job dependencies. It returns the job ID in a format that can be used as the input to other commands. For example: jobid=$(sbatch --parsable first_job.sh) sbatch --dependency=afterok:$jobid second_job.sh or for a longer chain: jobid1=$(sbatch --parsable first_job.sh) jobid2=$(sbatch --parsable --dependency=afterok:$jobid1 second_job.sh) jobid3=$(sbatch --parsable --dependency=afterok:$jobid1 third_job.sh) sbatch --dependency=afterok:$jobid2,afterok:$jobid3 last_job.sh Using multiple srun commands in a single job script You can use multiple srun commands within in a Slurm job submission script to allow you to use the resource requested more flexibly. For example, you could run a collection of smaller jobs within the requested resources or you could even subdivide nodes if your individual calculations do not scale up to use all 128 cores on a node. In this guide we will cover two scenarios: Subdividing the job into multiple full-node or multi-node subjobs, e.g. requesting 100 nodes and running 100, 1-node subjobs or 50, 2-node subjobs. Subdividing the job into multiple subjobs that each use a fraction of a node, e.g. requesting 2 nodes and running 256, 1-core subjobs or 16, 16-core subjobs. Running multiple, full-node subjobs within a larger job When subdivding a larger job into smaller subjobs you typically need to overwrite the --nodes option to srun and add the --ntasks option to ensure that each subjob runs on the correct number of nodes and that subjobs are placed correctly onto separate nodes. For example, we will show how to request 100 nodes and then run 100 separate 1-node jobs, each of which use 128 MPI processes and which run on a different compute node. We start by showing the job script that would achieve this and then explain how this works and the options used. In our case, we will run 100 copies of the xthi program that prints the process placement on the node it is running on. #!/bin/bash # Slurm job options (job-name, compute nodes, job time) #SBATCH --job-name=multi_xthi #SBATCH --time=0:20:0 #SBATCH --nodes=100 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 # Replace [budget code] below with your budget code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the job environment (this module needs to be loaded before any other modules) module load epcc-job-env # Load the xthi module module load xthi # Set the number of threads to 1 # This prevents any threaded system libraries from automatically # using threading. export OMP_NUM_THREADS = 1 # Loop over 100 subjobs starting each of them on a separate node for i in $( seq 1 100 ) do # Launch this subjob on 1 node, note nodes and ntasks options and & to place subjob in the background srun --nodes = 1 --ntasks = 128 --distribution = block:block --hint = nomultithread xthi > placement ${ i } .txt & done # Wait for all background subjobs to finish wait Key points from the example job script: The #SBATCH options select 100 full nodes in the usual way. Each subjob srun command sets the following: --nodes=1 We need override this setting from the main job so that each subjob only uses 1 node --ntasks=128 For normal jobs, the number of parallel tasks (MPI processes) is calculated from the number of nodes you request and the number of tasks per node. We need to explicitly tell srun how many we require for this subjob. --distribution=block:block --hint=nomultithread These options ensure correct placement of processes within the compute nodes. & Each subjob srun command ends with an ampersand to place the process in the background and move on to the next loop iteration (and subjob submission). Without this, the script would wait for this subjob to complete before moving on to submit the next. Finally, there is the wait command to tell the script to wait for all the background subjobs to complete before exiting. If we did not have this in place, the script would exit as soon as the last subjob was submitted and kill all running subjobs. Interactive Jobs Using salloc to reserve resources When you are developing or debugging code you often want to run many short jobs with a small amount of editing the code between runs. This can be achieved by using the login nodes to run MPI but you may want to test on the compute nodes (e.g. you may want to test running on multiple nodes across the high performance interconnect). One of the best ways to achieve this on Tursa is to use interactive jobs. An interactive job allows you to issue srun commands directly from the command line without using a job submission script, and to see the output from your program directly in the terminal. You use the salloc command to reserve compute nodes for interactive jobs. To submit a request for an interactive job reserving 8 nodes (1024 physical cores) for 20 minutes on the short queue you would issue the following command from the command line: auser@login01:> salloc --nodes=8 --tasks-per-node=128 --cpus-per-task=1 \\ --time=00:20:00 --partition=standard --qos=short \\ --reservation=shortqos --account=[budget code] When you submit this job your terminal will display something like: salloc: Granted job allocation 24236 salloc: Waiting for resource configuration salloc: Nodes nid000002 are ready for job auser@login01:> It may take some time for your interactive job to start. Once it runs you will enter a standard interactive terminal session (a new shell). Note that this shell is still on the front end (the prompt has not change). Whilst the interactive session lasts you will be able to run parallel jobs on the compute nodes by issuing the srun --distribution=block:block --hint=nomultithread command directly at your command prompt using the same syntax as you would inside a job script. The maximum number of nodes you can use is limited by resources requested in the salloc command. If you know you will be doing a lot of intensive debugging you may find it useful to request an interactive session lasting the expected length of your working session, say a full day. Your session will end when you hit the requested walltime. If you wish to finish before this you should use the exit command - this will return you to your prompt before you issued the salloc command. Using srun directly A second way to run an interactive job is to use srun directly in the following way (here using the \"short queue\"): auser@login01:/work/t01/t01/auser> srun --nodes=1 --exclusive --time=00:20:00 \\ --partition=standard --qos=short --reservation=shortqos \\ --pty /bin/bash auser@login01:/work/t01/t01/auser> hostname nid001261 The --pty /bin/bash will cause a new shell to be started on the first node of a new allocation (note that while the shell prompt has not changed, we are now on the compute node). This is perhaps closer to what many people consider an 'interactive' job than the method using salloc appears. One can now issue shell commands in the usual way. A further invocation of srun is required to launch a parallel job in the allocation. When finished, type exit to relinquish the allocation and control will be returned to the front end. By default, the interactive shell will retain the environment of the parent. If you want a clean shell, remember to specify --export=none . If you need to use modules within your job , you will need to start a login shell by passing the --login argument to bash . Heterogeneous jobs The SLURM submissions discussed above involve a single executable image. However, there are situtions where two or more distinct executables are coupled and need to be run at the same time. This is most easily handled via the SLURM heterogeneous job mechanism. The essential feature of a heterogeneous job is to create a single batch submission which specifies the resource requirements for the individual components. Schematically, we would use #!/bin/bash # SLURM specifications for the first component #SBATCH --partition=standard ... #SBATCH hetjob # SLURM specifications for the second component #SBATCH --partition=standard ... where new each component beyond the first is introduced by the special token #SBATCH hetjob (note this is not a normal option and is not --hetjob ). Each component must specify a partition. Such a job will appear in the queue system as, e.g., 50098+0 standard qscript- user PD 0:00 1 (None) 50098+1 standard qscript- user PD 0:00 2 (None) and counts as (in this case) two separate jobs from the point of QoS limits. Two common cases are discussed below: first, a client server model in which client and server each have a different MPI_COMM_WORLD , and second the case were two or more executables share MPI_COMM_WORLD . Heterogeneous jobs for a client/server model Consider a case where we have two executables which may both be parallel (in that they use MPI), both run at the same time, and communicate with each other by some means other than MPI. In the following example, we run two different executables, both of which must finish before the jobs completes. #!/bin/bash #SBATCH --time=00:20:00 #SBATCH --exclusive #SBATCH --export=none #SBATCH --partition=standard #SBATCH --qos=standard #SBATCH --nodes=1 #SBATCH --ntasks-per-node=8 #SBATCH hetjob #SBATCH --partition=standard #SBATCH --qos=standard #SBATCH --nodes=2 #SBATCH --ntasks-per-node=4 # Run two execuatables with separate MPI_COMM_WORLD srun --distribution=block:block --hint=nomultithread --het-group=0 ./xthi-a & srun --distribution=block:block --hint=nomultithread --het-group=1 ./xthi-b & wait In this case, each executable is launched with a separate call to srun but specifies a different heterogeneous group via the --het-group option. The first group is --het-group=0 . Both are run in the background with & and the wait is required to ensure both executables have completed before the job submission exits. In this rather artificial example, where each component makes a simple report about its placement, the output might be Node 0, hostname nid001028, mpi 4, omp 1, executable xthi-b Node 1, hostname nid001048, mpi 4, omp 1, executable xthi-b Node 0, rank 0, thread 0, (affinity = 0) Node 0, rank 1, thread 0, (affinity = 1) Node 0, rank 2, thread 0, (affinity = 2) Node 0, rank 3, thread 0, (affinity = 3) Node 1, rank 4, thread 0, (affinity = 0) Node 1, rank 5, thread 0, (affinity = 1) Node 1, rank 6, thread 0, (affinity = 2) Node 1, rank 7, thread 0, (affinity = 3) Node 0, hostname nid001027, mpi 8, omp 1, executable xthi-a Node 0, rank 0, thread 0, (affinity = 0) Node 0, rank 1, thread 0, (affinity = 1) Node 0, rank 2, thread 0, (affinity = 2) Node 0, rank 3, thread 0, (affinity = 3) Node 0, rank 4, thread 0, (affinity = 4) Node 0, rank 5, thread 0, (affinity = 5) Node 0, rank 6, thread 0, (affinity = 6) Node 0, rank 7, thread 0, (affinity = 7) Here we have the first executable running on one node with a communicator size 8 (ranks 0-7). The second executable runs on two nodes also with communicator size 8 (ranks 0-7, 4 ranks per node). Further examples of placement for heterogenenous jobs are given below. Heterogeneous jobs for a shared MPI_COM_WORLD If two or more heterogeneous components need to share a unique MPI_COMM_WORLD , a single srun invocation with the differrent components separated by a colon : should be used. For example, #!/bin/bash #SBATCH --time=00:20:00 #SBATCH --exclusive #SBATCH --export=none #SBATCH --partition=standard #SBATCH --qos=standard #SBATCH --nodes=1 #SBATCH --ntasks-per-node=8 #SBATCH hetjob #SBATCH --partition=standard #SBATCH --qos=standard #SBATCH --nodes=2 #SBATCH --ntasks-per-node=4 srun --distribution=block:block --hint=nomultithread --het-group=0 ./xthi-a : \\ --distribution=block:block --hint=nomultithread --het-group=1 ./xthi-b The output should confirm we have a single MPI_COMM_WORLD with ranks 0-15. Node 0, hostname nid001027, mpi 8, omp 1, executable xthi-a Node 1, hostname nid001028, mpi 4, omp 1, executable xthi-b Node 2, hostname nid001048, mpi 4, omp 1, executable xthi-b Node 0, rank 0, thread 0, (affinity = 0) Node 0, rank 1, thread 0, (affinity = 1) Node 0, rank 2, thread 0, (affinity = 2) Node 0, rank 3, thread 0, (affinity = 3) Node 0, rank 4, thread 0, (affinity = 4) Node 0, rank 5, thread 0, (affinity = 5) Node 0, rank 6, thread 0, (affinity = 6) Node 0, rank 7, thread 0, (affinity = 7) Node 1, rank 8, thread 0, (affinity = 0) Node 1, rank 9, thread 0, (affinity = 1) Node 1, rank 10, thread 0, (affinity = 2) Node 1, rank 11, thread 0, (affinity = 3) Node 2, rank 12, thread 0, (affinity = 0) Node 2, rank 13, thread 0, (affinity = 1) Node 2, rank 14, thread 0, (affinity = 2) Node 2, rank 15, thread 0, (affinity = 3) Heterogeneous placement for mixed MPI/OpenMP work Some care may be required for placement of tasks/threads in heterogeneous jobs in which the number of threads needs to be specified differently for different components. In the following we have two components. The first component runs 8 MPI tasks each with 16 OpenMP threads. The second component runs 8 MPI tasks with one task per NUMA region on one node; each task has one thread. An appropriate SLURM submission might be: #!/bin/bash #SBATCH --time=00:20:00 #SBATCH --exclusive #SBATCH --export=none # First component #SBATCH --partition=standard #SBATCH --qos=standard #SBATCH --nodes=1 #SBATCH --ntasks-per-node=8 #SBATCH --cpus-per-task=16 #SBATCH --hint=nomultithread # Second component #SBATCH hetjob #SBATCH --partition=standard #SBATCH --nodes=1 #SBATCH --ntasks-per-node=8 #SBATCH --cpus-per-task=16 # Do not set OMP_NUM_THREADS in the calling environment unset OMP_NUM_THREADS export OMP_PROC_BIND=spread srun --het-group=0 --export=all,OMP_NUM_THREADS=16 ./xthi-a : \\ --het-group=1 --export=all,OMP_NUM_THREADS=1 ./xthi-b The important point here is that OMP_NUM_THREADS must not be set in the environment that calls srun in order that the different specifications for the separate groups via --export on the srun command line take effect. If OMP_NUM_THREADS is set in the calling environment, then that value takes precedence, and each component will see the same value of OMP_NUM_THREADS . The output would be: Node 0, hostname nid001111, mpi 8, omp 16, executable xthi-a Node 1, hostname nid001126, mpi 8, omp 1, executable xthi-b Node 0, rank 0, thread 0, (affinity = 0) Node 0, rank 0, thread 1, (affinity = 1) Node 0, rank 0, thread 2, (affinity = 2) Node 0, rank 0, thread 3, (affinity = 3) Node 0, rank 0, thread 4, (affinity = 4) Node 0, rank 0, thread 5, (affinity = 5) Node 0, rank 0, thread 6, (affinity = 6) Node 0, rank 0, thread 7, (affinity = 7) Node 0, rank 0, thread 8, (affinity = 8) Node 0, rank 0, thread 9, (affinity = 9) Node 0, rank 0, thread 10, (affinity = 10) Node 0, rank 0, thread 11, (affinity = 11) Node 0, rank 0, thread 12, (affinity = 12) Node 0, rank 0, thread 13, (affinity = 13) Node 0, rank 0, thread 14, (affinity = 14) Node 0, rank 0, thread 15, (affinity = 15) Node 0, rank 1, thread 0, (affinity = 16) Node 0, rank 1, thread 1, (affinity = 17) ... Node 0, rank 7, thread 14, (affinity = 126) Node 0, rank 7, thread 15, (affinity = 127) Node 1, rank 8, thread 0, (affinity = 0) Node 1, rank 9, thread 0, (affinity = 16) Node 1, rank 10, thread 0, (affinity = 32) Node 1, rank 11, thread 0, (affinity = 48) Node 1, rank 12, thread 0, (affinity = 64) Node 1, rank 13, thread 0, (affinity = 80) Node 1, rank 14, thread 0, (affinity = 96) Node 1, rank 15, thread 0, (affinity = 112) Low priority access Low priority jobs are not charged against your allocation but will only run when other, higher-priority, jobs cannot be run or there are no higher-priority jobs in the queue. Although low priority jobs are not charged, you do need a valid, positive budget to be able to submit and run low priority jobs, i.e. you need at least 1 CU in your budget. Low priority access is always available and has the following limits: 256 node maximum job size 256 nodes maximum in use by any one user 512 nodes maximum in use by low priority at any one time Maximum 4 low priority jobs in the queue per user Maximum 1 low priority job running per user (of the 4 queued) Maximum runtime of 3 hours You submit a low priority job on Tursa by using the lowpriority QoS. For example, you would usually have the following line in your job submission script sbatch options: #SBATCH --qos=lowpriority Reservations Reservations are available on Tursa. These allow users to reserve a number of nodes for a specified length of time starting at a particular time on the system. Reservations require justification. They will only be approved if the request could not be fulfilled with the standard queues. For instance, you require a job/jobs to run at a particular time e.g. for a demonstration or course. Note Reservation requests must be submitted at least 60 hours in advance of the reservation start time. If requesting a reservation for a Monday at 18:00, please ensure this is received by the Friday at 12:00 the latest. The same applies over Service Holidays. Note Reservations are only valid for standard compute nodes, high memory compute nodes and/or PP nodes cannot be included in reservations. Reservations will be charged at 1.5 times the usual CU rate and our policy is that they will be charged the full rate for the entire reservation at the time of booking, whether or not you use the nodes for the full time. In addition, you will not be refunded the CUs if you fail to use them due to a job crash unless this crash is due to a system failure. Bug At the moment, we are only able to charge for jobs in reservations, not for the full reservation itself. Jobs in reservations are charged at 1.5x the standard rate. To request a reservation please contact the Tursa Service Desk . You need to provide the following: The start time and date of the reservation. The end time and date of the reservation. The project code for the reservation. The number of nodes required. Your justification for the reservation -- this must be provided or the request will be rejected. Your request will be checked by the Tursa User Administration team and, if approved, you will be provided a reservation ID which can be used on the system. To submit jobs to a reservation, you need to add --reservation=<reservation ID> to your job submission script or command. Important You must have at least 1 CU in the budget to submit a job on Tursa, even to a pre-paid reservation. Tip You can submit jobs to a reservation as soon as the reservation has been set up; jobs will remain queued until the reservation starts. Best practices for job submission This guidance is adapted from the advice provided by NERSC Time Limits Due to backfill scheduling, short and variable-length jobs generally start quickly resulting in much better job throughput. You can specify a minimum time for your job with the --time-min option to SBATCH: #SBATCH --time-min=<lower_bound> #SBATCH --time=<upper_bound> Within your job script, you can get the time remaining in the job with squeue -h -j ${Slurm_JOBID} -o %L to allow you to deal with potentially varying runtimes when using this option. Long Running Jobs Simulations which must run for a long period of time achieve the best throughput when composed of many small jobs using a checkpoint and restart method chained together (see above for how to chain jobs together). However, this method does occur a startup and shutdown overhead for each job as the state is saved and loaded so you should experiment to find the best balance between runtime (long runtimes minimise the checkpoint/restart overheads) and throughput (short runtimes maximise throughput). I/O performance Large Jobs Large jobs may take longer to start up. The sbcast command is recommended for large jobs requesting over 1500 MPI tasks. By default, Slurm reads the executable on the allocated compute nodes from the location where it is installed; this may take long time when the file system (where the executable resides) is slow or busy. The sbcast command, the executable can be copied to the /tmp directory on each of the compute nodes. Since /tmp is part of the memory on the compute nodes, it can speed up the job startup time. sbcast --compress=lz4 /path/to/exe /tmp/exe srun /tmp/exe","title":"Running jobs on Tursa"},{"location":"tursa-user-guide/scheduler/#running-jobs-on-tursa","text":"As with most HPC services, Tursa uses a scheduler to manage access to resources and ensure that the thousands of different users of system are able to share the system and all get access to the resources they require. Tursa uses the Slurm software to schedule jobs. Writing a submission script is typically the most convenient way to submit your job to the scheduler. Example submission scripts (with explanations) for the most common job types are provided below. Interactive jobs are also available and can be particularly useful for developing and debugging applications. More details are available below. Hint If you have any questions on how to run jobs on Tursa do not hesitate to contact the DiRAC Service Desk . You typically interact with Slurm by issuing Slurm commands from the login nodes (to submit, check and cancel jobs), and by specifying Slurm directives that describe the resources required for your jobs in job submission scripts.","title":"Running jobs on Tursa"},{"location":"tursa-user-guide/scheduler/#resources","text":"","title":"Resources"},{"location":"tursa-user-guide/scheduler/#coreh","text":"Time used on Tursa CPU nodes is measured in coreh. 1 coreh = 1 physical core for 1 hour. So a Tursa compute node with 2, 64 core CPUs would cost 128 coreh per hour.","title":"coreh"},{"location":"tursa-user-guide/scheduler/#gpuh","text":"Time used on Tursa GPU nodes is measured in GPUh. 1 GPUh = 1 GPU for 1 hour. So a Tursa compute node with 4 GPUs would cost 4 GPUh per hour.","title":"GPUh"},{"location":"tursa-user-guide/scheduler/#checking-available-budget","text":"You can check in SAFE by selecting Login accounts from the menu, select the login account you want to query. Under Login account details you will see each of the budget codes you have access to listed e.g. e123 resources and then under Resource Pool to the right of this, a note of the remaining budgets. When logged in to the machine you can also use the command sacctmgr show assoc where user=$LOGNAME format=account,user,maxtresmins This will list all the budget codes that you have access to e.g. Account User MaxTRESMins ---------- ---------- ------------- e123 userx cpu=0 e123-test userx This shows that userx is a member of budgets e123 and e123-test . However, the cpu=0 indicates that the e123 budget is empty or disabled. This user can submit jobs using the e123-test budget. To see the number of CUs remaining you must check in SAFE .","title":"Checking available budget"},{"location":"tursa-user-guide/scheduler/#charging","text":"Jobs run on Tursa are charged for the time they use i.e. from the time the job begins to run until the time the job ends (not the full wall time requested). Jobs are charged for the full number of nodes which are requested, even if they are not all used. Charging takes place at the time the job ends, and the job is charged in full to the budget which is live at the end time.","title":"Charging"},{"location":"tursa-user-guide/scheduler/#basic-slurm-commands","text":"There are three key commands used to interact with the Slurm on the command line: sinfo - Get information on the partitions and resources available sbatch jobscript.slurm - Submit a job submission script (in this case called: jobscript.slurm ) to the scheduler squeue - Get the current status of jobs submitted to the scheduler scancel 12345 - Cancel a job (in this case with the job ID 12345 ) We cover each of these commands in more detail below.","title":"Basic Slurm commands"},{"location":"tursa-user-guide/scheduler/#sinfo-information-on-resources","text":"sinfo is used to query information about available resources and partitions. Without any options, sinfo lists the status of all resources and partitions, e.g. sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST standard up 1-00:00:00 105 down* nid[001006,...,002014] standard up 1-00:00:00 12 drain nid[001016,...,001969] standard up 1-00:00:00 5 resv nid[001000,001002-001004,001114] standard up 1-00:00:00 683 alloc nid[001001,...,001970-001991] standard up 1-00:00:00 214 idle nid[001022-001023,...,002015-002023] standard up 1-00:00:00 2 down nid[001021,001050] Here we see the number of nodes in different states. For example, 683 nodes are allocated (running jobs), and 214 are idle (available to run jobs). !!! note that long lists of node IDs have been abbreviated with ... .","title":"sinfo: information on resources"},{"location":"tursa-user-guide/scheduler/#sbatch-submitting-jobs","text":"sbatch is used to submit a job script to the job submission system. The script will typically contain one or more srun commands to launch parallel tasks. When you submit the job, the scheduler provides the job ID, which is used to identify this job in other Slurm commands and when looking at resource usage in SAFE. sbatch test-job.slurm Submitted batch job 12345","title":"sbatch: submitting jobs"},{"location":"tursa-user-guide/scheduler/#squeue-monitoring-jobs","text":"squeue without any options or arguments shows the current status of all jobs known to the scheduler. For example: squeue will list all jobs on Tursa. The output of this is often overwhelmingly large. You can restrict the output to just your jobs by adding the -u $USER option: squeue -u $USER","title":"squeue: monitoring jobs"},{"location":"tursa-user-guide/scheduler/#scancel-deleting-jobs","text":"scancel is used to delete a jobs from the scheduler. If the job is waiting to run it is simply cancelled, if it is a running job then it is stopped immediately. You need to provide the job ID of the job you wish to cancel/stop. For example: scancel 12345 will cancel (if waiting) or stop (if running) the job with ID 12345 .","title":"scancel: deleting jobs"},{"location":"tursa-user-guide/scheduler/#resource-limits","text":"The Tursa resource limits for any given job are covered by three separate attributes. The amount of primary resource you require, i.e., number of compute nodes. The partition that you want to use - this specifies the nodes that are eligible to run your job. The Quality of Service (QoS) that you want to use - this specifies the job limits that apply.","title":"Resource Limits"},{"location":"tursa-user-guide/scheduler/#primary-resource","text":"The primary resource you can request for your job is the compute node. Information The --exclusive option is enforced on Tursa which means you will always have access to all of the memory on the compute node regardless of how many processes are actually running on the node. Note You will not generally have access to the full amount of memory resource on the the node as some is retained for running the operating system and other system processes.","title":"Primary resource"},{"location":"tursa-user-guide/scheduler/#partitions","text":"On Tursa, compute nodes are grouped into partitions. You will have to specify a partition using the --partition option in your Slurm submission script. The following table has a list of active partitions on Tursa. Partition Description Max nodes available standard CPU nodes with AMD EPYC 7742 64-core processor \u00d7 2 1024 Tursa Partitions You can list the active partitions by running sinfo . Tip You may not have access to all the available partitions.","title":"Partitions"},{"location":"tursa-user-guide/scheduler/#quality-of-service-qos","text":"On Tursa, job limits are defined by the requested Quality of Service (QoS), as specified by the --qos Slurm directive. The following table lists the active QoS on Tursa. QoS Max Nodes Per Job Max Walltime Jobs Queued Jobs Running Partition(s) Notes standard 256 24 hrs 64 16 standard Maximum of 256 nodes in use by any one user at any time short 8 20 mins 16 4 standard long 64 48 hrs 16 16 standard Minimum walltime of 24 hrs largescale 940 3 hrs 4 1 standard Minimum job size of 257 nodes lowpriority 256 3 hrs 4 1 standard Maximum of 256 nodes in use by any one user at any time. Jobs not charged but requires at least 1 CU in budget to use. Warning If you want to use the short QoS then you also need to add the --reservation=shortqos to your job submission command. You can find out the QoS that you can use by running the following command: sacctmgr show assoc user=$USER cluster=tursa-es format=cluster,account,user,qos%50 Hint If you have needs which do not fit within the current QoS, please contact the Service Desk and we can discuss how to accommodate your requirements.","title":"Quality of Service (QoS)"},{"location":"tursa-user-guide/scheduler/#e-mail-notifications","text":"E-mail notifications from the batch system are not currently available on Tursa.","title":"E-mail notifications"},{"location":"tursa-user-guide/scheduler/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"tursa-user-guide/scheduler/#slurm-error-messages","text":"An incorrect submission will cause Slurm to return an error. Some common problems are listed below, with a suggestion about the likely cause: sbatch: unrecognized option <text> One of your options is invalid or has a typo. man sbatch to help. error: Batch job submission failed: No partition specified or system default partition A --partition= option is missing. You must specify the partition (see the list above). This is most often --partition=standard . error: invalid partition specified: <partition> error: Batch job submission failed: Invalid partition name specified Check the partition exists and check the spelling is correct. error: Batch job submission failed: Invalid account or account/partition combination specified This probably means an invalid account has been given. Check the --account= options against valid accounts in SAFE. error: Batch job submission failed: Invalid qos specification A QoS option is either missing or invalid. Check the script has a --qos= option and that the option is a valid one from the table above. (Check the spelling of the QoS is correct.) error: Your job has no time specification (--time=)... Add an option of the form --time=hours:minutes:seconds to the submission script. E.g., --time=01:30:00 gives a time limit of 90 minutes. error: QOSMaxWallDurationPerJobLimit error: Batch job submission failed: Job violates accounting/QOS policy (job submit limit, user's size and/or time limits) The script has probably specified a time limit which is too long for the corresponding QoS. E.g., the time limit for the short QoS is 20 minutes.","title":"Slurm error messages"},{"location":"tursa-user-guide/scheduler/#slurm-queued-reasons","text":"The squeue command allows users to view information for jobs managed by Slurm. Jobs typically go through the following states: PENDING, RUNNING, COMPLETING, and COMPLETED. The first table provides a description of some job state codes. The second table provides a description of the reasons that cause a job to be in a state. Status Code Description PENDING PD Job is awaiting resource allocation. RUNNING R Job currently has an allocation. SUSPENDED S Job currently has an allocation. COMPLETING CG Job is in the process of completing. Some processes on some nodes may still be active. COMPLETED CD Job has terminated all processes on all nodes with an exit code of zero. TIMEOUT TO Job terminated upon reaching its time limit. STOPPED ST Job has an allocation, but execution has been stopped with SIGSTOP signal. CPUS have been retained by this job. OUT_OF_MEMORY OOM Job experienced out of memory error. FAILED F Job terminated with non-zero exit code or other failure condition. NODE_FAIL NF Job terminated due to failure of one or more allocated nodes. CANCELLED CA Job was explicitly cancelled by the user or system administrator. The job may or may not have been initiated. For a full list of see Job State Codes . Reason Description Priority One or more higher priority jobs exist for this partition or advanced reservation. Resources The job is waiting for resources to become available. BadConstraints The job's constraints can not be satisfied. BeginTime The job's earliest start time has not yet been reached. Dependency This job is waiting for a dependent job to complete. Licenses The job is waiting for a license. WaitingForScheduling No reason has been set for this job yet. Waiting for the scheduler to determine the appropriate reason. Prolog Its PrologSlurmctld program is still running. JobHeldAdmin The job is held by a system administrator. JobHeldUser The job is held by the user. JobLaunchFailure The job could not be launched. This may be due to a file system problem, invalid program name, etc. NonZeroExitCode The job terminated with a non-zero exit code. InvalidAccount The job's account is invalid. InvalidQOS The job's QOS is invalid. QOSUsageThreshold Required QOS threshold has been breached. QOSJobLimit The job's QOS has reached its maximum job count. QOSResourceLimit The job's QOS has reached some resource limit. QOSTimeLimit The job's QOS has reached its time limit. NodeDown A node required by the job is down. TimeLimit The job exhausted its time limit. ReqNodeNotAvail Some node specifically required by the job is not currently available. The node may currently be in use, reserved for another job, in an advanced reservation, DOWN, DRAINED, or not responding. Nodes which are DOWN, DRAINED, or not responding will be identified as part of the job's \"reason\" field as \"UnavailableNodes\". Such nodes will typically require the intervention of a system administrator to make available. For a full list of see Job Reasons .","title":"Slurm queued reasons"},{"location":"tursa-user-guide/scheduler/#output-from-slurm-jobs","text":"Slurm places standard output (STDOUT) and standard error (STDERR) for each job in the file slurm_<JobID>.out . This file appears in the job's working directory once your job starts running. Hint Output may be buffered - to enable live output, e.g. for monitoring job status, add --unbuffered to the srun command in your SLURM script.","title":"Output from Slurm jobs"},{"location":"tursa-user-guide/scheduler/#specifying-resources-in-job-scripts","text":"You specify the resources you require for your job using directives at the top of your job submission script using lines that start with the directive #SBATCH . Hint Most options provided using #SBATCH directives can also be specified as command line options to srun . If you do not specify any options, then the default for each option will be applied. As a minimum, all job submissions must specify the budget that they wish to charge the job too with the option: --account=<budgetID> your budget ID is usually something like t01 or t01-test . You can see which budget codes you can charge to in SAFE. Other common options that are used are: --time=<hh:mm:ss> the maximum walltime for your job. e.g. For a 6.5 hour walltime, you would use --time=6:30:0 . --job-name=<jobname> set a name for the job to help identify it in In addition, parallel jobs will also need to specify how many nodes, parallel processes and threads they require. --nodes=<nodes> the number of nodes to use for the job. --tasks-per-node=<processes per node> the number of parallel processes (e.g. MPI ranks) per node. --cpus-per-task=1 if you are using parallel processes only with no threading then you should set the number of CPUs (cores) per parallel process to 1. !!! note: if you are using threading (e.g. with OpenMP) then you will need to change this option as described below. For parallel jobs that use threading (e.g. OpenMP), you will also need to change the --cpus-per-task option. --cpus-per-task=<threads per task> the number of threads per parallel process (e.g. number of OpenMP threads per MPI task for hybrid MPI/OpenMP jobs). !!! note: you must also set the OMP_NUM_THREADS environment variable if using OpenMP in your job. Note For parallel jobs, Tursa operates in a node exclusive way. This means that you are assigned resources in the units of full compute nodes for your jobs ( i.e. 128 cores) and that no other user can share those compute nodes with you. Hence, the minimum amount of resource you can request for a parallel job is 1 node (or 128 cores). To prevent the behaviour of batch scripts being dependent on the user environment at the point of submission, the option --export=none prevents the user environment from being exported to the batch system. Using the --export=none means that the behaviour of batch submissions should be repeatable. We strongly recommend its use, although see the following section to enable access to the usual modules.","title":"Specifying resources in job scripts"},{"location":"tursa-user-guide/scheduler/#srun-launching-parallel-jobs","text":"If you are running parallel jobs, your job submission script should contain one or more srun commands to launch the parallel executable across the compute nodes. In most cases you will want to add the options --distribution=block:block and --hint=nomultithread to your srun command to ensure you get the correct pinning of processes to cores on a compute node. Warning If you do not add the --distribution=block:block and --hint=nomultithread options to your srun command the default process placement may lead to a drop in performance for your jobs on Tursa. A brief explanation of these options: - --hint=nomultithread - do not use hyperthreads/SMP - --distribution=block:block - the first block means use a block distribution of processes across nodes (i.e. fill nodes before moving onto the next one) and the second block means use a block distribution of processes across NUMA regions within a node (i.e. fill a NUMA region before moving on to the next one).","title":"srun: Launching parallel jobs"},{"location":"tursa-user-guide/scheduler/#example-job-submission-scripts","text":"A subset of example job submission scripts are included in full below.","title":"Example job submission scripts"},{"location":"tursa-user-guide/scheduler/#example-job-submission-script-for-mpi-parallel-job","text":"A simple MPI job submission script to submit a job using 4 compute nodes and 128 MPI ranks per node for 20 minutes would look like: #!/bin/bash # Slurm job options (job-name, compute nodes, job time) #SBATCH --job-name=Example_MPI_Job #SBATCH --time=0:20:0 #SBATCH --nodes=4 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 # Replace [budget code] below with your budget code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the job environment (this module needs to be loaded before any other modules) module load epcc-job-env # Set the number of threads to 1 # This prevents any threaded system libraries from automatically # using threading. export OMP_NUM_THREADS=1 # Launch the parallel job # Using 512 MPI processes and 128 MPI processes per node # srun picks up the distribution from the sbatch options srun --distribution=block:block --hint=nomultithread ./my_mpi_executable.x This will run your executable \"my_mpi_executable.x\" in parallel on 512 MPI processes using 4 nodes (128 cores per node, i.e. not using hyper-threading). Slurm will allocate 4 nodes to your job and srun will place 128 MPI processes on each node (one per physical core). See above for a more detailed discussion of the different sbatch options","title":"Example: job submission script for MPI parallel job"},{"location":"tursa-user-guide/scheduler/#example-job-submission-script-for-mpiopenmp-mixed-mode-parallel-job","text":"Mixed mode codes that use both MPI (or another distributed memory parallel model) and OpenMP should take care to ensure that the shared memory portion of the process/thread placement does not span more than one NUMA region. Nodes on Tursa are made up of two sockets each containing 4 NUMA regions of 16 cores, i.e. there are 8 NUMA regions in total. Therefore the total number of threads should ideally not be greater than 16, and also needs to be a factor of 16. Sensible choices for the number of threads are therefore 1 (single-threaded), 2, 4, 8, and 16. More information about using OpenMP and MPI+OpenMP can be found in the Tuning chapter. To ensure correct placement of MPI processes the number of cpus-per-task needs to match the number of OpenMP threads, and the number of tasks-per-node should be set to ensure the entire node is filled with MPI tasks. In the example below, we are using 4 nodes for 6 hours. There are 32 MPI processes in total (8 MPI processes per node) and 16 OpenMP threads per MPI process. This results in all 128 physical cores per node being used. Hint Note the use of the export OMP_PLACES=cores environment option to generate the correct thread pinning. #!/bin/bash # Slurm job options (job-name, compute nodes, job time) #SBATCH --job-name=Example_MPI_Job #SBATCH --time=0:20:0 #SBATCH --nodes=4 #SBATCH --tasks-per-node=8 #SBATCH --cpus-per-task=16 # Replace [budget code] below with your project code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the job environment (this module needs to be loaded before any other modules) module load epcc-job-env # Set the number of threads to 16 and specify placement # There are 16 OpenMP threads per MPI process # We want one thread per physical core export OMP_NUM_THREADS=16 export OMP_PLACES=cores # Launch the parallel job # Using 32 MPI processes # 8 MPI processes per node # 16 OpenMP threads per MPI process # Additional srun options to pin one thread per physical core srun --hint=nomultithread --distribution=block:block ./my_mixed_executable.x arg1 arg2","title":"Example: job submission script for MPI+OpenMP (mixed mode) parallel job"},{"location":"tursa-user-guide/scheduler/#job-arrays","text":"The Slurm job scheduling system offers the job array concept, for running collections of almost-identical jobs. For example, running the same program several times with different arguments or input data. Each job in a job array is called a subjob . The subjobs of a job array can be submitted and queried as a unit, making it easier and cleaner to handle the full set, compared to individual jobs. All subjobs in a job array are started by running the same job script. The job script also contains information on the number of jobs to be started, and Slurm provides a subjob index which can be passed to the individual subjobs or used to select the input data per subjob.","title":"Job arrays"},{"location":"tursa-user-guide/scheduler/#job-script-for-a-job-array","text":"As an example, the following script runs 56 subjobs, with the subjob index as the only argument to the executable. Each subjob requests a single node and uses all 128 cores on the node by placing 1 MPI process per core and specifies 4 hours maximum runtime per subjob: #!/bin/bash # Slurm job options (job-name, compute nodes, job time) #SBATCH --job-name=Example_Array_Job #SBATCH --time=04:00:00 #SBATCH --nodes=1 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 #SBATCH --array=0-55 # Replace [budget code] below with your budget code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the job environment (this module needs to be loaded before any other modules) module load epcc-job-env # Set the number of threads to 1 # This prevents any threaded system libraries from automatically # using threading. export OMP_NUM_THREADS=1 srun --distribution=block:block --hint=nomultithread /path/to/exe $SLURM_ARRAY_TASK_ID","title":"Job script for a job array"},{"location":"tursa-user-guide/scheduler/#submitting-a-job-array","text":"Job arrays are submitted using sbatch in the same way as for standard jobs: sbatch job_script.pbs","title":"Submitting a job array"},{"location":"tursa-user-guide/scheduler/#job-chaining","text":"Job dependencies can be used to construct complex pipelines or chain together long simulations requiring multiple steps. Hint The --parsable option to sbatch can simplify working with job dependencies. It returns the job ID in a format that can be used as the input to other commands. For example: jobid=$(sbatch --parsable first_job.sh) sbatch --dependency=afterok:$jobid second_job.sh or for a longer chain: jobid1=$(sbatch --parsable first_job.sh) jobid2=$(sbatch --parsable --dependency=afterok:$jobid1 second_job.sh) jobid3=$(sbatch --parsable --dependency=afterok:$jobid1 third_job.sh) sbatch --dependency=afterok:$jobid2,afterok:$jobid3 last_job.sh","title":"Job chaining"},{"location":"tursa-user-guide/scheduler/#using-multiple-srun-commands-in-a-single-job-script","text":"You can use multiple srun commands within in a Slurm job submission script to allow you to use the resource requested more flexibly. For example, you could run a collection of smaller jobs within the requested resources or you could even subdivide nodes if your individual calculations do not scale up to use all 128 cores on a node. In this guide we will cover two scenarios: Subdividing the job into multiple full-node or multi-node subjobs, e.g. requesting 100 nodes and running 100, 1-node subjobs or 50, 2-node subjobs. Subdividing the job into multiple subjobs that each use a fraction of a node, e.g. requesting 2 nodes and running 256, 1-core subjobs or 16, 16-core subjobs.","title":"Using multiple srun commands in a single job script"},{"location":"tursa-user-guide/scheduler/#running-multiple-full-node-subjobs-within-a-larger-job","text":"When subdivding a larger job into smaller subjobs you typically need to overwrite the --nodes option to srun and add the --ntasks option to ensure that each subjob runs on the correct number of nodes and that subjobs are placed correctly onto separate nodes. For example, we will show how to request 100 nodes and then run 100 separate 1-node jobs, each of which use 128 MPI processes and which run on a different compute node. We start by showing the job script that would achieve this and then explain how this works and the options used. In our case, we will run 100 copies of the xthi program that prints the process placement on the node it is running on. #!/bin/bash # Slurm job options (job-name, compute nodes, job time) #SBATCH --job-name=multi_xthi #SBATCH --time=0:20:0 #SBATCH --nodes=100 #SBATCH --tasks-per-node=128 #SBATCH --cpus-per-task=1 # Replace [budget code] below with your budget code (e.g. t01) #SBATCH --account=[budget code] #SBATCH --partition=standard #SBATCH --qos=standard # Setup the job environment (this module needs to be loaded before any other modules) module load epcc-job-env # Load the xthi module module load xthi # Set the number of threads to 1 # This prevents any threaded system libraries from automatically # using threading. export OMP_NUM_THREADS = 1 # Loop over 100 subjobs starting each of them on a separate node for i in $( seq 1 100 ) do # Launch this subjob on 1 node, note nodes and ntasks options and & to place subjob in the background srun --nodes = 1 --ntasks = 128 --distribution = block:block --hint = nomultithread xthi > placement ${ i } .txt & done # Wait for all background subjobs to finish wait Key points from the example job script: The #SBATCH options select 100 full nodes in the usual way. Each subjob srun command sets the following: --nodes=1 We need override this setting from the main job so that each subjob only uses 1 node --ntasks=128 For normal jobs, the number of parallel tasks (MPI processes) is calculated from the number of nodes you request and the number of tasks per node. We need to explicitly tell srun how many we require for this subjob. --distribution=block:block --hint=nomultithread These options ensure correct placement of processes within the compute nodes. & Each subjob srun command ends with an ampersand to place the process in the background and move on to the next loop iteration (and subjob submission). Without this, the script would wait for this subjob to complete before moving on to submit the next. Finally, there is the wait command to tell the script to wait for all the background subjobs to complete before exiting. If we did not have this in place, the script would exit as soon as the last subjob was submitted and kill all running subjobs.","title":"Running multiple, full-node subjobs within a larger job"},{"location":"tursa-user-guide/scheduler/#interactive-jobs","text":"","title":"Interactive Jobs"},{"location":"tursa-user-guide/scheduler/#using-salloc-to-reserve-resources","text":"When you are developing or debugging code you often want to run many short jobs with a small amount of editing the code between runs. This can be achieved by using the login nodes to run MPI but you may want to test on the compute nodes (e.g. you may want to test running on multiple nodes across the high performance interconnect). One of the best ways to achieve this on Tursa is to use interactive jobs. An interactive job allows you to issue srun commands directly from the command line without using a job submission script, and to see the output from your program directly in the terminal. You use the salloc command to reserve compute nodes for interactive jobs. To submit a request for an interactive job reserving 8 nodes (1024 physical cores) for 20 minutes on the short queue you would issue the following command from the command line: auser@login01:> salloc --nodes=8 --tasks-per-node=128 --cpus-per-task=1 \\ --time=00:20:00 --partition=standard --qos=short \\ --reservation=shortqos --account=[budget code] When you submit this job your terminal will display something like: salloc: Granted job allocation 24236 salloc: Waiting for resource configuration salloc: Nodes nid000002 are ready for job auser@login01:> It may take some time for your interactive job to start. Once it runs you will enter a standard interactive terminal session (a new shell). Note that this shell is still on the front end (the prompt has not change). Whilst the interactive session lasts you will be able to run parallel jobs on the compute nodes by issuing the srun --distribution=block:block --hint=nomultithread command directly at your command prompt using the same syntax as you would inside a job script. The maximum number of nodes you can use is limited by resources requested in the salloc command. If you know you will be doing a lot of intensive debugging you may find it useful to request an interactive session lasting the expected length of your working session, say a full day. Your session will end when you hit the requested walltime. If you wish to finish before this you should use the exit command - this will return you to your prompt before you issued the salloc command.","title":"Using salloc to reserve resources"},{"location":"tursa-user-guide/scheduler/#using-srun-directly","text":"A second way to run an interactive job is to use srun directly in the following way (here using the \"short queue\"): auser@login01:/work/t01/t01/auser> srun --nodes=1 --exclusive --time=00:20:00 \\ --partition=standard --qos=short --reservation=shortqos \\ --pty /bin/bash auser@login01:/work/t01/t01/auser> hostname nid001261 The --pty /bin/bash will cause a new shell to be started on the first node of a new allocation (note that while the shell prompt has not changed, we are now on the compute node). This is perhaps closer to what many people consider an 'interactive' job than the method using salloc appears. One can now issue shell commands in the usual way. A further invocation of srun is required to launch a parallel job in the allocation. When finished, type exit to relinquish the allocation and control will be returned to the front end. By default, the interactive shell will retain the environment of the parent. If you want a clean shell, remember to specify --export=none . If you need to use modules within your job , you will need to start a login shell by passing the --login argument to bash .","title":"Using srun directly"},{"location":"tursa-user-guide/scheduler/#heterogeneous-jobs","text":"The SLURM submissions discussed above involve a single executable image. However, there are situtions where two or more distinct executables are coupled and need to be run at the same time. This is most easily handled via the SLURM heterogeneous job mechanism. The essential feature of a heterogeneous job is to create a single batch submission which specifies the resource requirements for the individual components. Schematically, we would use #!/bin/bash # SLURM specifications for the first component #SBATCH --partition=standard ... #SBATCH hetjob # SLURM specifications for the second component #SBATCH --partition=standard ... where new each component beyond the first is introduced by the special token #SBATCH hetjob (note this is not a normal option and is not --hetjob ). Each component must specify a partition. Such a job will appear in the queue system as, e.g., 50098+0 standard qscript- user PD 0:00 1 (None) 50098+1 standard qscript- user PD 0:00 2 (None) and counts as (in this case) two separate jobs from the point of QoS limits. Two common cases are discussed below: first, a client server model in which client and server each have a different MPI_COMM_WORLD , and second the case were two or more executables share MPI_COMM_WORLD .","title":"Heterogeneous jobs"},{"location":"tursa-user-guide/scheduler/#heterogeneous-jobs-for-a-clientserver-model","text":"Consider a case where we have two executables which may both be parallel (in that they use MPI), both run at the same time, and communicate with each other by some means other than MPI. In the following example, we run two different executables, both of which must finish before the jobs completes. #!/bin/bash #SBATCH --time=00:20:00 #SBATCH --exclusive #SBATCH --export=none #SBATCH --partition=standard #SBATCH --qos=standard #SBATCH --nodes=1 #SBATCH --ntasks-per-node=8 #SBATCH hetjob #SBATCH --partition=standard #SBATCH --qos=standard #SBATCH --nodes=2 #SBATCH --ntasks-per-node=4 # Run two execuatables with separate MPI_COMM_WORLD srun --distribution=block:block --hint=nomultithread --het-group=0 ./xthi-a & srun --distribution=block:block --hint=nomultithread --het-group=1 ./xthi-b & wait In this case, each executable is launched with a separate call to srun but specifies a different heterogeneous group via the --het-group option. The first group is --het-group=0 . Both are run in the background with & and the wait is required to ensure both executables have completed before the job submission exits. In this rather artificial example, where each component makes a simple report about its placement, the output might be Node 0, hostname nid001028, mpi 4, omp 1, executable xthi-b Node 1, hostname nid001048, mpi 4, omp 1, executable xthi-b Node 0, rank 0, thread 0, (affinity = 0) Node 0, rank 1, thread 0, (affinity = 1) Node 0, rank 2, thread 0, (affinity = 2) Node 0, rank 3, thread 0, (affinity = 3) Node 1, rank 4, thread 0, (affinity = 0) Node 1, rank 5, thread 0, (affinity = 1) Node 1, rank 6, thread 0, (affinity = 2) Node 1, rank 7, thread 0, (affinity = 3) Node 0, hostname nid001027, mpi 8, omp 1, executable xthi-a Node 0, rank 0, thread 0, (affinity = 0) Node 0, rank 1, thread 0, (affinity = 1) Node 0, rank 2, thread 0, (affinity = 2) Node 0, rank 3, thread 0, (affinity = 3) Node 0, rank 4, thread 0, (affinity = 4) Node 0, rank 5, thread 0, (affinity = 5) Node 0, rank 6, thread 0, (affinity = 6) Node 0, rank 7, thread 0, (affinity = 7) Here we have the first executable running on one node with a communicator size 8 (ranks 0-7). The second executable runs on two nodes also with communicator size 8 (ranks 0-7, 4 ranks per node). Further examples of placement for heterogenenous jobs are given below.","title":"Heterogeneous jobs for a client/server model"},{"location":"tursa-user-guide/scheduler/#heterogeneous-jobs-for-a-shared-mpi_com_world","text":"If two or more heterogeneous components need to share a unique MPI_COMM_WORLD , a single srun invocation with the differrent components separated by a colon : should be used. For example, #!/bin/bash #SBATCH --time=00:20:00 #SBATCH --exclusive #SBATCH --export=none #SBATCH --partition=standard #SBATCH --qos=standard #SBATCH --nodes=1 #SBATCH --ntasks-per-node=8 #SBATCH hetjob #SBATCH --partition=standard #SBATCH --qos=standard #SBATCH --nodes=2 #SBATCH --ntasks-per-node=4 srun --distribution=block:block --hint=nomultithread --het-group=0 ./xthi-a : \\ --distribution=block:block --hint=nomultithread --het-group=1 ./xthi-b The output should confirm we have a single MPI_COMM_WORLD with ranks 0-15. Node 0, hostname nid001027, mpi 8, omp 1, executable xthi-a Node 1, hostname nid001028, mpi 4, omp 1, executable xthi-b Node 2, hostname nid001048, mpi 4, omp 1, executable xthi-b Node 0, rank 0, thread 0, (affinity = 0) Node 0, rank 1, thread 0, (affinity = 1) Node 0, rank 2, thread 0, (affinity = 2) Node 0, rank 3, thread 0, (affinity = 3) Node 0, rank 4, thread 0, (affinity = 4) Node 0, rank 5, thread 0, (affinity = 5) Node 0, rank 6, thread 0, (affinity = 6) Node 0, rank 7, thread 0, (affinity = 7) Node 1, rank 8, thread 0, (affinity = 0) Node 1, rank 9, thread 0, (affinity = 1) Node 1, rank 10, thread 0, (affinity = 2) Node 1, rank 11, thread 0, (affinity = 3) Node 2, rank 12, thread 0, (affinity = 0) Node 2, rank 13, thread 0, (affinity = 1) Node 2, rank 14, thread 0, (affinity = 2) Node 2, rank 15, thread 0, (affinity = 3)","title":"Heterogeneous jobs for a shared MPI_COM_WORLD"},{"location":"tursa-user-guide/scheduler/#heterogeneous-placement-for-mixed-mpiopenmp-work","text":"Some care may be required for placement of tasks/threads in heterogeneous jobs in which the number of threads needs to be specified differently for different components. In the following we have two components. The first component runs 8 MPI tasks each with 16 OpenMP threads. The second component runs 8 MPI tasks with one task per NUMA region on one node; each task has one thread. An appropriate SLURM submission might be: #!/bin/bash #SBATCH --time=00:20:00 #SBATCH --exclusive #SBATCH --export=none # First component #SBATCH --partition=standard #SBATCH --qos=standard #SBATCH --nodes=1 #SBATCH --ntasks-per-node=8 #SBATCH --cpus-per-task=16 #SBATCH --hint=nomultithread # Second component #SBATCH hetjob #SBATCH --partition=standard #SBATCH --nodes=1 #SBATCH --ntasks-per-node=8 #SBATCH --cpus-per-task=16 # Do not set OMP_NUM_THREADS in the calling environment unset OMP_NUM_THREADS export OMP_PROC_BIND=spread srun --het-group=0 --export=all,OMP_NUM_THREADS=16 ./xthi-a : \\ --het-group=1 --export=all,OMP_NUM_THREADS=1 ./xthi-b The important point here is that OMP_NUM_THREADS must not be set in the environment that calls srun in order that the different specifications for the separate groups via --export on the srun command line take effect. If OMP_NUM_THREADS is set in the calling environment, then that value takes precedence, and each component will see the same value of OMP_NUM_THREADS . The output would be: Node 0, hostname nid001111, mpi 8, omp 16, executable xthi-a Node 1, hostname nid001126, mpi 8, omp 1, executable xthi-b Node 0, rank 0, thread 0, (affinity = 0) Node 0, rank 0, thread 1, (affinity = 1) Node 0, rank 0, thread 2, (affinity = 2) Node 0, rank 0, thread 3, (affinity = 3) Node 0, rank 0, thread 4, (affinity = 4) Node 0, rank 0, thread 5, (affinity = 5) Node 0, rank 0, thread 6, (affinity = 6) Node 0, rank 0, thread 7, (affinity = 7) Node 0, rank 0, thread 8, (affinity = 8) Node 0, rank 0, thread 9, (affinity = 9) Node 0, rank 0, thread 10, (affinity = 10) Node 0, rank 0, thread 11, (affinity = 11) Node 0, rank 0, thread 12, (affinity = 12) Node 0, rank 0, thread 13, (affinity = 13) Node 0, rank 0, thread 14, (affinity = 14) Node 0, rank 0, thread 15, (affinity = 15) Node 0, rank 1, thread 0, (affinity = 16) Node 0, rank 1, thread 1, (affinity = 17) ... Node 0, rank 7, thread 14, (affinity = 126) Node 0, rank 7, thread 15, (affinity = 127) Node 1, rank 8, thread 0, (affinity = 0) Node 1, rank 9, thread 0, (affinity = 16) Node 1, rank 10, thread 0, (affinity = 32) Node 1, rank 11, thread 0, (affinity = 48) Node 1, rank 12, thread 0, (affinity = 64) Node 1, rank 13, thread 0, (affinity = 80) Node 1, rank 14, thread 0, (affinity = 96) Node 1, rank 15, thread 0, (affinity = 112)","title":"Heterogeneous placement for mixed MPI/OpenMP work"},{"location":"tursa-user-guide/scheduler/#low-priority-access","text":"Low priority jobs are not charged against your allocation but will only run when other, higher-priority, jobs cannot be run or there are no higher-priority jobs in the queue. Although low priority jobs are not charged, you do need a valid, positive budget to be able to submit and run low priority jobs, i.e. you need at least 1 CU in your budget. Low priority access is always available and has the following limits: 256 node maximum job size 256 nodes maximum in use by any one user 512 nodes maximum in use by low priority at any one time Maximum 4 low priority jobs in the queue per user Maximum 1 low priority job running per user (of the 4 queued) Maximum runtime of 3 hours You submit a low priority job on Tursa by using the lowpriority QoS. For example, you would usually have the following line in your job submission script sbatch options: #SBATCH --qos=lowpriority","title":"Low priority access"},{"location":"tursa-user-guide/scheduler/#reservations","text":"Reservations are available on Tursa. These allow users to reserve a number of nodes for a specified length of time starting at a particular time on the system. Reservations require justification. They will only be approved if the request could not be fulfilled with the standard queues. For instance, you require a job/jobs to run at a particular time e.g. for a demonstration or course. Note Reservation requests must be submitted at least 60 hours in advance of the reservation start time. If requesting a reservation for a Monday at 18:00, please ensure this is received by the Friday at 12:00 the latest. The same applies over Service Holidays. Note Reservations are only valid for standard compute nodes, high memory compute nodes and/or PP nodes cannot be included in reservations. Reservations will be charged at 1.5 times the usual CU rate and our policy is that they will be charged the full rate for the entire reservation at the time of booking, whether or not you use the nodes for the full time. In addition, you will not be refunded the CUs if you fail to use them due to a job crash unless this crash is due to a system failure. Bug At the moment, we are only able to charge for jobs in reservations, not for the full reservation itself. Jobs in reservations are charged at 1.5x the standard rate. To request a reservation please contact the Tursa Service Desk . You need to provide the following: The start time and date of the reservation. The end time and date of the reservation. The project code for the reservation. The number of nodes required. Your justification for the reservation -- this must be provided or the request will be rejected. Your request will be checked by the Tursa User Administration team and, if approved, you will be provided a reservation ID which can be used on the system. To submit jobs to a reservation, you need to add --reservation=<reservation ID> to your job submission script or command. Important You must have at least 1 CU in the budget to submit a job on Tursa, even to a pre-paid reservation. Tip You can submit jobs to a reservation as soon as the reservation has been set up; jobs will remain queued until the reservation starts.","title":"Reservations"},{"location":"tursa-user-guide/scheduler/#best-practices-for-job-submission","text":"This guidance is adapted from the advice provided by NERSC","title":"Best practices for job submission"},{"location":"tursa-user-guide/scheduler/#time-limits","text":"Due to backfill scheduling, short and variable-length jobs generally start quickly resulting in much better job throughput. You can specify a minimum time for your job with the --time-min option to SBATCH: #SBATCH --time-min=<lower_bound> #SBATCH --time=<upper_bound> Within your job script, you can get the time remaining in the job with squeue -h -j ${Slurm_JOBID} -o %L to allow you to deal with potentially varying runtimes when using this option.","title":"Time Limits"},{"location":"tursa-user-guide/scheduler/#long-running-jobs","text":"Simulations which must run for a long period of time achieve the best throughput when composed of many small jobs using a checkpoint and restart method chained together (see above for how to chain jobs together). However, this method does occur a startup and shutdown overhead for each job as the state is saved and loaded so you should experiment to find the best balance between runtime (long runtimes minimise the checkpoint/restart overheads) and throughput (short runtimes maximise throughput).","title":"Long Running Jobs"},{"location":"tursa-user-guide/scheduler/#io-performance","text":"","title":"I/O performance"},{"location":"tursa-user-guide/scheduler/#large-jobs","text":"Large jobs may take longer to start up. The sbcast command is recommended for large jobs requesting over 1500 MPI tasks. By default, Slurm reads the executable on the allocated compute nodes from the location where it is installed; this may take long time when the file system (where the executable resides) is slow or busy. The sbcast command, the executable can be copied to the /tmp directory on each of the compute nodes. Since /tmp is part of the memory on the compute nodes, it can speed up the job startup time. sbcast --compress=lz4 /path/to/exe /tmp/exe srun /tmp/exe","title":"Large Jobs"},{"location":"tursa-user-guide/sw-environment/","text":"Software environment The software environment on Tursa is primarily controlled through the module command. By loading and switching software modules you control which software and versions are available to you. Information A module is a self-contained description of a software package -- it contains the settings required to run a software package and, usually, encodes required dependencies on other software packages. By default, all users on Tursa start with the default software environment loaded. Software modules on Tursa are provided by both ATOS and by EPCC. In this section, we provide: A brief overview of the module command A brief description of how the module command manipulates your environment Using the module command We only cover basic usage of the module command here. For full documentation please see the Linux manual page on modules The module command takes a subcommand to indicate what operation you wish to perform. Common subcommands are: module list [name] - List modules currently loaded in your environment, optionally filtered by [name] module avail [name] - List modules available, optionally filtered by [name] module savelist - List module collections available (usually used for accessing different programming environments) module restore name - Restore the module collection called name (usually used for setting up a programming environment) module load name - Load the module called name into your environment module remove name - Remove the module called name from your environment module swap old new - Swap module new for module old in your environment module help name - Show help information on module name module show name - List what module name actually does to your environment These are described in more detail below. Information on the available modules The module list command will give the names of the modules and their versions you have presently loaded in your environment: auser@login01:~> module list Currently Loaded Modulefiles: Finding out which software modules are available on the system is performed using the module avail command. To list all software modules available, use: auser@login01:~> module avail This will list all the names and versions of the modules available on the service. Not all of them may work in your account though due to, for example, licencing restrictions. You will notice that for many modules we have more than one version, each of which is identified by a version number. One of these versions is the default. As the service develops the default version will change and old versions of software may be deleted. You can list all the modules of a particular type by providing an argument to the module avail command. For example, to list all available versions of the OpenMPI library, use: auser@login01:~> module avail openmpi If you want more info on any of the modules, you can use the module help command: auser@login01:~> module help openmpi The module show command reveals what operations the module actually performs to change your environment when it is loaded. We provide a brief overview of what the significance of these different settings mean below. For example, for the default openmpi module: auser@login01:~> module show openmpi - Loading, removing and swapping modules To load a module to use the module load command. For example, to load the default version of OpenMPI into your environment, use: auser@login01:~> module load openmpi Once you have done this, your environment will be setup to use the OpenMPI library. The above command will load the default version of OpenMPI. If you need a specific version of the software, you can add more information: auser@login01:~> module load cray-fftw/4.0.5 will load OpenMPI version 4.0.5 into your environment, regardless of the default. If you want to remove software from your environment, module remove will remove a loaded module: auser@login01:~> module remove openmpi will unload what ever version of openmpi (even if it is not the default) you might have loaded. There are many situations in which you might want to change the presently loaded version to a different one, such as trying the latest version which is not yet the default or using a legacy version to keep compatibility with old data. This can be achieved most easily by using module swap oldmodule newmodule . Suppose you have loaded version 4.0.4 of openmpi , the following command will change to version 4.0.5: auser@login01:~> module swap openmpi openmpi/4.0.5 You did not need to specify the version of the loaded module in your current environment as this can be inferred as it will be the only one you have loaded. Capturing your environment for reuse Sometimes it is useful to save the module environment that you are using to compile a piece of code or execute a piece of software. This is saved as a module collection. You can save a collection from your current environment by executing: auser@login01:~> module save [collection_name] Note If you do not specify the environment name, it is called default . You can find the list of saved module environments by executing: auser@login01:~> module savelist Named collection list: 1) default To list the modules in a collection, you can execute, e.g.,: auser@login01:~> module saveshow default ------------------------------------------------------------------- /home/t01/t01/auser/.module/default: module use --append /opt/cray/pe/perftools/20.09.0/modulefiles module use --append /opt/cray/pe/craype/2.7.0/modulefiles module use --append /usr/local/Modules/modulefiles module use --append /opt/cray/pe/cpe-prgenv/7.0.0 module use --append /opt/modulefiles module use --append /opt/cray/modulefiles module use --append /opt/cray/pe/modulefiles module use --append /opt/cray/pe/craype-targets/default/modulefiles module load cpe-gnu module load gcc module load craype module load craype-x86-rome module load --notuasked libfabric module load craype-network-ofi module load cray-dsmml module load perftools-base module load xpmem module load cray-mpich module load cray-libsci module load /work/y07/shared/tursa-modules/modulefiles-cse/epcc-setup-env Note again that the details of the collection have been saved to the home directory (the first line of output above). It is possible to save a module collection with a fully qualified path, e.g., auser@login1:~> module save /work/t01/z01/auser/.module/myenv which would make it available from the batch system. To delete a module environment, you can execute: auser@login01:~> module saverm <environment_name> Shell environment overview When you log in to Tursa, you are using the bash shell by default. As any other software, the bash shell has loaded a set of environment variables that can be listed by executing printenv or export . The environment variables listed before are useful to define the behaviour of the software you run. For instance, OMP_NUM_THREADS define the number of threads. To define an environment variable, you need to execute: export OMP_NUM_THREADS=4 Please note there are no blanks between the variable name, the assignation symbol, and the value. If the value is a string, enclose the string in double quotation marks. You can show the value of a specific environment variable if you print it: echo $OMP_NUM_THREADS Do not forget the dollar symbol. To remove an environment variable, just execute: unset OMP_NUM_THREADS","title":"Software environment"},{"location":"tursa-user-guide/sw-environment/#software-environment","text":"The software environment on Tursa is primarily controlled through the module command. By loading and switching software modules you control which software and versions are available to you. Information A module is a self-contained description of a software package -- it contains the settings required to run a software package and, usually, encodes required dependencies on other software packages. By default, all users on Tursa start with the default software environment loaded. Software modules on Tursa are provided by both ATOS and by EPCC. In this section, we provide: A brief overview of the module command A brief description of how the module command manipulates your environment","title":"Software environment"},{"location":"tursa-user-guide/sw-environment/#using-the-module-command","text":"We only cover basic usage of the module command here. For full documentation please see the Linux manual page on modules The module command takes a subcommand to indicate what operation you wish to perform. Common subcommands are: module list [name] - List modules currently loaded in your environment, optionally filtered by [name] module avail [name] - List modules available, optionally filtered by [name] module savelist - List module collections available (usually used for accessing different programming environments) module restore name - Restore the module collection called name (usually used for setting up a programming environment) module load name - Load the module called name into your environment module remove name - Remove the module called name from your environment module swap old new - Swap module new for module old in your environment module help name - Show help information on module name module show name - List what module name actually does to your environment These are described in more detail below.","title":"Using the module command"},{"location":"tursa-user-guide/sw-environment/#information-on-the-available-modules","text":"The module list command will give the names of the modules and their versions you have presently loaded in your environment: auser@login01:~> module list Currently Loaded Modulefiles: Finding out which software modules are available on the system is performed using the module avail command. To list all software modules available, use: auser@login01:~> module avail This will list all the names and versions of the modules available on the service. Not all of them may work in your account though due to, for example, licencing restrictions. You will notice that for many modules we have more than one version, each of which is identified by a version number. One of these versions is the default. As the service develops the default version will change and old versions of software may be deleted. You can list all the modules of a particular type by providing an argument to the module avail command. For example, to list all available versions of the OpenMPI library, use: auser@login01:~> module avail openmpi If you want more info on any of the modules, you can use the module help command: auser@login01:~> module help openmpi The module show command reveals what operations the module actually performs to change your environment when it is loaded. We provide a brief overview of what the significance of these different settings mean below. For example, for the default openmpi module: auser@login01:~> module show openmpi -","title":"Information on the available modules"},{"location":"tursa-user-guide/sw-environment/#loading-removing-and-swapping-modules","text":"To load a module to use the module load command. For example, to load the default version of OpenMPI into your environment, use: auser@login01:~> module load openmpi Once you have done this, your environment will be setup to use the OpenMPI library. The above command will load the default version of OpenMPI. If you need a specific version of the software, you can add more information: auser@login01:~> module load cray-fftw/4.0.5 will load OpenMPI version 4.0.5 into your environment, regardless of the default. If you want to remove software from your environment, module remove will remove a loaded module: auser@login01:~> module remove openmpi will unload what ever version of openmpi (even if it is not the default) you might have loaded. There are many situations in which you might want to change the presently loaded version to a different one, such as trying the latest version which is not yet the default or using a legacy version to keep compatibility with old data. This can be achieved most easily by using module swap oldmodule newmodule . Suppose you have loaded version 4.0.4 of openmpi , the following command will change to version 4.0.5: auser@login01:~> module swap openmpi openmpi/4.0.5 You did not need to specify the version of the loaded module in your current environment as this can be inferred as it will be the only one you have loaded.","title":"Loading, removing and swapping modules"},{"location":"tursa-user-guide/sw-environment/#capturing-your-environment-for-reuse","text":"Sometimes it is useful to save the module environment that you are using to compile a piece of code or execute a piece of software. This is saved as a module collection. You can save a collection from your current environment by executing: auser@login01:~> module save [collection_name] Note If you do not specify the environment name, it is called default . You can find the list of saved module environments by executing: auser@login01:~> module savelist Named collection list: 1) default To list the modules in a collection, you can execute, e.g.,: auser@login01:~> module saveshow default ------------------------------------------------------------------- /home/t01/t01/auser/.module/default: module use --append /opt/cray/pe/perftools/20.09.0/modulefiles module use --append /opt/cray/pe/craype/2.7.0/modulefiles module use --append /usr/local/Modules/modulefiles module use --append /opt/cray/pe/cpe-prgenv/7.0.0 module use --append /opt/modulefiles module use --append /opt/cray/modulefiles module use --append /opt/cray/pe/modulefiles module use --append /opt/cray/pe/craype-targets/default/modulefiles module load cpe-gnu module load gcc module load craype module load craype-x86-rome module load --notuasked libfabric module load craype-network-ofi module load cray-dsmml module load perftools-base module load xpmem module load cray-mpich module load cray-libsci module load /work/y07/shared/tursa-modules/modulefiles-cse/epcc-setup-env Note again that the details of the collection have been saved to the home directory (the first line of output above). It is possible to save a module collection with a fully qualified path, e.g., auser@login1:~> module save /work/t01/z01/auser/.module/myenv which would make it available from the batch system. To delete a module environment, you can execute: auser@login01:~> module saverm <environment_name>","title":"Capturing your environment for reuse"},{"location":"tursa-user-guide/sw-environment/#shell-environment-overview","text":"When you log in to Tursa, you are using the bash shell by default. As any other software, the bash shell has loaded a set of environment variables that can be listed by executing printenv or export . The environment variables listed before are useful to define the behaviour of the software you run. For instance, OMP_NUM_THREADS define the number of threads. To define an environment variable, you need to execute: export OMP_NUM_THREADS=4 Please note there are no blanks between the variable name, the assignation symbol, and the value. If the value is a string, enclose the string in double quotation marks. You can show the value of a specific environment variable if you print it: echo $OMP_NUM_THREADS Do not forget the dollar symbol. To remove an environment variable, just execute: unset OMP_NUM_THREADS","title":"Shell environment overview"}]}